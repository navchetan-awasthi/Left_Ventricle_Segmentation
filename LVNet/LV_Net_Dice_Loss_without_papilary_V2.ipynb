{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/navchetan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, BatchNormalization, UpSampling2D, concatenate\n",
    "# from tensorflow.keras.utils import np_utils\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras import backend as Ks\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,CSVLogger\n",
    "import os\n",
    "import utilModels\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = 'LV_Net_Dice_Loss_without_papilary_V2'\n",
    "name_save_directory = \"LV_Net_Dice_Loss_without_papilary_V2\"\n",
    "saving_metrics = 'Metrics_LV_Net_Dice_Loss_without_papilary_V2.csv'\n",
    "\n",
    "\n",
    "results = \"_Results\"\n",
    "images = \"_Joint_Images\"\n",
    "parent_directory = \"/tank/data/navchetan/Lars_Annotated_Datasets/Results/\"\n",
    "saveFolder = os.path.join(parent_directory,name_save_directory)\n",
    "os.mkdir(saveFolder)\n",
    "\n",
    "name_save_results_directory = name_save_directory+results\n",
    "path_results_save = os.path.join(saveFolder,name_save_results_directory)\n",
    "os.mkdir(path_results_save)\n",
    "\n",
    "name_save_images_directory = name_save_directory+images\n",
    "path_images_save = os.path.join(saveFolder,name_save_images_directory)\n",
    "os.mkdir(path_images_save)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customLoss(yTrue,yPred):\n",
    "    return (1e4)*(Ks.mean(Ks.square(yPred - yTrue), axis=-1))\n",
    "\n",
    "\n",
    "IMAGE_WIDTH = 256\n",
    "IMAGE_HEIGHT = 256\n",
    "N_TRAIN_SAMPLES = 1445\n",
    "CHANNELS = 1\n",
    "N_EVALUATE_SAMPLES = 475\n",
    "N_TESTING_SAMPLES = 342\n",
    "EVALUATE_FROM = 1\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.0001\n",
    "F = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomImgGenSC(path_train_input,path_train_output,indlst_train,\\\n",
    "                   df_train_input,df_train_output,\\\n",
    "                   H=256,W=256,onlyX=False,shuffle=False,BATCH_SIZE=16):\n",
    "    L_train = len(indlst_train)\n",
    "    while True:\n",
    "        if(shuffle):\n",
    "            random.shuffle(indlst_train)\n",
    "        ii = 0 # Current image index\n",
    "        left = L_train\n",
    "        while left>0:\n",
    "            BL = min(BATCH_SIZE,left)\n",
    "            X_BATCH = np.zeros((BL,H,W,1))\n",
    "            Y_BATCH = np.zeros((BL,H,W,1))\n",
    "            for bi in range(BL):\n",
    "                os.chdir(path_train_input)\n",
    "                imgIdx = indlst_train[ii] \n",
    "                pathr = 'Image'+str(imgIdx).zfill(5)+'.mat'\n",
    "                x = sio.loadmat(pathr)\n",
    "                X_BATCH[bi,:,:,0] = x['U']/255.0           \n",
    "                \n",
    "                if(not onlyX):\n",
    "                    os.chdir(path_train_output)\n",
    "                    pathr = 'Segment'+str(imgIdx).zfill(5)+'.mat'\n",
    "                    x = sio.loadmat(pathr)\n",
    "                    Y_BATCH[bi,:,:,0] = x['S']/255.0\n",
    "                ii+=1\n",
    "                \n",
    "            left = left - BL\n",
    "            if(not onlyX):\n",
    "                yield (X_BATCH,Y_BATCH)\n",
    "            else:\n",
    "                yield X_BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_input = \"/tank/data/navchetan/Lars_Annotated_Datasets/Training_Input\"\n",
    "path_train_output = \"/tank/data/navchetan/Lars_Annotated_Datasets/Training_Output_Without_Papillary\"\n",
    "path = \"/tank/data/navchetan/Lars_Annotated_Datasets/\"\n",
    "os.chdir(path)\n",
    "indlist_train_input = list(pd.read_csv('Training_indices.csv')['Indices'].values)\n",
    "csvFileTrain_Input = 'Training_Input.csv'\n",
    "df_train_input = pd.read_csv(csvFileTrain_Input)\n",
    "csvFileTrain_Output = 'Training_Output.csv'\n",
    "df_train_output = pd.read_csv(csvFileTrain_Output)\n",
    "BS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = CustomImgGenSC(path_train_input,path_train_output,indlist_train_input\\\n",
    "                                 ,df_train_input,df_train_output\\\n",
    "                                 ,H=256,W=256,onlyX=False,shuffle=False,BATCH_SIZE=BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_validation_input = \"/tank/data/navchetan/Lars_Annotated_Datasets/Validation_Input\"\n",
    "path_validation_output = \"/tank/data/navchetan/Lars_Annotated_Datasets/Validation_Output_Without_Papillary\"\n",
    "path = \"/tank/data/navchetan/Lars_Annotated_Datasets/\"\n",
    "os.chdir(path)\n",
    "indlist_validation_input = list(pd.read_csv('Validation_indices.csv')['Indices'].values)\n",
    "# indlist_validation_output = list(pd.read_csv('Training_indices.csv')['Indices'].values)\n",
    "csvFilevalidation_Input = 'Validation_Input.csv'\n",
    "df_validation_input = pd.read_csv(csvFilevalidation_Input)\n",
    "csvFilevalidation_Output = 'Validation_Output.csv'\n",
    "df_validation_output = pd.read_csv(csvFilevalidation_Output)\n",
    "BS = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator = CustomImgGenSC(path_validation_input,path_validation_output,indlist_validation_input\\\n",
    "                                 ,df_validation_input,df_validation_output\\\n",
    "                                 ,H=256,W=256,onlyX=False,shuffle=False,BATCH_SIZE=BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "batch_size = 8\n",
    "N_TRAIN=len(indlist_train_input)\n",
    "N_VALIDATE=len(indlist_validation_input)\n",
    "# N_TEST=len(indlist_testing_input)\n",
    "    \n",
    "t_steps = math.ceil(N_TRAIN/batch_size)\n",
    "v_steps = math.ceil(N_VALIDATE/batch_size)\n",
    "# tt_steps = math.ceil(N_TEST/batch_size)\n",
    "\n",
    "min_lr=0.0001\n",
    "epochs=300\n",
    "LR_patience=20\n",
    "LR_factor=0.1\n",
    "stop_patience=50\n",
    "retrainFlag=False\n",
    "\n",
    "modelSave = saveFolder + '/' + modelName + '.h5'\n",
    "trainGraphSave = saveFolder + '/' + modelName+ '_training_plot.png'\n",
    "\n",
    "callbacks = [EarlyStopping(patience=stop_patience, verbose=1),\n",
    "                 ReduceLROnPlateau(factor=LR_factor, patience=LR_patience, min_lr=min_lr, verbose=1),\n",
    "                 ModelCheckpoint(modelSave, verbose=1, save_best_only=True, save_weights_only=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 256\n",
    "W = 256\n",
    "input_img = Input((H,W,1),name='img')\n",
    "model = utilModels.LVNET(input_img,k=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "img (InputLayer)                [(None, 256, 256, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 256, 256, 16) 160         img[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 256, 256, 16) 784         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 256, 256, 16) 784         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 256, 256, 16) 784         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d (DepthwiseConv (None, 256, 256, 16) 144         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_1 (DepthwiseCo (None, 256, 256, 16) 144         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 256, 256, 16) 784         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_4 (DepthwiseCo (None, 256, 256, 16) 144         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_5 (DepthwiseCo (None, 256, 256, 16) 144         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 256, 256, 16) 64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 256, 256, 16) 64          depthwise_conv2d[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256, 256, 16) 64          depthwise_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 256, 256, 16) 64          conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 256, 256, 16) 64          depthwise_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 256, 256, 16) 64          depthwise_conv2d_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 256, 256, 16) 784         batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 256, 256, 16) 0           batch_normalization[0][0]        \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 256, 256, 16) 784         batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 256, 256, 16) 0           batch_normalization_15[0][0]     \n",
      "                                                                 batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 256, 256, 16) 784         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 256, 256, 16) 256         add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 256, 256, 16) 784         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 256, 256, 16) 256         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 256, 256, 16) 64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 256, 256, 16) 64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 256, 256, 16) 64          conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 256, 256, 16) 64          conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 256, 256, 32) 0           batch_normalization_4[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 256, 256, 32) 0           batch_normalization_19[0][0]     \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 256, 256, 32) 128         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 256, 256, 32) 128         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 256, 256, 32) 128         batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 256, 256, 32) 128         batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 256, 256, 16) 512         batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 256, 256, 16) 512         batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256, 256, 16) 0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 256, 256, 16) 0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 128, 128, 16) 0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 128, 128, 16) 0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128, 128, 16) 64          max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 128, 128, 16) 64          max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 128, 128, 16) 784         batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 128, 128, 16) 784         batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 128, 128, 16) 784         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 128, 128, 16) 784         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128, 128, 16) 0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128, 128, 16) 0           conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128, 128, 32) 0           max_pooling2d[0][0]              \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 128, 128, 32) 0           max_pooling2d_2[0][0]            \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 128, 128, 32) 128         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 128, 128, 32) 128         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 128, 128, 32) 1024        batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 128, 128, 32) 1024        batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128, 128, 32) 0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 128, 128, 32) 0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 32)   0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 64, 64, 32)   0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 64, 64, 32)   3104        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 64, 64, 32)   3104        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 64, 64, 32)   3104        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 64, 64, 32)   3104        conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64, 64, 64)   0           max_pooling2d_1[0][0]            \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 64, 64, 64)   0           max_pooling2d_3[0][0]            \n",
      "                                                                 conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 64, 64, 32)   6176        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 64, 64, 32)   6176        concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 64, 64, 32)   3104        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 64, 64, 32)   3104        conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 64, 64, 96)   0           concatenate_2[0][0]              \n",
      "                                                                 conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 64, 64, 96)   0           concatenate_9[0][0]              \n",
      "                                                                 conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 32)   9248        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 64, 64, 32)   9248        concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 32)   3104        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 64, 64, 32)   3104        conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 64, 64, 64)   0           conv2d_15[0][0]                  \n",
      "                                                                 conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 64, 64, 64)   0           conv2d_35[0][0]                  \n",
      "                                                                 conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 64, 64, 96)   0           concatenate_4[0][0]              \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 64, 64, 96)   0           concatenate_11[0][0]             \n",
      "                                                                 conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 64, 64, 32)   9248        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 64, 64, 32)   9248        concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 64, 64, 32)   3104        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_2 (DepthwiseCo (None, 64, 64, 96)   864         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_3 (DepthwiseCo (None, 64, 64, 96)   864         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 64, 64, 32)   3104        conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_6 (DepthwiseCo (None, 64, 64, 96)   864         concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_7 (DepthwiseCo (None, 64, 64, 96)   864         concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 64, 64, 32)   128         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64, 64, 96)   384         depthwise_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 64, 64, 96)   384         depthwise_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 64, 64, 32)   128         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 64, 64, 96)   384         depthwise_conv2d_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 64, 64, 96)   384         depthwise_conv2d_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 64, 64, 32)   3104        batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64, 64, 96)   0           batch_normalization_9[0][0]      \n",
      "                                                                 batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 64, 64, 32)   3104        batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 64, 64, 96)   0           batch_normalization_24[0][0]     \n",
      "                                                                 batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 64, 64, 32)   3104        conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 64, 64, 32)   3072        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 64, 64, 32)   3104        conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 64, 64, 32)   3072        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 64, 64, 32)   128         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 64, 64, 32)   128         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 64, 64, 32)   128         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 64, 64, 32)   128         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 64, 64, 64)   0           batch_normalization_13[0][0]     \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 64, 64, 64)   0           batch_normalization_28[0][0]     \n",
      "                                                                 batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 64)   256         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 64, 64, 64)   256         concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 64, 64, 128)  0           batch_normalization_14[0][0]     \n",
      "                                                                 batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 64, 64, 128)  512         concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 64, 64, 64)   8192        batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 64, 64, 64)   0           conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 32, 32, 64)   0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 32, 32, 64)   256         max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 32, 32, 64)   12352       batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 32, 32, 64)   12352       conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 32, 32, 64)   0           conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 32, 32, 128)  0           max_pooling2d_4[0][0]            \n",
      "                                                                 dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 32, 32, 64)   24640       concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 32, 32, 64)   12352       conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_10 (DepthwiseC (None, 32, 32, 128)  1152        concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_11 (DepthwiseC (None, 32, 32, 128)  1152        concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 32, 32, 64)   256         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 32, 32, 64)   12352       batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 32, 32, 128)  0           batch_normalization_38[0][0]     \n",
      "                                                                 batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 32, 32, 64)   12352       conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 32, 32, 64)   8192        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 32, 32, 64)   256         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 32, 32, 64)   256         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 32, 32, 128)  0           batch_normalization_42[0][0]     \n",
      "                                                                 batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 32, 32, 128)  0           concatenate_15[0][0]             \n",
      "                                                                 concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 32, 32, 128)  512         add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 32, 32, 128)  512         batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 32, 32, 64)   24640       batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 32, 32, 64)   12352       conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 32, 32, 64)   0           conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 32, 32, 192)  0           batch_normalization_43[0][0]     \n",
      "                                                                 dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 64, 64, 32)   55328       concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 64, 64, 32)   128         conv2d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 128, 128, 16) 4624        batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 128, 128, 16) 64          conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 256, 256, 8)  1160        batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 256, 256, 8)  32          conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 256, 256, 64) 4672        batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 256, 256, 1)  65          conv2d_56[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 332,217\n",
      "Trainable params: 328,137\n",
      "Non-trainable params: 4,080\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dice_loss1(y_true, y_pred):\n",
    "#   y_true = tf.cast(y_true, tf.float64)\n",
    "#   y_pred = tf.math.sigmoid(y_pred)\n",
    "#   numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
    "#   denominator = tf.reduce_sum(y_true + y_pred)\n",
    "\n",
    "#   return 1 - numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "def generalized_dice_coefficient(y_true, y_pred):\n",
    "        smooth = 1.\n",
    "        y_true_f = K.flatten(y_true)\n",
    "        y_pred_f = K.flatten(y_pred)\n",
    "        intersection = K.sum(y_true_f * y_pred_f)\n",
    "        score = (2. * intersection + smooth) / (\n",
    "                    K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "        return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1 - generalized_dice_coefficient(y_true, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.4583 - acc: 0.8297\n",
      "Epoch 00001: val_loss improved from inf to 0.91606, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/LV_Net_Dice_Loss_without_papilary_V2/LV_Net_Dice_Loss_without_papilary_V2.h5\n",
      "181/181 [==============================] - 90s 496ms/step - loss: 0.4571 - acc: 0.8301 - val_loss: 0.9161 - val_acc: 0.8631\n",
      "Epoch 2/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.1531 - acc: 0.9635\n",
      "Epoch 00002: val_loss did not improve from 0.91606\n",
      "181/181 [==============================] - 53s 294ms/step - loss: 0.1530 - acc: 0.9635 - val_loss: 0.9922 - val_acc: 0.8631\n",
      "Epoch 3/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9744\n",
      "Epoch 00003: val_loss did not improve from 0.91606\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.1037 - acc: 0.9743 - val_loss: 1.0001 - val_acc: 0.8631\n",
      "Epoch 4/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.9774\n",
      "Epoch 00004: val_loss improved from 0.91606 to 0.20753, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/LV_Net_Dice_Loss_without_papilary_V2/LV_Net_Dice_Loss_without_papilary_V2.h5\n",
      "181/181 [==============================] - 54s 299ms/step - loss: 0.0895 - acc: 0.9774 - val_loss: 0.2075 - val_acc: 0.9522\n",
      "Epoch 5/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9789\n",
      "Epoch 00005: val_loss improved from 0.20753 to 0.16287, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/LV_Net_Dice_Loss_without_papilary_V2/LV_Net_Dice_Loss_without_papilary_V2.h5\n",
      "181/181 [==============================] - 54s 298ms/step - loss: 0.0824 - acc: 0.9789 - val_loss: 0.1629 - val_acc: 0.9571\n",
      "Epoch 6/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9802\n",
      "Epoch 00006: val_loss improved from 0.16287 to 0.15268, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/LV_Net_Dice_Loss_without_papilary_V2/LV_Net_Dice_Loss_without_papilary_V2.h5\n",
      "181/181 [==============================] - 54s 298ms/step - loss: 0.0770 - acc: 0.9802 - val_loss: 0.1527 - val_acc: 0.9586\n",
      "Epoch 7/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0730 - acc: 0.9812\n",
      "Epoch 00007: val_loss improved from 0.15268 to 0.14247, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/LV_Net_Dice_Loss_without_papilary_V2/LV_Net_Dice_Loss_without_papilary_V2.h5\n",
      "181/181 [==============================] - 54s 300ms/step - loss: 0.0730 - acc: 0.9812 - val_loss: 0.1425 - val_acc: 0.9617\n",
      "Epoch 8/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0704 - acc: 0.9818\n",
      "Epoch 00008: val_loss improved from 0.14247 to 0.14018, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/LV_Net_Dice_Loss_without_papilary_V2/LV_Net_Dice_Loss_without_papilary_V2.h5\n",
      "181/181 [==============================] - 54s 299ms/step - loss: 0.0704 - acc: 0.9818 - val_loss: 0.1402 - val_acc: 0.9628\n",
      "Epoch 9/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9821\n",
      "Epoch 00009: val_loss improved from 0.14018 to 0.13469, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/LV_Net_Dice_Loss_without_papilary_V2/LV_Net_Dice_Loss_without_papilary_V2.h5\n",
      "181/181 [==============================] - 54s 300ms/step - loss: 0.0691 - acc: 0.9821 - val_loss: 0.1347 - val_acc: 0.9643\n",
      "Epoch 10/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0676 - acc: 0.9824\n",
      "Epoch 00010: val_loss improved from 0.13469 to 0.13270, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/LV_Net_Dice_Loss_without_papilary_V2/LV_Net_Dice_Loss_without_papilary_V2.h5\n",
      "181/181 [==============================] - 54s 300ms/step - loss: 0.0676 - acc: 0.9825 - val_loss: 0.1327 - val_acc: 0.9648\n",
      "Epoch 11/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9828\n",
      "Epoch 00011: val_loss improved from 0.13270 to 0.12671, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/LV_Net_Dice_Loss_without_papilary_V2/LV_Net_Dice_Loss_without_papilary_V2.h5\n",
      "181/181 [==============================] - 54s 300ms/step - loss: 0.0663 - acc: 0.9828 - val_loss: 0.1267 - val_acc: 0.9664\n",
      "Epoch 12/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0652 - acc: 0.9830\n",
      "Epoch 00012: val_loss improved from 0.12671 to 0.12342, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/LV_Net_Dice_Loss_without_papilary_V2/LV_Net_Dice_Loss_without_papilary_V2.h5\n",
      "181/181 [==============================] - 54s 301ms/step - loss: 0.0652 - acc: 0.9830 - val_loss: 0.1234 - val_acc: 0.9675\n",
      "Epoch 13/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9833\n",
      "Epoch 00013: val_loss did not improve from 0.12342\n",
      "181/181 [==============================] - 54s 298ms/step - loss: 0.0639 - acc: 0.9833 - val_loss: 0.1241 - val_acc: 0.9675\n",
      "Epoch 14/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.9837\n",
      "Epoch 00014: val_loss improved from 0.12342 to 0.12276, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/LV_Net_Dice_Loss_without_papilary_V2/LV_Net_Dice_Loss_without_papilary_V2.h5\n",
      "181/181 [==============================] - 54s 300ms/step - loss: 0.0627 - acc: 0.9837 - val_loss: 0.1228 - val_acc: 0.9682\n",
      "Epoch 15/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0618 - acc: 0.9839\n",
      "Epoch 00015: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.0617 - acc: 0.9839 - val_loss: 0.1286 - val_acc: 0.9669\n",
      "Epoch 16/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9842\n",
      "Epoch 00016: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.0606 - acc: 0.9842 - val_loss: 0.1357 - val_acc: 0.9652\n",
      "Epoch 17/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9845\n",
      "Epoch 00017: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.0595 - acc: 0.9845 - val_loss: 0.1410 - val_acc: 0.9638\n",
      "Epoch 18/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0582 - acc: 0.9848\n",
      "Epoch 00018: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 296ms/step - loss: 0.0582 - acc: 0.9848 - val_loss: 0.1519 - val_acc: 0.9614\n",
      "Epoch 19/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9851\n",
      "Epoch 00019: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 296ms/step - loss: 0.0571 - acc: 0.9851 - val_loss: 0.1508 - val_acc: 0.9610\n",
      "Epoch 20/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9855\n",
      "Epoch 00020: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 296ms/step - loss: 0.0559 - acc: 0.9855 - val_loss: 0.1615 - val_acc: 0.9585\n",
      "Epoch 21/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9858\n",
      "Epoch 00021: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.0547 - acc: 0.9858 - val_loss: 0.1681 - val_acc: 0.9564\n",
      "Epoch 22/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9859\n",
      "Epoch 00022: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.0541 - acc: 0.9859 - val_loss: 0.1591 - val_acc: 0.9588\n",
      "Epoch 23/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9860\n",
      "Epoch 00023: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.0537 - acc: 0.9860 - val_loss: 0.1527 - val_acc: 0.9608\n",
      "Epoch 24/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0535 - acc: 0.9860\n",
      "Epoch 00024: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.0535 - acc: 0.9860 - val_loss: 0.1517 - val_acc: 0.9613\n",
      "Epoch 25/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9861\n",
      "Epoch 00025: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.0533 - acc: 0.9861 - val_loss: 0.1561 - val_acc: 0.9608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9860\n",
      "Epoch 00026: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 298ms/step - loss: 0.0534 - acc: 0.9860 - val_loss: 0.1521 - val_acc: 0.9626\n",
      "Epoch 27/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9862\n",
      "Epoch 00027: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 296ms/step - loss: 0.0527 - acc: 0.9862 - val_loss: 0.1486 - val_acc: 0.9639\n",
      "Epoch 28/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9862\n",
      "Epoch 00028: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.0524 - acc: 0.9862 - val_loss: 0.1482 - val_acc: 0.9642\n",
      "Epoch 29/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9864\n",
      "Epoch 00029: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.0518 - acc: 0.9864 - val_loss: 0.1472 - val_acc: 0.9644\n",
      "Epoch 30/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9865\n",
      "Epoch 00030: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 296ms/step - loss: 0.0513 - acc: 0.9865 - val_loss: 0.1419 - val_acc: 0.9652\n",
      "Epoch 31/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9867\n",
      "Epoch 00031: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.0506 - acc: 0.9867 - val_loss: 0.1419 - val_acc: 0.9648\n",
      "Epoch 32/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9869\n",
      "Epoch 00032: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.0498 - acc: 0.9869 - val_loss: 0.1401 - val_acc: 0.9648\n",
      "Epoch 33/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9872\n",
      "Epoch 00033: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 296ms/step - loss: 0.0489 - acc: 0.9872 - val_loss: 0.1363 - val_acc: 0.9659\n",
      "Epoch 34/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9873\n",
      "Epoch 00034: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.0484 - acc: 0.9873 - val_loss: 0.1372 - val_acc: 0.9651\n",
      "Epoch 35/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9874\n",
      "Epoch 00035: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 296ms/step - loss: 0.0480 - acc: 0.9874 - val_loss: 0.1381 - val_acc: 0.9647\n",
      "Epoch 36/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9875\n",
      "Epoch 00036: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 298ms/step - loss: 0.0474 - acc: 0.9875 - val_loss: 0.1401 - val_acc: 0.9643\n",
      "Epoch 37/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9875\n",
      "Epoch 00037: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 296ms/step - loss: 0.0475 - acc: 0.9875 - val_loss: 0.1407 - val_acc: 0.9640\n",
      "Epoch 38/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9875\n",
      "Epoch 00038: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.0475 - acc: 0.9875 - val_loss: 0.1340 - val_acc: 0.9658\n",
      "Epoch 39/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9875\n",
      "Epoch 00039: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 298ms/step - loss: 0.0472 - acc: 0.9875 - val_loss: 0.1312 - val_acc: 0.9666\n",
      "Epoch 40/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9878\n",
      "Epoch 00040: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.0462 - acc: 0.9878 - val_loss: 0.1354 - val_acc: 0.9660\n",
      "Epoch 41/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9879\n",
      "Epoch 00041: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.0457 - acc: 0.9879 - val_loss: 0.1374 - val_acc: 0.9656\n",
      "Epoch 42/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9880\n",
      "Epoch 00042: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.0455 - acc: 0.9880 - val_loss: 0.1332 - val_acc: 0.9663\n",
      "Epoch 43/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9880\n",
      "Epoch 00043: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.0453 - acc: 0.9880 - val_loss: 0.1340 - val_acc: 0.9661\n",
      "Epoch 44/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9881\n",
      "Epoch 00044: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.0451 - acc: 0.9881 - val_loss: 0.1299 - val_acc: 0.9668\n",
      "Epoch 45/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9881\n",
      "Epoch 00045: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.0449 - acc: 0.9881 - val_loss: 0.1335 - val_acc: 0.9663\n",
      "Epoch 46/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9882\n",
      "Epoch 00046: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.0444 - acc: 0.9883 - val_loss: 0.1366 - val_acc: 0.9662\n",
      "Epoch 47/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9879\n",
      "Epoch 00047: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.0453 - acc: 0.9880 - val_loss: 0.1325 - val_acc: 0.9666\n",
      "Epoch 48/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9879\n",
      "Epoch 00048: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.0456 - acc: 0.9879 - val_loss: 0.1332 - val_acc: 0.9667\n",
      "Epoch 49/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9880\n",
      "Epoch 00049: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.0453 - acc: 0.9880 - val_loss: 0.1384 - val_acc: 0.9655\n",
      "Epoch 50/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9881\n",
      "Epoch 00050: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.0448 - acc: 0.9881 - val_loss: 0.1333 - val_acc: 0.9671\n",
      "Epoch 51/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9882\n",
      "Epoch 00051: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 298ms/step - loss: 0.0445 - acc: 0.9882 - val_loss: 0.1374 - val_acc: 0.9657\n",
      "Epoch 52/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9884\n",
      "Epoch 00052: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.0435 - acc: 0.9884 - val_loss: 0.1397 - val_acc: 0.9654\n",
      "Epoch 53/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9887\n",
      "Epoch 00053: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.0425 - acc: 0.9887 - val_loss: 0.1318 - val_acc: 0.9669\n",
      "Epoch 54/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9889\n",
      "Epoch 00054: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.0419 - acc: 0.9889 - val_loss: 0.1343 - val_acc: 0.9665\n",
      "Epoch 55/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9891\n",
      "Epoch 00055: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 296ms/step - loss: 0.0411 - acc: 0.9891 - val_loss: 0.1334 - val_acc: 0.9669\n",
      "Epoch 56/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9891\n",
      "Epoch 00056: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 296ms/step - loss: 0.0413 - acc: 0.9891 - val_loss: 0.1326 - val_acc: 0.9668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9890\n",
      "Epoch 00057: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.0413 - acc: 0.9890 - val_loss: 0.1321 - val_acc: 0.9670\n",
      "Epoch 58/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9890\n",
      "Epoch 00058: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.0414 - acc: 0.9890 - val_loss: 0.1343 - val_acc: 0.9667\n",
      "Epoch 59/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9891\n",
      "Epoch 00059: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 298ms/step - loss: 0.0410 - acc: 0.9891 - val_loss: 0.1369 - val_acc: 0.9662\n",
      "Epoch 60/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9892\n",
      "Epoch 00060: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.0406 - acc: 0.9892 - val_loss: 0.1370 - val_acc: 0.9662\n",
      "Epoch 61/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9892\n",
      "Epoch 00061: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 298ms/step - loss: 0.0406 - acc: 0.9892 - val_loss: 0.1408 - val_acc: 0.9652\n",
      "Epoch 62/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9894\n",
      "Epoch 00062: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 298ms/step - loss: 0.0402 - acc: 0.9894 - val_loss: 0.1422 - val_acc: 0.9649\n",
      "Epoch 63/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9894\n",
      "Epoch 00063: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.0399 - acc: 0.9894 - val_loss: 0.1338 - val_acc: 0.9666\n",
      "Epoch 64/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9896\n",
      "Epoch 00064: val_loss did not improve from 0.12276\n",
      "181/181 [==============================] - 54s 297ms/step - loss: 0.0393 - acc: 0.9896 - val_loss: 0.1430 - val_acc: 0.9646\n",
      "Epoch 00064: early stopping\n"
     ]
    }
   ],
   "source": [
    "adam = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer=adam,loss= dice_loss, metrics=[\"accuracy\"]) \n",
    "results = model.fit_generator(train_generator, steps_per_epoch=t_steps,  epochs=epochs,use_multiprocessing=False, \n",
    "                                  workers=0,validation_data=validation_generator,validation_steps=v_steps,callbacks=callbacks, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Learning curve\")\n",
    "plt.plot(results.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(results.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot( np.argmin(results.history[\"val_loss\"]), np.min(results.history[\"val_loss\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"log_loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prediction Steps.. 43\n",
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "path_testing_input = \"/tank/data/navchetan/Lars_Annotated_Datasets/Testing_Input\"\n",
    "path_testing_output = \"/tank/data/navchetan/Lars_Annotated_Datasets/Testing_Output_Without_Papillary\"\n",
    "path = \"/tank/data/navchetan/Lars_Annotated_Datasets/\"\n",
    "os.chdir(path)\n",
    "indlist_testing_input = list(pd.read_csv('Testing_indices.csv')['Indices'].values)\n",
    "# indlist_validation_output = list(pd.read_csv('Training_indices.csv')['Indices'].values)\n",
    "csvFiletesting_Input = 'Testing_Input.csv'\n",
    "df_testing_input = pd.read_csv(csvFiletesting_Input)\n",
    "csvFiletesting_Output = 'Testing_Output.csv'\n",
    "df_testing_output = pd.read_csv(csvFiletesting_Output)\n",
    "BS = 8\n",
    "\n",
    "N_PRED=len(indlist_testing_input)\n",
    "pred_steps= math.ceil(N_PRED/batch_size)\n",
    "    \n",
    "\n",
    "\n",
    "pred_X_generator = CustomImgGenSC(path_testing_input,path_testing_output,indlist_testing_input\\\n",
    "                                 ,df_testing_input,df_testing_output\\\n",
    "                                 ,H=256,W=256,onlyX=True,shuffle=False,BATCH_SIZE=BS)\n",
    "    \n",
    "# path = \"/data/navchetan/Data/Testing\"\n",
    "# os.chdir(path)\n",
    "\n",
    "# Create and save result matrix batch by batch\n",
    "a = 0\n",
    "print('Running prediction Steps.. ' + str(pred_steps))\n",
    "count = 1\n",
    "for step in range(pred_steps): \n",
    "    pred_Y_predict = model.predict_generator(pred_X_generator,steps=1,use_multiprocessing=False,workers=0)\n",
    "    print(step,end=\" \")\n",
    "    b = np.shape(pred_Y_predict)[0]\n",
    "    short_lst = indlist_testing_input[a:a+b]\n",
    "    a+=b\n",
    "    pos = 0\n",
    "    os.chdir(path_results_save)\n",
    "    for i in short_lst:\n",
    "        S = pred_Y_predict[pos,:,:,0]\n",
    "        fsave = 'Segment_Output'+str(count).zfill(5)+'.mat'\n",
    "        sio.savemat(fsave, {'S':S})\n",
    "        pos+=1\n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Testing Data for computation of Jaccard Index\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "0.8749328733833562\n",
      "0.9516186196960335\n",
      "0.81745948161042\n",
      "0.9402946338318942\n",
      "0.6937481560477509\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "cols = ['sensitivity','specificity','accuracy','dice_score','Jaccard']\n",
    "df = pd.DataFrame(columns=cols)\n",
    "path_testing_ground_truth = \"/tank/data/navchetan/Lars_Annotated_Datasets/Testing_Output_Without_Papillary\"\n",
    "N_TESTING_SAMPLES = 342\n",
    "print('Loading Testing Data for computation of Jaccard Index')\n",
    "sensitivity_t = 0\n",
    "specificity_t = 0\n",
    "accuracy_t = 0\n",
    "dice_score_t = 0\n",
    "Jaccard_t = 0\n",
    "for i in range(N_TESTING_SAMPLES):\n",
    "    if(i%10 == 0):\n",
    "        print(i)\n",
    "    os.chdir(path_results_save)    \n",
    "    pathr = 'Segment_Output'+str(i+1).zfill(5)+'.mat'\n",
    "    x = sio.loadmat(pathr)\n",
    "    Segment_model = x['S']\n",
    "    Segment_model1 = Segment_model*255;\n",
    "    retval1, Segment_model_threshold = cv2.threshold(Segment_model1,100,255,cv2.THRESH_BINARY)\n",
    "    Segment_model_threshold1 = Segment_model_threshold/255.0\n",
    "    \n",
    "    os.chdir(path_testing_ground_truth)\n",
    "    path_t = 'Segment'+str(i+1).zfill(5)+'.mat'\n",
    "    y = sio.loadmat(path_t)\n",
    "    ground_truth_testing  = y['S']\n",
    "    ground_truth1 = ground_truth_testing*255;\n",
    "    retval2, ground_truth_testing_threshold = cv2.threshold(ground_truth1,0,255,cv2.THRESH_BINARY)\n",
    "    ground_truth_testing_threshold1 = ground_truth_testing_threshold/255.0\n",
    "\n",
    "    mcm = confusion_matrix(np.ndarray.flatten(Segment_model_threshold1),np.ndarray.flatten(ground_truth_testing_threshold1))\n",
    "    tn = mcm[0, 0]\n",
    "    fp = mcm[0, 1]\n",
    "    fn = mcm[1, 0]\n",
    "    tp = mcm[1, 1]\n",
    "    \n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    accuracy = (tp + tn) / (tp + tn + fn + fp)\n",
    "    dice_score = 2*tp / (2*tp + fp + fn)\n",
    "    Jaccard = dice_score / (2-dice_score)\n",
    "    \n",
    "    sensitivity_t = sensitivity_t + sensitivity\n",
    "    specificity_t = specificity_t + specificity\n",
    "    accuracy_t = accuracy_t + accuracy\n",
    "    dice_score_t = dice_score_t + dice_score\n",
    "    Jaccard_t = Jaccard_t + Jaccard\n",
    "    \n",
    "    \n",
    "    df = df.append(pd.Series([sensitivity,specificity,accuracy,dice_score,Jaccard],index=df.columns), ignore_index=True)\n",
    "\n",
    "os.chdir(saveFolder)\n",
    "df.to_csv(saving_metrics,index=False)\n",
    "print(sensitivity_t/N_TESTING_SAMPLES)\n",
    "print(specificity_t/N_TESTING_SAMPLES)\n",
    "print(dice_score_t/N_TESTING_SAMPLES)\n",
    "print(accuracy_t/N_TESTING_SAMPLES)\n",
    "print(Jaccard_t/N_TESTING_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.056814217083012965\n",
      "0.02075258808800257\n",
      "0.04564920973939403\n",
      "0.01918287796133273\n",
      "0.06419094127680323\n"
     ]
    }
   ],
   "source": [
    "print(np.std(df.sensitivity))\n",
    "print(np.std(df.specificity))\n",
    "print(np.std(df.dice_score))\n",
    "print(np.std(df.accuracy))\n",
    "print(np.std(df.Jaccard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n"
     ]
    }
   ],
   "source": [
    "O1 = np.zeros((N_TESTING_SAMPLES,256,256))\n",
    "O2 = np.zeros((N_TESTING_SAMPLES,256,256))\n",
    "Ground_Truth = np.zeros((N_TESTING_SAMPLES,256,256))\n",
    "Segmentation_from_model = np.zeros((N_TESTING_SAMPLES,256,256))\n",
    "\n",
    "path = \"/tank/data/navchetan/Lars_Annotated_Datasets/Testing_Input\"\n",
    "os.chdir(path)\n",
    "for i in range(N_TESTING_SAMPLES):\n",
    "    if(i%10 == 0):\n",
    "        print(i)\n",
    "    pathr = 'Image'+str(i+1).zfill(5)+'.mat'\n",
    "    x = sio.loadmat(pathr)\n",
    "    O1[i,:,:] = x['U']/255\n",
    "    O2[i,:,:] = x['U']/255\n",
    "    \n",
    "path = \"/tank/data/navchetan/Lars_Annotated_Datasets/Testing_Output_Without_Papillary\"\n",
    "os.chdir(path)\n",
    "for i in range(N_TESTING_SAMPLES):\n",
    "    if(i%10 == 0):\n",
    "        print(i)\n",
    "    pathr = 'Segment'+str(i+1).zfill(5)+'.mat'\n",
    "    x = sio.loadmat(pathr)\n",
    "    Ground_Truth[i,:,:] = x['S']\n",
    "    retval2, ground_truth_testing_threshold = cv2.threshold(Ground_Truth[i,:,:],0,255,cv2.THRESH_BINARY)\n",
    "    Ground_Truth[i,:,:] = ground_truth_testing_threshold/255.0\n",
    "    \n",
    "\n",
    "os.chdir(path_results_save)\n",
    "for i in range(N_TESTING_SAMPLES):\n",
    "    if(i%10 == 0):\n",
    "        print(i)\n",
    "    pathr = 'Segment_Output'+str(i+1).zfill(5)+'.mat'\n",
    "    x = sio.loadmat(pathr)\n",
    "    Segmentation_from_model[i,:,:] = x['S']\n",
    "    Segment_model1 = Segmentation_from_model[i,:,:]*255;\n",
    "    retval1, Segment_model_threshold = cv2.threshold(Segment_model1,100,255,cv2.THRESH_BINARY)\n",
    "    Segmentation_from_model[i,:,:] = Segment_model_threshold/255.0\n",
    " \n",
    "def concat_images(imga, imgb):\n",
    "    \"\"\"\n",
    "    Combines two color image ndarrays side-by-side.\n",
    "    \"\"\"\n",
    "    [ha,wa] = np.shape(imga)\n",
    "    [hb,wb] = np.shape(imgb)\n",
    "    max_height = np.max([ha, hb])\n",
    "    total_width = wa+wb\n",
    "    new_img = np.zeros(shape=(max_height, total_width))\n",
    "    new_img[:ha,:wa]=imga\n",
    "    new_img[:hb,wa:wa+wb]=imgb\n",
    "    return new_img\n",
    "\n",
    "\n",
    "h = 256\n",
    "w = 256\n",
    "img_array = []    \n",
    "for k in range(N_TESTING_SAMPLES):\n",
    "    print(k)\n",
    "    for i in range(h):\n",
    "        for j in range (w):\n",
    "            if Ground_Truth[k,i,j]>0:\n",
    "                O1[k,i,j]=Ground_Truth[k,i,j]\n",
    "            if Segmentation_from_model[k,i,j]>0:\n",
    "                O2[k,i,j]=Segmentation_from_model[k,i,j]\n",
    "    W1 = O1[k,:,:]\n",
    "    W2 = O2[k,:,:]\n",
    "    img = concat_images(W1,W2)\n",
    "    img_array.append(img)\n",
    "    \n",
    "    os.chdir(path_images_save) \n",
    "    pathr = 'Joint_Image'+str(k+1).zfill(5)+'.png'\n",
    "    cv2.imwrite(pathr, img*255) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
