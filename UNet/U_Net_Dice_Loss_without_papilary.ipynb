{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, BatchNormalization, UpSampling2D, concatenate\n",
    "# from tensorflow.keras.utils import np_utils\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras import backend as Ks\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,CSVLogger\n",
    "import os\n",
    "import utilModels\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = 'U_Net_Dice_Loss_without_papilary'\n",
    "name_save_directory = \"U_Net_Dice_Loss_without_papilary\"\n",
    "saving_metrics = 'Metrics_U_Net_Dice_Loss_without_papilary.csv'\n",
    "\n",
    "\n",
    "results = \"_Results\"\n",
    "images = \"_Joint_Images\"\n",
    "parent_directory = \"/tank/data/navchetan/Lars_Annotated_Datasets/Results/\"\n",
    "saveFolder = os.path.join(parent_directory,name_save_directory)\n",
    "os.mkdir(saveFolder)\n",
    "\n",
    "name_save_results_directory = name_save_directory+results\n",
    "path_results_save = os.path.join(saveFolder,name_save_results_directory)\n",
    "os.mkdir(path_results_save)\n",
    "\n",
    "name_save_images_directory = name_save_directory+images\n",
    "path_images_save = os.path.join(saveFolder,name_save_images_directory)\n",
    "os.mkdir(path_images_save)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customLoss(yTrue,yPred):\n",
    "    return (1e4)*(Ks.mean(Ks.square(yPred - yTrue), axis=-1))\n",
    "\n",
    "\n",
    "IMAGE_WIDTH = 256\n",
    "IMAGE_HEIGHT = 256\n",
    "N_TRAIN_SAMPLES = 1445\n",
    "CHANNELS = 1\n",
    "N_EVALUATE_SAMPLES = 475\n",
    "N_TESTING_SAMPLES = 342\n",
    "EVALUATE_FROM = 1\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.0001\n",
    "F = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomImgGenSC(path_train_input,path_train_output,indlst_train,\\\n",
    "                   df_train_input,df_train_output,\\\n",
    "                   H=256,W=256,onlyX=False,shuffle=False,BATCH_SIZE=16):\n",
    "    L_train = len(indlst_train)\n",
    "    while True:\n",
    "        if(shuffle):\n",
    "            random.shuffle(indlst_train)\n",
    "        ii = 0 # Current image index\n",
    "        left = L_train\n",
    "        while left>0:\n",
    "            BL = min(BATCH_SIZE,left)\n",
    "            X_BATCH = np.zeros((BL,H,W,1))\n",
    "            Y_BATCH = np.zeros((BL,H,W,1))\n",
    "            for bi in range(BL):\n",
    "                os.chdir(path_train_input)\n",
    "                imgIdx = indlst_train[ii] \n",
    "                pathr = 'Image'+str(imgIdx).zfill(5)+'.mat'\n",
    "                x = sio.loadmat(pathr)\n",
    "                X_BATCH[bi,:,:,0] = x['U']/255.0           \n",
    "                \n",
    "                if(not onlyX):\n",
    "                    os.chdir(path_train_output)\n",
    "                    pathr = 'Segment'+str(imgIdx).zfill(5)+'.mat'\n",
    "                    x = sio.loadmat(pathr)\n",
    "                    Y_BATCH[bi,:,:,0] = x['S']/255.0\n",
    "                ii+=1\n",
    "                \n",
    "            left = left - BL\n",
    "            if(not onlyX):\n",
    "                yield (X_BATCH,Y_BATCH)\n",
    "            else:\n",
    "                yield X_BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_input = \"/tank/data/navchetan/Lars_Annotated_Datasets/Training_Input\"\n",
    "path_train_output = \"/tank/data/navchetan/Lars_Annotated_Datasets/Training_Output_Without_Papillary\"\n",
    "path = \"/tank/data/navchetan/Lars_Annotated_Datasets/\"\n",
    "os.chdir(path)\n",
    "indlist_train_input = list(pd.read_csv('Training_indices.csv')['Indices'].values)\n",
    "csvFileTrain_Input = 'Training_Input.csv'\n",
    "df_train_input = pd.read_csv(csvFileTrain_Input)\n",
    "csvFileTrain_Output = 'Training_Output.csv'\n",
    "df_train_output = pd.read_csv(csvFileTrain_Output)\n",
    "BS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = CustomImgGenSC(path_train_input,path_train_output,indlist_train_input\\\n",
    "                                 ,df_train_input,df_train_output\\\n",
    "                                 ,H=256,W=256,onlyX=False,shuffle=False,BATCH_SIZE=BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_validation_input = \"/tank/data/navchetan/Lars_Annotated_Datasets/Validation_Input\"\n",
    "path_validation_output = \"/tank/data/navchetan/Lars_Annotated_Datasets/Validation_Output_Without_Papillary\"\n",
    "path = \"/tank/data/navchetan/Lars_Annotated_Datasets/\"\n",
    "os.chdir(path)\n",
    "indlist_validation_input = list(pd.read_csv('Validation_indices.csv')['Indices'].values)\n",
    "# indlist_validation_output = list(pd.read_csv('Training_indices.csv')['Indices'].values)\n",
    "csvFilevalidation_Input = 'Validation_Input.csv'\n",
    "df_validation_input = pd.read_csv(csvFilevalidation_Input)\n",
    "csvFilevalidation_Output = 'Validation_Output.csv'\n",
    "df_validation_output = pd.read_csv(csvFilevalidation_Output)\n",
    "BS = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator = CustomImgGenSC(path_validation_input,path_validation_output,indlist_validation_input\\\n",
    "                                 ,df_validation_input,df_validation_output\\\n",
    "                                 ,H=256,W=256,onlyX=False,shuffle=False,BATCH_SIZE=BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "batch_size = 8\n",
    "N_TRAIN=len(indlist_train_input)\n",
    "N_VALIDATE=len(indlist_validation_input)\n",
    "# N_TEST=len(indlist_testing_input)\n",
    "    \n",
    "t_steps = math.ceil(N_TRAIN/batch_size)\n",
    "v_steps = math.ceil(N_VALIDATE/batch_size)\n",
    "# tt_steps = math.ceil(N_TEST/batch_size)\n",
    "\n",
    "min_lr=0.0001\n",
    "epochs=300\n",
    "LR_patience=20\n",
    "LR_factor=0.1\n",
    "stop_patience=50\n",
    "retrainFlag=False\n",
    "\n",
    "modelSave = saveFolder + '/' + modelName + '.h5'\n",
    "trainGraphSave = saveFolder + '/' + modelName+ '_training_plot.png'\n",
    "\n",
    "callbacks = [EarlyStopping(patience=stop_patience, verbose=1),\n",
    "                 ReduceLROnPlateau(factor=LR_factor, patience=LR_patience, min_lr=min_lr, verbose=1),\n",
    "                 ModelCheckpoint(modelSave, verbose=1, save_best_only=True, save_weights_only=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/navchetan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "H = 256\n",
    "W = 256\n",
    "input_img = Input((H,W,1),name='img')\n",
    "model = utilModels.get_unet_large(input_img, n_filters = 32, dropout = 0.0, batchnorm = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 256, 256, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 256, 256, 32) 320         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 256, 256, 32) 9248        conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 256, 256, 32) 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 256, 256, 32) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 128, 128, 32) 0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 128, 128, 32) 0           max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 128, 128, 64) 18496       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 128, 128, 64) 36928       conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 64) 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 64) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 64)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64, 64, 64)   0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 64, 128)  73856       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 64, 128)  147584      conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 128)  512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 128)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 128)  0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32, 32, 128)  0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 256)  295168      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 256)  590080      conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 256)  1024        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 256)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 256)  0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 16, 16, 256)  0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 512)  1180160     dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 512)  2359808     conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 16, 16, 512)  2048        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 16, 16, 512)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 32, 32, 512)  0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 256)  524544      up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32, 32, 512)  0           conv2d_10[0][0]                  \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32, 32, 512)  0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 256)  1179904     dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 256)  590080      conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 256)  1024        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 256)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 256)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 64, 64, 128)  131200      up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64, 64, 256)  0           conv2d_13[0][0]                  \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 64, 64, 256)  0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 128)  295040      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 128)  147584      conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64, 64, 128)  512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 64, 64, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 128, 128, 128 0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 64) 32832       up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128, 128, 128 0           conv2d_16[0][0]                  \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 128, 128, 128 0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 64) 73792       dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 128, 64) 36928       conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128, 128, 64) 256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 128, 128, 64) 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 256, 256, 64) 0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 256, 256, 32) 8224        up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 256, 256, 64) 0           conv2d_19[0][0]                  \n",
      "                                                                 activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 256, 256, 64) 0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 256, 256, 32) 18464       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 256, 256, 32) 9248        conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 256, 256, 32) 128         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 256, 256, 32) 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 256, 256, 1)  33          activation_8[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 7,765,409\n",
      "Trainable params: 7,762,465\n",
      "Non-trainable params: 2,944\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dice_loss1(y_true, y_pred):\n",
    "#   y_true = tf.cast(y_true, tf.float64)\n",
    "#   y_pred = tf.math.sigmoid(y_pred)\n",
    "#   numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
    "#   denominator = tf.reduce_sum(y_true + y_pred)\n",
    "\n",
    "#   return 1 - numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "def generalized_dice_coefficient(y_true, y_pred):\n",
    "        smooth = 1.\n",
    "        y_true_f = K.flatten(y_true)\n",
    "        y_pred_f = K.flatten(y_pred)\n",
    "        intersection = K.sum(y_true_f * y_pred_f)\n",
    "        score = (2. * intersection + smooth) / (\n",
    "                    K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "        return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1 - generalized_dice_coefficient(y_true, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.4157 - acc: 0.9216\n",
      "Epoch 00001: val_loss improved from inf to 0.46836, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_without_papilary/U_Net_Dice_Loss_without_papilary.h5\n",
      "181/181 [==============================] - 40s 218ms/step - loss: 0.4155 - acc: 0.9218 - val_loss: 0.4684 - val_acc: 0.9378\n",
      "Epoch 2/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.3281 - acc: 0.9687\n",
      "Epoch 00002: val_loss improved from 0.46836 to 0.37086, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_without_papilary/U_Net_Dice_Loss_without_papilary.h5\n",
      "181/181 [==============================] - 26s 142ms/step - loss: 0.3280 - acc: 0.9687 - val_loss: 0.3709 - val_acc: 0.9534\n",
      "Epoch 3/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.2808 - acc: 0.9738\n",
      "Epoch 00003: val_loss improved from 0.37086 to 0.33587, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_without_papilary/U_Net_Dice_Loss_without_papilary.h5\n",
      "181/181 [==============================] - 26s 143ms/step - loss: 0.2807 - acc: 0.9738 - val_loss: 0.3359 - val_acc: 0.9577\n",
      "Epoch 4/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.2411 - acc: 0.9764\n",
      "Epoch 00004: val_loss improved from 0.33587 to 0.29664, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_without_papilary/U_Net_Dice_Loss_without_papilary.h5\n",
      "181/181 [==============================] - 27s 147ms/step - loss: 0.2410 - acc: 0.9764 - val_loss: 0.2966 - val_acc: 0.9600\n",
      "Epoch 5/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.2069 - acc: 0.9778\n",
      "Epoch 00005: val_loss improved from 0.29664 to 0.24864, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_without_papilary/U_Net_Dice_Loss_without_papilary.h5\n",
      "181/181 [==============================] - 28s 156ms/step - loss: 0.2068 - acc: 0.9778 - val_loss: 0.2486 - val_acc: 0.9658\n",
      "Epoch 6/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.1779 - acc: 0.9792\n",
      "Epoch 00006: val_loss improved from 0.24864 to 0.21327, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_without_papilary/U_Net_Dice_Loss_without_papilary.h5\n",
      "181/181 [==============================] - 33s 181ms/step - loss: 0.1779 - acc: 0.9792 - val_loss: 0.2133 - val_acc: 0.9689\n",
      "Epoch 7/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.1553 - acc: 0.9800\n",
      "Epoch 00007: val_loss improved from 0.21327 to 0.18169, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_without_papilary/U_Net_Dice_Loss_without_papilary.h5\n",
      "181/181 [==============================] - 35s 196ms/step - loss: 0.1553 - acc: 0.9800 - val_loss: 0.1817 - val_acc: 0.9719\n",
      "Epoch 8/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.9806\n",
      "Epoch 00008: val_loss improved from 0.18169 to 0.16474, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_without_papilary/U_Net_Dice_Loss_without_papilary.h5\n",
      "181/181 [==============================] - 37s 202ms/step - loss: 0.1373 - acc: 0.9806 - val_loss: 0.1647 - val_acc: 0.9726\n",
      "Epoch 9/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.1229 - acc: 0.9811\n",
      "Epoch 00009: val_loss improved from 0.16474 to 0.15975, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_without_papilary/U_Net_Dice_Loss_without_papilary.h5\n",
      "181/181 [==============================] - 37s 205ms/step - loss: 0.1229 - acc: 0.9811 - val_loss: 0.1598 - val_acc: 0.9711\n",
      "Epoch 10/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.1109 - acc: 0.9817\n",
      "Epoch 00010: val_loss improved from 0.15975 to 0.14916, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_without_papilary/U_Net_Dice_Loss_without_papilary.h5\n",
      "181/181 [==============================] - 37s 207ms/step - loss: 0.1108 - acc: 0.9817 - val_loss: 0.1492 - val_acc: 0.9717\n",
      "Epoch 11/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.1016 - acc: 0.9820\n",
      "Epoch 00011: val_loss improved from 0.14916 to 0.13389, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_without_papilary/U_Net_Dice_Loss_without_papilary.h5\n",
      "181/181 [==============================] - 38s 209ms/step - loss: 0.1015 - acc: 0.9820 - val_loss: 0.1339 - val_acc: 0.9743\n",
      "Epoch 12/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0941 - acc: 0.9823\n",
      "Epoch 00012: val_loss did not improve from 0.13389\n",
      "181/181 [==============================] - 38s 209ms/step - loss: 0.0941 - acc: 0.9823 - val_loss: 0.1436 - val_acc: 0.9708\n",
      "Epoch 13/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0875 - acc: 0.9827\n",
      "Epoch 00013: val_loss did not improve from 0.13389\n",
      "181/181 [==============================] - 38s 210ms/step - loss: 0.0875 - acc: 0.9827 - val_loss: 0.1457 - val_acc: 0.9691\n",
      "Epoch 14/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9827\n",
      "Epoch 00014: val_loss did not improve from 0.13389\n",
      "181/181 [==============================] - 38s 212ms/step - loss: 0.0828 - acc: 0.9827 - val_loss: 0.1410 - val_acc: 0.9672\n",
      "Epoch 15/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9828\n",
      "Epoch 00015: val_loss did not improve from 0.13389\n",
      "181/181 [==============================] - 38s 212ms/step - loss: 0.0791 - acc: 0.9828 - val_loss: 0.1502 - val_acc: 0.9642\n",
      "Epoch 16/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9828\n",
      "Epoch 00016: val_loss did not improve from 0.13389\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0760 - acc: 0.9828 - val_loss: 0.1428 - val_acc: 0.9660\n",
      "Epoch 17/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9832\n",
      "Epoch 00017: val_loss did not improve from 0.13389\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0723 - acc: 0.9832 - val_loss: 0.1432 - val_acc: 0.9656\n",
      "Epoch 18/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9836\n",
      "Epoch 00018: val_loss did not improve from 0.13389\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0689 - acc: 0.9836 - val_loss: 0.1355 - val_acc: 0.9674\n",
      "Epoch 19/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0662 - acc: 0.9839\n",
      "Epoch 00019: val_loss improved from 0.13389 to 0.12259, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_without_papilary/U_Net_Dice_Loss_without_papilary.h5\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0661 - acc: 0.9839 - val_loss: 0.1226 - val_acc: 0.9708\n",
      "Epoch 20/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9843\n",
      "Epoch 00020: val_loss did not improve from 0.12259\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0634 - acc: 0.9843 - val_loss: 0.1322 - val_acc: 0.9669\n",
      "Epoch 21/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9844\n",
      "Epoch 00021: val_loss did not improve from 0.12259\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0616 - acc: 0.9844 - val_loss: 0.1451 - val_acc: 0.9621\n",
      "Epoch 22/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9847\n",
      "Epoch 00022: val_loss did not improve from 0.12259\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0596 - acc: 0.9847 - val_loss: 0.1307 - val_acc: 0.9684\n",
      "Epoch 23/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9848\n",
      "Epoch 00023: val_loss did not improve from 0.12259\n",
      "181/181 [==============================] - 39s 216ms/step - loss: 0.0584 - acc: 0.9848 - val_loss: 0.1237 - val_acc: 0.9700\n",
      "Epoch 24/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.9848\n",
      "Epoch 00024: val_loss did not improve from 0.12259\n",
      "181/181 [==============================] - 39s 216ms/step - loss: 0.0577 - acc: 0.9848 - val_loss: 0.1279 - val_acc: 0.9693\n",
      "Epoch 25/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/181 [============================>.] - ETA: 0s - loss: 0.0569 - acc: 0.9848\n",
      "Epoch 00025: val_loss did not improve from 0.12259\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0568 - acc: 0.9848 - val_loss: 0.1295 - val_acc: 0.9677\n",
      "Epoch 26/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9849\n",
      "Epoch 00026: val_loss did not improve from 0.12259\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0558 - acc: 0.9850 - val_loss: 0.1366 - val_acc: 0.9671\n",
      "Epoch 27/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9851\n",
      "Epoch 00027: val_loss did not improve from 0.12259\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0545 - acc: 0.9852 - val_loss: 0.1398 - val_acc: 0.9665\n",
      "Epoch 28/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9854\n",
      "Epoch 00028: val_loss improved from 0.12259 to 0.12057, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_without_papilary/U_Net_Dice_Loss_without_papilary.h5\n",
      "181/181 [==============================] - 39s 217ms/step - loss: 0.0533 - acc: 0.9854 - val_loss: 0.1206 - val_acc: 0.9703\n",
      "Epoch 29/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9855\n",
      "Epoch 00029: val_loss did not improve from 0.12057\n",
      "181/181 [==============================] - 39s 216ms/step - loss: 0.0526 - acc: 0.9855 - val_loss: 0.1251 - val_acc: 0.9685\n",
      "Epoch 30/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9854\n",
      "Epoch 00030: val_loss did not improve from 0.12057\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0523 - acc: 0.9855 - val_loss: 0.1474 - val_acc: 0.9646\n",
      "Epoch 31/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9857\n",
      "Epoch 00031: val_loss did not improve from 0.12057\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0511 - acc: 0.9857 - val_loss: 0.1274 - val_acc: 0.9690\n",
      "Epoch 32/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9859\n",
      "Epoch 00032: val_loss did not improve from 0.12057\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0502 - acc: 0.9859 - val_loss: 0.1334 - val_acc: 0.9673\n",
      "Epoch 33/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9860\n",
      "Epoch 00033: val_loss did not improve from 0.12057\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0495 - acc: 0.9860 - val_loss: 0.1569 - val_acc: 0.9630\n",
      "Epoch 34/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9860\n",
      "Epoch 00034: val_loss improved from 0.12057 to 0.11877, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_without_papilary/U_Net_Dice_Loss_without_papilary.h5\n",
      "181/181 [==============================] - 39s 217ms/step - loss: 0.0492 - acc: 0.9860 - val_loss: 0.1188 - val_acc: 0.9693\n",
      "Epoch 35/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9862\n",
      "Epoch 00035: val_loss improved from 0.11877 to 0.11606, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_without_papilary/U_Net_Dice_Loss_without_papilary.h5\n",
      "181/181 [==============================] - 39s 216ms/step - loss: 0.0485 - acc: 0.9862 - val_loss: 0.1161 - val_acc: 0.9698\n",
      "Epoch 36/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9863\n",
      "Epoch 00036: val_loss did not improve from 0.11606\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0476 - acc: 0.9864 - val_loss: 0.1212 - val_acc: 0.9691\n",
      "Epoch 37/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9864\n",
      "Epoch 00037: val_loss improved from 0.11606 to 0.10746, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_without_papilary/U_Net_Dice_Loss_without_papilary.h5\n",
      "181/181 [==============================] - 39s 216ms/step - loss: 0.0473 - acc: 0.9864 - val_loss: 0.1075 - val_acc: 0.9720\n",
      "Epoch 38/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9867\n",
      "Epoch 00038: val_loss improved from 0.10746 to 0.10508, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_without_papilary/U_Net_Dice_Loss_without_papilary.h5\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0462 - acc: 0.9867 - val_loss: 0.1051 - val_acc: 0.9720\n",
      "Epoch 39/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9866\n",
      "Epoch 00039: val_loss did not improve from 0.10508\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0463 - acc: 0.9866 - val_loss: 0.1455 - val_acc: 0.9646\n",
      "Epoch 40/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9867\n",
      "Epoch 00040: val_loss did not improve from 0.10508\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0458 - acc: 0.9868 - val_loss: 0.1077 - val_acc: 0.9712\n",
      "Epoch 41/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9869\n",
      "Epoch 00041: val_loss did not improve from 0.10508\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0450 - acc: 0.9870 - val_loss: 0.1101 - val_acc: 0.9710\n",
      "Epoch 42/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9872\n",
      "Epoch 00042: val_loss improved from 0.10508 to 0.10079, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_without_papilary/U_Net_Dice_Loss_without_papilary.h5\n",
      "181/181 [==============================] - 39s 216ms/step - loss: 0.0440 - acc: 0.9872 - val_loss: 0.1008 - val_acc: 0.9729\n",
      "Epoch 43/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9874\n",
      "Epoch 00043: val_loss did not improve from 0.10079\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0431 - acc: 0.9875 - val_loss: 0.1050 - val_acc: 0.9723\n",
      "Epoch 44/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9876\n",
      "Epoch 00044: val_loss improved from 0.10079 to 0.10029, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_without_papilary/U_Net_Dice_Loss_without_papilary.h5\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0424 - acc: 0.9877 - val_loss: 0.1003 - val_acc: 0.9733\n",
      "Epoch 45/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9878\n",
      "Epoch 00045: val_loss did not improve from 0.10029\n",
      "181/181 [==============================] - 38s 212ms/step - loss: 0.0418 - acc: 0.9878 - val_loss: 0.1097 - val_acc: 0.9713\n",
      "Epoch 46/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9879\n",
      "Epoch 00046: val_loss did not improve from 0.10029\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0415 - acc: 0.9879 - val_loss: 0.1303 - val_acc: 0.9673\n",
      "Epoch 47/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9881\n",
      "Epoch 00047: val_loss did not improve from 0.10029\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0405 - acc: 0.9882 - val_loss: 0.1213 - val_acc: 0.9686\n",
      "Epoch 48/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9882\n",
      "Epoch 00048: val_loss did not improve from 0.10029\n",
      "181/181 [==============================] - 39s 213ms/step - loss: 0.0403 - acc: 0.9882 - val_loss: 0.1035 - val_acc: 0.9718\n",
      "Epoch 49/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9883\n",
      "Epoch 00049: val_loss did not improve from 0.10029\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0399 - acc: 0.9883 - val_loss: 0.1087 - val_acc: 0.9719\n",
      "Epoch 50/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9885\n",
      "Epoch 00050: val_loss improved from 0.10029 to 0.09991, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_without_papilary/U_Net_Dice_Loss_without_papilary.h5\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0393 - acc: 0.9885 - val_loss: 0.0999 - val_acc: 0.9731\n",
      "Epoch 51/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9887\n",
      "Epoch 00051: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0385 - acc: 0.9887 - val_loss: 0.1134 - val_acc: 0.9701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9887\n",
      "Epoch 00052: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0383 - acc: 0.9887 - val_loss: 0.1035 - val_acc: 0.9721\n",
      "Epoch 53/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9890\n",
      "Epoch 00053: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0374 - acc: 0.9890 - val_loss: 0.1055 - val_acc: 0.9719\n",
      "Epoch 54/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9893\n",
      "Epoch 00054: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0365 - acc: 0.9893 - val_loss: 0.1348 - val_acc: 0.9659\n",
      "Epoch 55/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9894\n",
      "Epoch 00055: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0359 - acc: 0.9895 - val_loss: 0.1469 - val_acc: 0.9636\n",
      "Epoch 56/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9896\n",
      "Epoch 00056: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0353 - acc: 0.9896 - val_loss: 0.1016 - val_acc: 0.9721\n",
      "Epoch 57/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9896\n",
      "Epoch 00057: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0352 - acc: 0.9896 - val_loss: 0.1134 - val_acc: 0.9704\n",
      "Epoch 58/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9898\n",
      "Epoch 00058: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0347 - acc: 0.9898 - val_loss: 0.1167 - val_acc: 0.9695\n",
      "Epoch 59/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9899\n",
      "Epoch 00059: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0341 - acc: 0.9899 - val_loss: 0.1111 - val_acc: 0.9694\n",
      "Epoch 60/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9905\n",
      "Epoch 00060: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0323 - acc: 0.9905 - val_loss: 0.1301 - val_acc: 0.9668\n",
      "Epoch 61/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9908\n",
      "Epoch 00061: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0311 - acc: 0.9909 - val_loss: 0.1145 - val_acc: 0.9690\n",
      "Epoch 62/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9907\n",
      "Epoch 00062: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0315 - acc: 0.9907 - val_loss: 0.1071 - val_acc: 0.9710\n",
      "Epoch 63/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9909\n",
      "Epoch 00063: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0310 - acc: 0.9909 - val_loss: 0.1071 - val_acc: 0.9702\n",
      "Epoch 64/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9910\n",
      "Epoch 00064: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0307 - acc: 0.9910 - val_loss: 0.1195 - val_acc: 0.9680\n",
      "Epoch 65/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9913\n",
      "Epoch 00065: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 39s 216ms/step - loss: 0.0293 - acc: 0.9913 - val_loss: 0.1177 - val_acc: 0.9669\n",
      "Epoch 66/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9915\n",
      "Epoch 00066: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0289 - acc: 0.9915 - val_loss: 0.1209 - val_acc: 0.9686\n",
      "Epoch 67/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9916\n",
      "Epoch 00067: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0283 - acc: 0.9916 - val_loss: 0.1101 - val_acc: 0.9702\n",
      "Epoch 68/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9920\n",
      "Epoch 00068: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0270 - acc: 0.9920 - val_loss: 0.1429 - val_acc: 0.9636\n",
      "Epoch 69/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9921\n",
      "Epoch 00069: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 39s 213ms/step - loss: 0.0268 - acc: 0.9921 - val_loss: 0.1206 - val_acc: 0.9680\n",
      "Epoch 70/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9922\n",
      "Epoch 00070: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 39s 213ms/step - loss: 0.0263 - acc: 0.9923 - val_loss: 0.1144 - val_acc: 0.9686\n",
      "Epoch 71/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9924\n",
      "Epoch 00071: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 39s 213ms/step - loss: 0.0256 - acc: 0.9925 - val_loss: 0.1229 - val_acc: 0.9676\n",
      "Epoch 72/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9927\n",
      "Epoch 00072: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 39s 213ms/step - loss: 0.0246 - acc: 0.9927 - val_loss: 0.1414 - val_acc: 0.9640\n",
      "Epoch 73/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9927\n",
      "Epoch 00073: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0247 - acc: 0.9927 - val_loss: 0.1116 - val_acc: 0.9695\n",
      "Epoch 74/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9928\n",
      "Epoch 00074: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0245 - acc: 0.9928 - val_loss: 0.1154 - val_acc: 0.9685\n",
      "Epoch 75/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9931\n",
      "Epoch 00075: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 39s 213ms/step - loss: 0.0234 - acc: 0.9931 - val_loss: 0.1321 - val_acc: 0.9656\n",
      "Epoch 76/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9932\n",
      "Epoch 00076: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0230 - acc: 0.9932 - val_loss: 0.1404 - val_acc: 0.9648\n",
      "Epoch 77/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9935\n",
      "Epoch 00077: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0220 - acc: 0.9935 - val_loss: 0.1184 - val_acc: 0.9674\n",
      "Epoch 78/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9936\n",
      "Epoch 00078: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0216 - acc: 0.9936 - val_loss: 0.1239 - val_acc: 0.9674\n",
      "Epoch 79/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9934\n",
      "Epoch 00079: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 39s 213ms/step - loss: 0.0222 - acc: 0.9934 - val_loss: 0.1314 - val_acc: 0.9659\n",
      "Epoch 80/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9938\n",
      "Epoch 00080: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 39s 213ms/step - loss: 0.0210 - acc: 0.9938 - val_loss: 0.1293 - val_acc: 0.9655\n",
      "Epoch 81/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9938\n",
      "Epoch 00081: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0210 - acc: 0.9938 - val_loss: 0.1202 - val_acc: 0.9679\n",
      "Epoch 82/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9941\n",
      "Epoch 00082: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 39s 213ms/step - loss: 0.0198 - acc: 0.9941 - val_loss: 0.1496 - val_acc: 0.9628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9943\n",
      "Epoch 00083: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 38s 212ms/step - loss: 0.0195 - acc: 0.9943 - val_loss: 0.1104 - val_acc: 0.9701\n",
      "Epoch 84/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9942\n",
      "Epoch 00084: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 38s 212ms/step - loss: 0.0197 - acc: 0.9942 - val_loss: 0.1189 - val_acc: 0.9683\n",
      "Epoch 85/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9943\n",
      "Epoch 00085: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 38s 212ms/step - loss: 0.0193 - acc: 0.9943 - val_loss: 0.1304 - val_acc: 0.9654\n",
      "Epoch 86/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9943\n",
      "Epoch 00086: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 39s 213ms/step - loss: 0.0192 - acc: 0.9943 - val_loss: 0.1309 - val_acc: 0.9649\n",
      "Epoch 87/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9942\n",
      "Epoch 00087: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 38s 212ms/step - loss: 0.0195 - acc: 0.9942 - val_loss: 0.1235 - val_acc: 0.9665\n",
      "Epoch 88/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9942\n",
      "Epoch 00088: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 38s 212ms/step - loss: 0.0196 - acc: 0.9942 - val_loss: 0.1160 - val_acc: 0.9691\n",
      "Epoch 89/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9944\n",
      "Epoch 00089: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 38s 212ms/step - loss: 0.0190 - acc: 0.9944 - val_loss: 0.1385 - val_acc: 0.9651\n",
      "Epoch 90/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9945\n",
      "Epoch 00090: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 38s 212ms/step - loss: 0.0186 - acc: 0.9945 - val_loss: 0.1499 - val_acc: 0.9624\n",
      "Epoch 91/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9946\n",
      "Epoch 00091: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 38s 212ms/step - loss: 0.0183 - acc: 0.9946 - val_loss: 0.1751 - val_acc: 0.9577\n",
      "Epoch 92/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9949\n",
      "Epoch 00092: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 38s 213ms/step - loss: 0.0172 - acc: 0.9949 - val_loss: 0.1425 - val_acc: 0.9638\n",
      "Epoch 93/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9949\n",
      "Epoch 00093: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 38s 212ms/step - loss: 0.0173 - acc: 0.9949 - val_loss: 0.1300 - val_acc: 0.9662\n",
      "Epoch 94/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9948\n",
      "Epoch 00094: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 38s 212ms/step - loss: 0.0177 - acc: 0.9948 - val_loss: 0.1308 - val_acc: 0.9657\n",
      "Epoch 95/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9949\n",
      "Epoch 00095: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 38s 211ms/step - loss: 0.0171 - acc: 0.9949 - val_loss: 0.1624 - val_acc: 0.9599\n",
      "Epoch 96/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9950\n",
      "Epoch 00096: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 38s 209ms/step - loss: 0.0166 - acc: 0.9951 - val_loss: 0.1123 - val_acc: 0.9688\n",
      "Epoch 97/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9954\n",
      "Epoch 00097: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 37s 206ms/step - loss: 0.0156 - acc: 0.9954 - val_loss: 0.1110 - val_acc: 0.9699\n",
      "Epoch 98/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9956\n",
      "Epoch 00098: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 37s 204ms/step - loss: 0.0150 - acc: 0.9956 - val_loss: 0.1391 - val_acc: 0.9637\n",
      "Epoch 99/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9956\n",
      "Epoch 00099: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 37s 203ms/step - loss: 0.0149 - acc: 0.9956 - val_loss: 0.1394 - val_acc: 0.9637\n",
      "Epoch 100/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9958\n",
      "Epoch 00100: val_loss did not improve from 0.09991\n",
      "181/181 [==============================] - 37s 202ms/step - loss: 0.0144 - acc: 0.9958 - val_loss: 0.1381 - val_acc: 0.9648\n",
      "Epoch 00100: early stopping\n"
     ]
    }
   ],
   "source": [
    "adam = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer=adam,loss= dice_loss, metrics=[\"accuracy\"]) \n",
    "results = model.fit_generator(train_generator, steps_per_epoch=t_steps,  epochs=epochs,use_multiprocessing=False, \n",
    "                                  workers=0,validation_data=validation_generator,validation_steps=v_steps,callbacks=callbacks, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Learning curve\")\n",
    "plt.plot(results.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(results.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot( np.argmin(results.history[\"val_loss\"]), np.min(results.history[\"val_loss\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"log_loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prediction Steps.. 43\n",
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "path_testing_input = \"/tank/data/navchetan/Lars_Annotated_Datasets/Testing_Input\"\n",
    "path_testing_output = \"/tank/data/navchetan/Lars_Annotated_Datasets/Testing_Output_Without_Papillary\"\n",
    "path = \"/tank/data/navchetan/Lars_Annotated_Datasets/\"\n",
    "os.chdir(path)\n",
    "indlist_testing_input = list(pd.read_csv('Testing_indices.csv')['Indices'].values)\n",
    "# indlist_validation_output = list(pd.read_csv('Training_indices.csv')['Indices'].values)\n",
    "csvFiletesting_Input = 'Testing_Input.csv'\n",
    "df_testing_input = pd.read_csv(csvFiletesting_Input)\n",
    "csvFiletesting_Output = 'Testing_Output.csv'\n",
    "df_testing_output = pd.read_csv(csvFiletesting_Output)\n",
    "BS = 8\n",
    "\n",
    "N_PRED=len(indlist_testing_input)\n",
    "pred_steps= math.ceil(N_PRED/batch_size)\n",
    "    \n",
    "\n",
    "\n",
    "pred_X_generator = CustomImgGenSC(path_testing_input,path_testing_output,indlist_testing_input\\\n",
    "                                 ,df_testing_input,df_testing_output\\\n",
    "                                 ,H=256,W=256,onlyX=True,shuffle=False,BATCH_SIZE=BS)\n",
    "    \n",
    "# path = \"/data/navchetan/Data/Testing\"\n",
    "# os.chdir(path)\n",
    "\n",
    "# Create and save result matrix batch by batch\n",
    "a = 0\n",
    "print('Running prediction Steps.. ' + str(pred_steps))\n",
    "count = 1\n",
    "for step in range(pred_steps): \n",
    "    pred_Y_predict = model.predict_generator(pred_X_generator,steps=1,use_multiprocessing=False,workers=0)\n",
    "    print(step,end=\" \")\n",
    "    b = np.shape(pred_Y_predict)[0]\n",
    "    short_lst = indlist_testing_input[a:a+b]\n",
    "    a+=b\n",
    "    pos = 0\n",
    "    os.chdir(path_results_save)\n",
    "    for i in short_lst:\n",
    "        S = pred_Y_predict[pos,:,:,0]\n",
    "        fsave = 'Segment_Output'+str(count).zfill(5)+'.mat'\n",
    "        sio.savemat(fsave, {'S':S})\n",
    "        pos+=1\n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Testing Data for computation of Jaccard Index\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "0.9140461631798802\n",
      "0.9445377134811073\n",
      "0.8119727220192551\n",
      "0.9402606361790707\n",
      "0.6883467285056679\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "cols = ['sensitivity','specificity','accuracy','dice_score','Jaccard']\n",
    "df = pd.DataFrame(columns=cols)\n",
    "path_testing_ground_truth = \"/tank/data/navchetan/Lars_Annotated_Datasets/Testing_Output_Without_Papillary\"\n",
    "N_TESTING_SAMPLES = 342\n",
    "print('Loading Testing Data for computation of Jaccard Index')\n",
    "sensitivity_t = 0\n",
    "specificity_t = 0\n",
    "accuracy_t = 0\n",
    "dice_score_t = 0\n",
    "Jaccard_t = 0\n",
    "for i in range(N_TESTING_SAMPLES):\n",
    "    if(i%10 == 0):\n",
    "        print(i)\n",
    "    os.chdir(path_results_save)    \n",
    "    pathr = 'Segment_Output'+str(i+1).zfill(5)+'.mat'\n",
    "    x = sio.loadmat(pathr)\n",
    "    Segment_model = x['S']\n",
    "    Segment_model1 = Segment_model*255;\n",
    "    retval1, Segment_model_threshold = cv2.threshold(Segment_model1,100,255,cv2.THRESH_BINARY)\n",
    "    Segment_model_threshold1 = Segment_model_threshold/255.0\n",
    "    \n",
    "    os.chdir(path_testing_ground_truth)\n",
    "    path_t = 'Segment'+str(i+1).zfill(5)+'.mat'\n",
    "    y = sio.loadmat(path_t)\n",
    "    ground_truth_testing  = y['S']\n",
    "    ground_truth1 = ground_truth_testing*255;\n",
    "    retval2, ground_truth_testing_threshold = cv2.threshold(ground_truth1,0,255,cv2.THRESH_BINARY)\n",
    "    ground_truth_testing_threshold1 = ground_truth_testing_threshold/255.0\n",
    "\n",
    "    mcm = confusion_matrix(np.ndarray.flatten(Segment_model_threshold1),np.ndarray.flatten(ground_truth_testing_threshold1))\n",
    "    tn = mcm[0, 0]\n",
    "    fp = mcm[0, 1]\n",
    "    fn = mcm[1, 0]\n",
    "    tp = mcm[1, 1]\n",
    "    \n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    accuracy = (tp + tn) / (tp + tn + fn + fp)\n",
    "    dice_score = 2*tp / (2*tp + fp + fn)\n",
    "    Jaccard = dice_score / (2-dice_score)\n",
    "    \n",
    "    sensitivity_t = sensitivity_t + sensitivity\n",
    "    specificity_t = specificity_t + specificity\n",
    "    accuracy_t = accuracy_t + accuracy\n",
    "    dice_score_t = dice_score_t + dice_score\n",
    "    Jaccard_t = Jaccard_t + Jaccard\n",
    "    \n",
    "    \n",
    "    df = df.append(pd.Series([sensitivity,specificity,accuracy,dice_score,Jaccard],index=df.columns), ignore_index=True)\n",
    "\n",
    "os.chdir(saveFolder)\n",
    "df.to_csv(saving_metrics,index=False)\n",
    "print(sensitivity_t/N_TESTING_SAMPLES)\n",
    "print(specificity_t/N_TESTING_SAMPLES)\n",
    "print(dice_score_t/N_TESTING_SAMPLES)\n",
    "print(accuracy_t/N_TESTING_SAMPLES)\n",
    "print(Jaccard_t/N_TESTING_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n"
     ]
    }
   ],
   "source": [
    "O1 = np.zeros((N_TESTING_SAMPLES,256,256))\n",
    "O2 = np.zeros((N_TESTING_SAMPLES,256,256))\n",
    "Ground_Truth = np.zeros((N_TESTING_SAMPLES,256,256))\n",
    "Segmentation_from_model = np.zeros((N_TESTING_SAMPLES,256,256))\n",
    "\n",
    "path = \"/tank/data/navchetan/Lars_Annotated_Datasets/Testing_Input\"\n",
    "os.chdir(path)\n",
    "for i in range(N_TESTING_SAMPLES):\n",
    "    if(i%10 == 0):\n",
    "        print(i)\n",
    "    pathr = 'Image'+str(i+1).zfill(5)+'.mat'\n",
    "    x = sio.loadmat(pathr)\n",
    "    O1[i,:,:] = x['U']/255\n",
    "    O2[i,:,:] = x['U']/255\n",
    "    \n",
    "path = \"/tank/data/navchetan/Lars_Annotated_Datasets/Testing_Output_Without_Papillary\"\n",
    "os.chdir(path)\n",
    "for i in range(N_TESTING_SAMPLES):\n",
    "    if(i%10 == 0):\n",
    "        print(i)\n",
    "    pathr = 'Segment'+str(i+1).zfill(5)+'.mat'\n",
    "    x = sio.loadmat(pathr)\n",
    "    Ground_Truth[i,:,:] = x['S']\n",
    "    retval2, ground_truth_testing_threshold = cv2.threshold(Ground_Truth[i,:,:],0,255,cv2.THRESH_BINARY)\n",
    "    Ground_Truth[i,:,:] = ground_truth_testing_threshold/255.0\n",
    "    \n",
    "\n",
    "os.chdir(path_results_save)\n",
    "for i in range(N_TESTING_SAMPLES):\n",
    "    if(i%10 == 0):\n",
    "        print(i)\n",
    "    pathr = 'Segment_Output'+str(i+1).zfill(5)+'.mat'\n",
    "    x = sio.loadmat(pathr)\n",
    "    Segmentation_from_model[i,:,:] = x['S']\n",
    "    Segment_model1 = Segmentation_from_model[i,:,:]*255;\n",
    "    retval1, Segment_model_threshold = cv2.threshold(Segment_model1,100,255,cv2.THRESH_BINARY)\n",
    "    Segmentation_from_model[i,:,:] = Segment_model_threshold/255.0\n",
    " \n",
    "def concat_images(imga, imgb):\n",
    "    \"\"\"\n",
    "    Combines two color image ndarrays side-by-side.\n",
    "    \"\"\"\n",
    "    [ha,wa] = np.shape(imga)\n",
    "    [hb,wb] = np.shape(imgb)\n",
    "    max_height = np.max([ha, hb])\n",
    "    total_width = wa+wb\n",
    "    new_img = np.zeros(shape=(max_height, total_width))\n",
    "    new_img[:ha,:wa]=imga\n",
    "    new_img[:hb,wa:wa+wb]=imgb\n",
    "    return new_img\n",
    "\n",
    "\n",
    "h = 256\n",
    "w = 256\n",
    "img_array = []    \n",
    "for k in range(N_TESTING_SAMPLES):\n",
    "    print(k)\n",
    "    for i in range(h):\n",
    "        for j in range (w):\n",
    "            if Ground_Truth[k,i,j]>0:\n",
    "                O1[k,i,j]=Ground_Truth[k,i,j]\n",
    "            if Segmentation_from_model[k,i,j]>0:\n",
    "                O2[k,i,j]=Segmentation_from_model[k,i,j]\n",
    "    W1 = O1[k,:,:]\n",
    "    W2 = O2[k,:,:]\n",
    "    img = concat_images(W1,W2)\n",
    "    img_array.append(img)\n",
    "    \n",
    "    os.chdir(path_images_save) \n",
    "    pathr = 'Joint_Image'+str(k+1).zfill(5)+'.png'\n",
    "    cv2.imwrite(pathr, img*255) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
