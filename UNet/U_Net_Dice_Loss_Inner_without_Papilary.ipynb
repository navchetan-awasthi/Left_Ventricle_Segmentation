{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, BatchNormalization, UpSampling2D, concatenate\n",
    "# from tensorflow.keras.utils import np_utils\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras import backend as Ks\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,CSVLogger\n",
    "import os\n",
    "import utilModels\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = 'U_Net_Dice_Loss_Inner_without_Papilary'\n",
    "name_save_directory = \"U_Net_Dice_Loss_Inner_without_Papilary\"\n",
    "saving_metrics = 'Metrics_U_Net_Dice_Loss_Inner_without_Papilary.csv'\n",
    "\n",
    "\n",
    "results = \"_Results\"\n",
    "images = \"_Joint_Images\"\n",
    "parent_directory = \"/tank/data/navchetan/Lars_Annotated_Datasets/Results/\"\n",
    "saveFolder = os.path.join(parent_directory,name_save_directory)\n",
    "# os.mkdir(saveFolder)\n",
    "\n",
    "name_save_results_directory = name_save_directory+results\n",
    "path_results_save = os.path.join(saveFolder,name_save_results_directory)\n",
    "# os.mkdir(path_results_save)\n",
    "\n",
    "name_save_images_directory = name_save_directory+images\n",
    "path_images_save = os.path.join(saveFolder,name_save_images_directory)\n",
    "# os.mkdir(path_images_save)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customLoss(yTrue,yPred):\n",
    "    return (1e4)*(Ks.mean(Ks.square(yPred - yTrue), axis=-1))\n",
    "\n",
    "\n",
    "IMAGE_WIDTH = 256\n",
    "IMAGE_HEIGHT = 256\n",
    "N_TRAIN_SAMPLES = 1445\n",
    "CHANNELS = 1\n",
    "N_EVALUATE_SAMPLES = 475\n",
    "N_TESTING_SAMPLES = 342\n",
    "EVALUATE_FROM = 1\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.0001\n",
    "F = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomImgGenSC(path_train_input,path_train_output,indlst_train,\\\n",
    "                   df_train_input,df_train_output,\\\n",
    "                   H=256,W=256,onlyX=False,shuffle=False,BATCH_SIZE=16):\n",
    "    L_train = len(indlst_train)\n",
    "    while True:\n",
    "        if(shuffle):\n",
    "            random.shuffle(indlst_train)\n",
    "        ii = 0 # Current image index\n",
    "        left = L_train\n",
    "        while left>0:\n",
    "            BL = min(BATCH_SIZE,left)\n",
    "            X_BATCH = np.zeros((BL,H,W,1))\n",
    "            Y_BATCH = np.zeros((BL,H,W,1))\n",
    "            for bi in range(BL):\n",
    "                os.chdir(path_train_input)\n",
    "                imgIdx = indlst_train[ii] \n",
    "                pathr = 'Image'+str(imgIdx).zfill(5)+'.mat'\n",
    "                x = sio.loadmat(pathr)\n",
    "                X_BATCH[bi,:,:,0] = x['U']/255.0           \n",
    "                \n",
    "                if(not onlyX):\n",
    "                    os.chdir(path_train_output)\n",
    "                    pathr = 'Segment'+str(imgIdx).zfill(5)+'.mat'\n",
    "                    x = sio.loadmat(pathr)\n",
    "                    Y_BATCH[bi,:,:,0] = x['S']/255.0\n",
    "                ii+=1\n",
    "                \n",
    "            left = left - BL\n",
    "            if(not onlyX):\n",
    "                yield (X_BATCH,Y_BATCH)\n",
    "            else:\n",
    "                yield X_BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_input = \"/tank/data/navchetan/Lars_Annotated_Datasets/Training_Input\"\n",
    "path_train_output = \"/tank/data/navchetan/Lars_Annotated_Datasets/Training_Inner_Without_Papillary\"\n",
    "path = \"/tank/data/navchetan/Lars_Annotated_Datasets/\"\n",
    "os.chdir(path)\n",
    "indlist_train_input = list(pd.read_csv('Training_indices.csv')['Indices'].values)\n",
    "csvFileTrain_Input = 'Training_Input.csv'\n",
    "df_train_input = pd.read_csv(csvFileTrain_Input)\n",
    "csvFileTrain_Output = 'Training_Output.csv'\n",
    "df_train_output = pd.read_csv(csvFileTrain_Output)\n",
    "BS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = CustomImgGenSC(path_train_input,path_train_output,indlist_train_input\\\n",
    "                                 ,df_train_input,df_train_output\\\n",
    "                                 ,H=256,W=256,onlyX=False,shuffle=False,BATCH_SIZE=BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_validation_input = \"/tank/data/navchetan/Lars_Annotated_Datasets/Validation_Input\"\n",
    "path_validation_output = \"/tank/data/navchetan/Lars_Annotated_Datasets/Validation_Inner_Without_Papillary\"\n",
    "path = \"/tank/data/navchetan/Lars_Annotated_Datasets/\"\n",
    "os.chdir(path)\n",
    "indlist_validation_input = list(pd.read_csv('Validation_indices.csv')['Indices'].values)\n",
    "# indlist_validation_output = list(pd.read_csv('Training_indices.csv')['Indices'].values)\n",
    "csvFilevalidation_Input = 'Validation_Input.csv'\n",
    "df_validation_input = pd.read_csv(csvFilevalidation_Input)\n",
    "csvFilevalidation_Output = 'Validation_Output.csv'\n",
    "df_validation_output = pd.read_csv(csvFilevalidation_Output)\n",
    "BS = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator = CustomImgGenSC(path_validation_input,path_validation_output,indlist_validation_input\\\n",
    "                                 ,df_validation_input,df_validation_output\\\n",
    "                                 ,H=256,W=256,onlyX=False,shuffle=False,BATCH_SIZE=BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "batch_size = 8\n",
    "N_TRAIN=len(indlist_train_input)\n",
    "N_VALIDATE=len(indlist_validation_input)\n",
    "# N_TEST=len(indlist_testing_input)\n",
    "    \n",
    "t_steps = math.ceil(N_TRAIN/batch_size)\n",
    "v_steps = math.ceil(N_VALIDATE/batch_size)\n",
    "# tt_steps = math.ceil(N_TEST/batch_size)\n",
    "\n",
    "min_lr=0.0001\n",
    "epochs=300\n",
    "LR_patience=20\n",
    "LR_factor=0.1\n",
    "stop_patience=50\n",
    "retrainFlag=False\n",
    "\n",
    "modelSave = saveFolder + '/' + modelName + '.h5'\n",
    "trainGraphSave = saveFolder + '/' + modelName+ '_training_plot.png'\n",
    "\n",
    "callbacks = [EarlyStopping(patience=stop_patience, verbose=1),\n",
    "                 ReduceLROnPlateau(factor=LR_factor, patience=LR_patience, min_lr=min_lr, verbose=1),\n",
    "                 ModelCheckpoint(modelSave, verbose=1, save_best_only=True, save_weights_only=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/navchetan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "H = 256\n",
    "W = 256\n",
    "input_img = Input((H,W,1),name='img')\n",
    "model = utilModels.get_unet_large(input_img, n_filters = 32, dropout = 0.0, batchnorm = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 256, 256, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 256, 256, 32) 320         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 256, 256, 32) 9248        conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 256, 256, 32) 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 256, 256, 32) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 128, 128, 32) 0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 128, 128, 32) 0           max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 128, 128, 64) 18496       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 128, 128, 64) 36928       conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 64) 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 64) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 64)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64, 64, 64)   0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 64, 128)  73856       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 64, 128)  147584      conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 128)  512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 128)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 128)  0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32, 32, 128)  0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 256)  295168      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 256)  590080      conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 256)  1024        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 256)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 256)  0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 16, 16, 256)  0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 512)  1180160     dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 512)  2359808     conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 16, 16, 512)  2048        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 16, 16, 512)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 32, 32, 512)  0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 256)  524544      up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32, 32, 512)  0           conv2d_10[0][0]                  \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32, 32, 512)  0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 256)  1179904     dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 256)  590080      conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 256)  1024        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 256)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 256)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 64, 64, 128)  131200      up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64, 64, 256)  0           conv2d_13[0][0]                  \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 64, 64, 256)  0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 128)  295040      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 128)  147584      conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64, 64, 128)  512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 64, 64, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 128, 128, 128 0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 64) 32832       up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128, 128, 128 0           conv2d_16[0][0]                  \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 128, 128, 128 0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 64) 73792       dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 128, 64) 36928       conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128, 128, 64) 256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 128, 128, 64) 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 256, 256, 64) 0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 256, 256, 32) 8224        up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 256, 256, 64) 0           conv2d_19[0][0]                  \n",
      "                                                                 activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 256, 256, 64) 0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 256, 256, 32) 18464       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 256, 256, 32) 9248        conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 256, 256, 32) 128         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 256, 256, 32) 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 256, 256, 1)  33          activation_8[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 7,765,409\n",
      "Trainable params: 7,762,465\n",
      "Non-trainable params: 2,944\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dice_loss1(y_true, y_pred):\n",
    "#   y_true = tf.cast(y_true, tf.float64)\n",
    "#   y_pred = tf.math.sigmoid(y_pred)\n",
    "#   numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
    "#   denominator = tf.reduce_sum(y_true + y_pred)\n",
    "\n",
    "#   return 1 - numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "def generalized_dice_coefficient(y_true, y_pred):\n",
    "        smooth = 1.\n",
    "        y_true_f = K.flatten(y_true)\n",
    "        y_pred_f = K.flatten(y_pred)\n",
    "        intersection = K.sum(y_true_f * y_pred_f)\n",
    "        score = (2. * intersection + smooth) / (\n",
    "                    K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "        return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1 - generalized_dice_coefficient(y_true, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.5207 - acc: 0.9521\n",
      "Epoch 00001: val_loss improved from inf to 0.44232, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Inner_without_Papilary/U_Net_Dice_Loss_Inner_without_Papilary.h5\n",
      "181/181 [==============================] - 36s 197ms/step - loss: 0.5193 - acc: 0.9523 - val_loss: 0.4423 - val_acc: 0.9660\n",
      "Epoch 2/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.4173 - acc: 0.9869\n",
      "Epoch 00002: val_loss improved from 0.44232 to 0.37102, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Inner_without_Papilary/U_Net_Dice_Loss_Inner_without_Papilary.h5\n",
      "181/181 [==============================] - 25s 140ms/step - loss: 0.4162 - acc: 0.9870 - val_loss: 0.3710 - val_acc: 0.9855\n",
      "Epoch 3/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.3513 - acc: 0.9894\n",
      "Epoch 00003: val_loss improved from 0.37102 to 0.32030, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Inner_without_Papilary/U_Net_Dice_Loss_Inner_without_Papilary.h5\n",
      "181/181 [==============================] - 25s 140ms/step - loss: 0.3503 - acc: 0.9894 - val_loss: 0.3203 - val_acc: 0.9855\n",
      "Epoch 4/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.2946 - acc: 0.9905\n",
      "Epoch 00004: val_loss improved from 0.32030 to 0.25894, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Inner_without_Papilary/U_Net_Dice_Loss_Inner_without_Papilary.h5\n",
      "181/181 [==============================] - 26s 141ms/step - loss: 0.2938 - acc: 0.9905 - val_loss: 0.2589 - val_acc: 0.9859\n",
      "Epoch 5/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.2458 - acc: 0.9910\n",
      "Epoch 00005: val_loss improved from 0.25894 to 0.23374, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Inner_without_Papilary/U_Net_Dice_Loss_Inner_without_Papilary.h5\n",
      "181/181 [==============================] - 26s 142ms/step - loss: 0.2451 - acc: 0.9910 - val_loss: 0.2337 - val_acc: 0.9847\n",
      "Epoch 6/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.2028 - acc: 0.9919\n",
      "Epoch 00006: val_loss improved from 0.23374 to 0.19644, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Inner_without_Papilary/U_Net_Dice_Loss_Inner_without_Papilary.h5\n",
      "181/181 [==============================] - 27s 150ms/step - loss: 0.2022 - acc: 0.9919 - val_loss: 0.1964 - val_acc: 0.9874\n",
      "Epoch 7/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.1694 - acc: 0.9922\n",
      "Epoch 00007: val_loss improved from 0.19644 to 0.16642, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Inner_without_Papilary/U_Net_Dice_Loss_Inner_without_Papilary.h5\n",
      "181/181 [==============================] - 29s 158ms/step - loss: 0.1689 - acc: 0.9922 - val_loss: 0.1664 - val_acc: 0.9879\n",
      "Epoch 8/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.1430 - acc: 0.9925\n",
      "Epoch 00008: val_loss improved from 0.16642 to 0.14956, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Inner_without_Papilary/U_Net_Dice_Loss_Inner_without_Papilary.h5\n",
      "181/181 [==============================] - 31s 172ms/step - loss: 0.1426 - acc: 0.9925 - val_loss: 0.1496 - val_acc: 0.9870\n",
      "Epoch 9/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.1230 - acc: 0.9925\n",
      "Epoch 00009: val_loss improved from 0.14956 to 0.13408, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Inner_without_Papilary/U_Net_Dice_Loss_Inner_without_Papilary.h5\n",
      "181/181 [==============================] - 34s 186ms/step - loss: 0.1227 - acc: 0.9925 - val_loss: 0.1341 - val_acc: 0.9870\n",
      "Epoch 10/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.1080 - acc: 0.9927\n",
      "Epoch 00010: val_loss improved from 0.13408 to 0.12168, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Inner_without_Papilary/U_Net_Dice_Loss_Inner_without_Papilary.h5\n",
      "181/181 [==============================] - 35s 196ms/step - loss: 0.1077 - acc: 0.9927 - val_loss: 0.1217 - val_acc: 0.9875\n",
      "Epoch 11/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0956 - acc: 0.9929\n",
      "Epoch 00011: val_loss improved from 0.12168 to 0.10590, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Inner_without_Papilary/U_Net_Dice_Loss_Inner_without_Papilary.h5\n",
      "181/181 [==============================] - 36s 197ms/step - loss: 0.0953 - acc: 0.9928 - val_loss: 0.1059 - val_acc: 0.9884\n",
      "Epoch 12/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0860 - acc: 0.9930\n",
      "Epoch 00012: val_loss improved from 0.10590 to 0.09738, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Inner_without_Papilary/U_Net_Dice_Loss_Inner_without_Papilary.h5\n",
      "181/181 [==============================] - 36s 200ms/step - loss: 0.0858 - acc: 0.9930 - val_loss: 0.0974 - val_acc: 0.9887\n",
      "Epoch 13/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9930\n",
      "Epoch 00013: val_loss improved from 0.09738 to 0.09079, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Inner_without_Papilary/U_Net_Dice_Loss_Inner_without_Papilary.h5\n",
      "181/181 [==============================] - 37s 203ms/step - loss: 0.0783 - acc: 0.9930 - val_loss: 0.0908 - val_acc: 0.9888\n",
      "Epoch 14/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9931\n",
      "Epoch 00014: val_loss did not improve from 0.09079\n",
      "181/181 [==============================] - 37s 204ms/step - loss: 0.0723 - acc: 0.9931 - val_loss: 0.0994 - val_acc: 0.9870\n",
      "Epoch 15/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9932\n",
      "Epoch 00015: val_loss did not improve from 0.09079\n",
      "181/181 [==============================] - 37s 204ms/step - loss: 0.0668 - acc: 0.9932 - val_loss: 0.1269 - val_acc: 0.9825\n",
      "Epoch 16/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9933\n",
      "Epoch 00016: val_loss did not improve from 0.09079\n",
      "181/181 [==============================] - 37s 205ms/step - loss: 0.0626 - acc: 0.9933 - val_loss: 0.2897 - val_acc: 0.9547\n",
      "Epoch 17/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.9930\n",
      "Epoch 00017: val_loss did not improve from 0.09079\n",
      "181/181 [==============================] - 37s 206ms/step - loss: 0.0612 - acc: 0.9930 - val_loss: 0.1141 - val_acc: 0.9848\n",
      "Epoch 18/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9933\n",
      "Epoch 00018: val_loss did not improve from 0.09079\n",
      "181/181 [==============================] - 37s 206ms/step - loss: 0.0563 - acc: 0.9933 - val_loss: 0.1159 - val_acc: 0.9844\n",
      "Epoch 19/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9934\n",
      "Epoch 00019: val_loss did not improve from 0.09079\n",
      "181/181 [==============================] - 37s 206ms/step - loss: 0.0535 - acc: 0.9934 - val_loss: 0.1148 - val_acc: 0.9842\n",
      "Epoch 20/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9936\n",
      "Epoch 00020: val_loss improved from 0.09079 to 0.08528, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Inner_without_Papilary/U_Net_Dice_Loss_Inner_without_Papilary.h5\n",
      "181/181 [==============================] - 38s 208ms/step - loss: 0.0510 - acc: 0.9935 - val_loss: 0.0853 - val_acc: 0.9875\n",
      "Epoch 21/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9935\n",
      "Epoch 00021: val_loss improved from 0.08528 to 0.07883, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Inner_without_Papilary/U_Net_Dice_Loss_Inner_without_Papilary.h5\n",
      "181/181 [==============================] - 37s 207ms/step - loss: 0.0493 - acc: 0.9935 - val_loss: 0.0788 - val_acc: 0.9880\n",
      "Epoch 22/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9936\n",
      "Epoch 00022: val_loss did not improve from 0.07883\n",
      "181/181 [==============================] - 37s 206ms/step - loss: 0.0476 - acc: 0.9936 - val_loss: 0.0892 - val_acc: 0.9862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9936\n",
      "Epoch 00023: val_loss did not improve from 0.07883\n",
      "181/181 [==============================] - 37s 206ms/step - loss: 0.0463 - acc: 0.9936 - val_loss: 0.0826 - val_acc: 0.9873\n",
      "Epoch 24/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9937\n",
      "Epoch 00024: val_loss did not improve from 0.07883\n",
      "181/181 [==============================] - 37s 207ms/step - loss: 0.0451 - acc: 0.9937 - val_loss: 0.1077 - val_acc: 0.9838\n",
      "Epoch 25/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9938\n",
      "Epoch 00025: val_loss did not improve from 0.07883\n",
      "181/181 [==============================] - 37s 207ms/step - loss: 0.0437 - acc: 0.9937 - val_loss: 0.1055 - val_acc: 0.9842\n",
      "Epoch 26/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9940\n",
      "Epoch 00026: val_loss did not improve from 0.07883\n",
      "181/181 [==============================] - 37s 207ms/step - loss: 0.0415 - acc: 0.9940 - val_loss: 0.1280 - val_acc: 0.9808\n",
      "Epoch 27/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9940\n",
      "Epoch 00027: val_loss did not improve from 0.07883\n",
      "181/181 [==============================] - 38s 208ms/step - loss: 0.0406 - acc: 0.9940 - val_loss: 0.2323 - val_acc: 0.9717\n",
      "Epoch 28/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9941\n",
      "Epoch 00028: val_loss did not improve from 0.07883\n",
      "181/181 [==============================] - 37s 207ms/step - loss: 0.0395 - acc: 0.9941 - val_loss: 0.0924 - val_acc: 0.9857\n",
      "Epoch 29/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9941\n",
      "Epoch 00029: val_loss did not improve from 0.07883\n",
      "181/181 [==============================] - 37s 206ms/step - loss: 0.0392 - acc: 0.9941 - val_loss: 0.0894 - val_acc: 0.9857\n",
      "Epoch 30/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9943\n",
      "Epoch 00030: val_loss did not improve from 0.07883\n",
      "181/181 [==============================] - 37s 206ms/step - loss: 0.0372 - acc: 0.9943 - val_loss: 0.0879 - val_acc: 0.9857\n",
      "Epoch 31/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9943\n",
      "Epoch 00031: val_loss did not improve from 0.07883\n",
      "181/181 [==============================] - 37s 207ms/step - loss: 0.0367 - acc: 0.9943 - val_loss: 0.0851 - val_acc: 0.9860\n",
      "Epoch 32/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9942\n",
      "Epoch 00032: val_loss did not improve from 0.07883\n",
      "181/181 [==============================] - 37s 206ms/step - loss: 0.0372 - acc: 0.9942 - val_loss: 0.2130 - val_acc: 0.9718\n",
      "Epoch 33/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9943\n",
      "Epoch 00033: val_loss improved from 0.07883 to 0.07511, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Inner_without_Papilary/U_Net_Dice_Loss_Inner_without_Papilary.h5\n",
      "181/181 [==============================] - 37s 207ms/step - loss: 0.0359 - acc: 0.9943 - val_loss: 0.0751 - val_acc: 0.9874\n",
      "Epoch 34/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9944\n",
      "Epoch 00034: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 207ms/step - loss: 0.0355 - acc: 0.9944 - val_loss: 0.0801 - val_acc: 0.9865\n",
      "Epoch 35/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9941\n",
      "Epoch 00035: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 206ms/step - loss: 0.0361 - acc: 0.9941 - val_loss: 0.4376 - val_acc: 0.8499\n",
      "Epoch 36/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9942\n",
      "Epoch 00036: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 205ms/step - loss: 0.0361 - acc: 0.9942 - val_loss: 0.0764 - val_acc: 0.9872\n",
      "Epoch 37/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9945\n",
      "Epoch 00037: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 206ms/step - loss: 0.0340 - acc: 0.9945 - val_loss: 0.1379 - val_acc: 0.9800\n",
      "Epoch 38/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9945\n",
      "Epoch 00038: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 205ms/step - loss: 0.0335 - acc: 0.9945 - val_loss: 0.1462 - val_acc: 0.9779\n",
      "Epoch 39/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9947\n",
      "Epoch 00039: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 206ms/step - loss: 0.0326 - acc: 0.9947 - val_loss: 0.1448 - val_acc: 0.9780\n",
      "Epoch 40/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9947\n",
      "Epoch 00040: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 206ms/step - loss: 0.0320 - acc: 0.9947 - val_loss: 0.1377 - val_acc: 0.9796\n",
      "Epoch 41/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9947\n",
      "Epoch 00041: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 206ms/step - loss: 0.0321 - acc: 0.9947 - val_loss: 0.1020 - val_acc: 0.9834\n",
      "Epoch 42/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9948\n",
      "Epoch 00042: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 206ms/step - loss: 0.0317 - acc: 0.9948 - val_loss: 0.1394 - val_acc: 0.9789\n",
      "Epoch 43/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9949\n",
      "Epoch 00043: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 206ms/step - loss: 0.0309 - acc: 0.9949 - val_loss: 0.0907 - val_acc: 0.9844\n",
      "Epoch 44/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9951\n",
      "Epoch 00044: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 207ms/step - loss: 0.0296 - acc: 0.9950 - val_loss: 0.0761 - val_acc: 0.9866\n",
      "Epoch 45/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9951\n",
      "Epoch 00045: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 207ms/step - loss: 0.0294 - acc: 0.9951 - val_loss: 0.0794 - val_acc: 0.9862\n",
      "Epoch 46/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9951\n",
      "Epoch 00046: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 206ms/step - loss: 0.0294 - acc: 0.9951 - val_loss: 0.1253 - val_acc: 0.9789\n",
      "Epoch 47/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9952\n",
      "Epoch 00047: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 206ms/step - loss: 0.0284 - acc: 0.9952 - val_loss: 0.1187 - val_acc: 0.9808\n",
      "Epoch 48/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9954\n",
      "Epoch 00048: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 205ms/step - loss: 0.0276 - acc: 0.9954 - val_loss: 0.1533 - val_acc: 0.9769\n",
      "Epoch 49/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9954\n",
      "Epoch 00049: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 205ms/step - loss: 0.0273 - acc: 0.9954 - val_loss: 0.0980 - val_acc: 0.9837\n",
      "Epoch 50/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9954\n",
      "Epoch 00050: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 205ms/step - loss: 0.0274 - acc: 0.9954 - val_loss: 0.0967 - val_acc: 0.9842\n",
      "Epoch 51/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9955\n",
      "Epoch 00051: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 207ms/step - loss: 0.0263 - acc: 0.9956 - val_loss: 0.1032 - val_acc: 0.9825\n",
      "Epoch 52/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9956\n",
      "Epoch 00052: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 206ms/step - loss: 0.0259 - acc: 0.9956 - val_loss: 0.1101 - val_acc: 0.9822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9956\n",
      "Epoch 00053: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 207ms/step - loss: 0.0259 - acc: 0.9956 - val_loss: 0.1590 - val_acc: 0.9761\n",
      "Epoch 54/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9957\n",
      "Epoch 00054: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 206ms/step - loss: 0.0254 - acc: 0.9957 - val_loss: 0.0920 - val_acc: 0.9842\n",
      "Epoch 55/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9958\n",
      "Epoch 00055: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 205ms/step - loss: 0.0248 - acc: 0.9958 - val_loss: 0.1055 - val_acc: 0.9822\n",
      "Epoch 56/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9957\n",
      "Epoch 00056: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 205ms/step - loss: 0.0250 - acc: 0.9957 - val_loss: 0.1339 - val_acc: 0.9790\n",
      "Epoch 57/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9959\n",
      "Epoch 00057: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 205ms/step - loss: 0.0240 - acc: 0.9959 - val_loss: 0.1179 - val_acc: 0.9797\n",
      "Epoch 58/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9959\n",
      "Epoch 00058: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 206ms/step - loss: 0.0240 - acc: 0.9959 - val_loss: 0.1439 - val_acc: 0.9765\n",
      "Epoch 59/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9960\n",
      "Epoch 00059: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 205ms/step - loss: 0.0235 - acc: 0.9960 - val_loss: 0.1269 - val_acc: 0.9788\n",
      "Epoch 60/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9961\n",
      "Epoch 00060: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 205ms/step - loss: 0.0227 - acc: 0.9961 - val_loss: 0.0774 - val_acc: 0.9862\n",
      "Epoch 61/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9961\n",
      "Epoch 00061: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 204ms/step - loss: 0.0225 - acc: 0.9961 - val_loss: 0.1866 - val_acc: 0.9726\n",
      "Epoch 62/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9963\n",
      "Epoch 00062: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 204ms/step - loss: 0.0216 - acc: 0.9963 - val_loss: 0.0871 - val_acc: 0.9848\n",
      "Epoch 63/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9963\n",
      "Epoch 00063: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 203ms/step - loss: 0.0214 - acc: 0.9963 - val_loss: 0.1030 - val_acc: 0.9824\n",
      "Epoch 64/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9964\n",
      "Epoch 00064: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 203ms/step - loss: 0.0206 - acc: 0.9964 - val_loss: 0.1164 - val_acc: 0.9801\n",
      "Epoch 65/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9966\n",
      "Epoch 00065: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 202ms/step - loss: 0.0197 - acc: 0.9966 - val_loss: 0.1012 - val_acc: 0.9833\n",
      "Epoch 66/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9967\n",
      "Epoch 00066: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 204ms/step - loss: 0.0190 - acc: 0.9967 - val_loss: 0.0869 - val_acc: 0.9848\n",
      "Epoch 67/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9966\n",
      "Epoch 00067: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 204ms/step - loss: 0.0195 - acc: 0.9966 - val_loss: 0.1079 - val_acc: 0.9815\n",
      "Epoch 68/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9968\n",
      "Epoch 00068: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 204ms/step - loss: 0.0187 - acc: 0.9968 - val_loss: 0.0999 - val_acc: 0.9830\n",
      "Epoch 69/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9968\n",
      "Epoch 00069: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 203ms/step - loss: 0.0186 - acc: 0.9968 - val_loss: 0.0948 - val_acc: 0.9840\n",
      "Epoch 70/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9970\n",
      "Epoch 00070: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 202ms/step - loss: 0.0177 - acc: 0.9970 - val_loss: 0.0876 - val_acc: 0.9850\n",
      "Epoch 71/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9970\n",
      "Epoch 00071: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 202ms/step - loss: 0.0172 - acc: 0.9970 - val_loss: 0.1000 - val_acc: 0.9828\n",
      "Epoch 72/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9972\n",
      "Epoch 00072: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 202ms/step - loss: 0.0161 - acc: 0.9972 - val_loss: 0.0898 - val_acc: 0.9848\n",
      "Epoch 73/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9973\n",
      "Epoch 00073: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 202ms/step - loss: 0.0157 - acc: 0.9973 - val_loss: 0.0885 - val_acc: 0.9845\n",
      "Epoch 74/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9970\n",
      "Epoch 00074: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 202ms/step - loss: 0.0171 - acc: 0.9970 - val_loss: 0.1425 - val_acc: 0.9778\n",
      "Epoch 75/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9973\n",
      "Epoch 00075: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 36s 202ms/step - loss: 0.0153 - acc: 0.9973 - val_loss: 0.1086 - val_acc: 0.9823\n",
      "Epoch 76/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9974\n",
      "Epoch 00076: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 203ms/step - loss: 0.0149 - acc: 0.9974 - val_loss: 0.1304 - val_acc: 0.9786\n",
      "Epoch 77/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9974\n",
      "Epoch 00077: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 203ms/step - loss: 0.0146 - acc: 0.9974 - val_loss: 0.0797 - val_acc: 0.9860\n",
      "Epoch 78/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9974\n",
      "Epoch 00078: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 203ms/step - loss: 0.0149 - acc: 0.9974 - val_loss: 0.1248 - val_acc: 0.9795\n",
      "Epoch 79/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9975\n",
      "Epoch 00079: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 202ms/step - loss: 0.0143 - acc: 0.9975 - val_loss: 0.1126 - val_acc: 0.9820\n",
      "Epoch 80/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9976\n",
      "Epoch 00080: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 202ms/step - loss: 0.0135 - acc: 0.9976 - val_loss: 0.0988 - val_acc: 0.9832\n",
      "Epoch 81/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9976\n",
      "Epoch 00081: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 202ms/step - loss: 0.0135 - acc: 0.9976 - val_loss: 0.0907 - val_acc: 0.9842\n",
      "Epoch 82/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9976\n",
      "Epoch 00082: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 202ms/step - loss: 0.0134 - acc: 0.9976 - val_loss: 0.1301 - val_acc: 0.9788\n",
      "Epoch 83/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9977\n",
      "Epoch 00083: val_loss did not improve from 0.07511\n",
      "181/181 [==============================] - 37s 202ms/step - loss: 0.0132 - acc: 0.9977 - val_loss: 0.1202 - val_acc: 0.9800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00083: early stopping\n"
     ]
    }
   ],
   "source": [
    "adam = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer=adam,loss= dice_loss, metrics=[\"accuracy\"]) \n",
    "results = model.fit_generator(train_generator, steps_per_epoch=t_steps,  epochs=epochs,use_multiprocessing=False, \n",
    "                                  workers=0,validation_data=validation_generator,validation_steps=v_steps,callbacks=callbacks, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Learning curve\")\n",
    "plt.plot(results.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(results.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot( np.argmin(results.history[\"val_loss\"]), np.min(results.history[\"val_loss\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"log_loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prediction Steps.. 43\n",
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "path_testing_input = \"/tank/data/navchetan/Lars_Annotated_Datasets/Testing_Input\"\n",
    "path_testing_output = \"/tank/data/navchetan/Lars_Annotated_Datasets/Testing_Inner_Without_Papillary\"\n",
    "path = \"/tank/data/navchetan/Lars_Annotated_Datasets/\"\n",
    "os.chdir(path)\n",
    "indlist_testing_input = list(pd.read_csv('Testing_indices.csv')['Indices'].values)\n",
    "# indlist_validation_output = list(pd.read_csv('Training_indices.csv')['Indices'].values)\n",
    "csvFiletesting_Input = 'Testing_Input.csv'\n",
    "df_testing_input = pd.read_csv(csvFiletesting_Input)\n",
    "csvFiletesting_Output = 'Testing_Output.csv'\n",
    "df_testing_output = pd.read_csv(csvFiletesting_Output)\n",
    "BS = 8\n",
    "\n",
    "N_PRED=len(indlist_testing_input)\n",
    "pred_steps= math.ceil(N_PRED/batch_size)\n",
    "    \n",
    "\n",
    "\n",
    "pred_X_generator = CustomImgGenSC(path_testing_input,path_testing_output,indlist_testing_input\\\n",
    "                                 ,df_testing_input,df_testing_output\\\n",
    "                                 ,H=256,W=256,onlyX=True,shuffle=False,BATCH_SIZE=BS)\n",
    "    \n",
    "# path = \"/data/navchetan/Data/Testing\"\n",
    "# os.chdir(path)\n",
    "\n",
    "# Create and save result matrix batch by batch\n",
    "a = 0\n",
    "print('Running prediction Steps.. ' + str(pred_steps))\n",
    "count = 1\n",
    "for step in range(pred_steps): \n",
    "    pred_Y_predict = model.predict_generator(pred_X_generator,steps=1,use_multiprocessing=False,workers=0)\n",
    "    print(step,end=\" \")\n",
    "    b = np.shape(pred_Y_predict)[0]\n",
    "    short_lst = indlist_testing_input[a:a+b]\n",
    "    a+=b\n",
    "    pos = 0\n",
    "    os.chdir(path_results_save)\n",
    "    for i in short_lst:\n",
    "        S = pred_Y_predict[pos,:,:,0]\n",
    "        fsave = 'Segment_Output'+str(count).zfill(5)+'.mat'\n",
    "        sio.savemat(fsave, {'S':S})\n",
    "        pos+=1\n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Testing Data for computation of Jaccard Index\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/navchetan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "nan\n",
      "0.9636437803409411\n",
      "0.7167015105444833\n",
      "0.965387489363464\n",
      "0.589692135933728\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "cols = ['sensitivity','specificity','accuracy','dice_score','Jaccard']\n",
    "df = pd.DataFrame(columns=cols)\n",
    "path_testing_ground_truth = \"/tank/data/navchetan/Lars_Annotated_Datasets/Testing_Inner_Without_Papillary\"\n",
    "N_TESTING_SAMPLES = 342\n",
    "print('Loading Testing Data for computation of Jaccard Index')\n",
    "sensitivity_t = 0\n",
    "specificity_t = 0\n",
    "accuracy_t = 0\n",
    "dice_score_t = 0\n",
    "Jaccard_t = 0\n",
    "for i in range(N_TESTING_SAMPLES):\n",
    "    if(i%10 == 0):\n",
    "        print(i)\n",
    "    os.chdir(path_results_save)    \n",
    "    pathr = 'Segment_Output'+str(i+1).zfill(5)+'.mat'\n",
    "    x = sio.loadmat(pathr)\n",
    "    Segment_model = x['S']\n",
    "    Segment_model1 = Segment_model*255;\n",
    "    retval1, Segment_model_threshold = cv2.threshold(Segment_model1,100,255,cv2.THRESH_BINARY)\n",
    "    Segment_model_threshold1 = Segment_model_threshold/255.0\n",
    "    \n",
    "    os.chdir(path_testing_ground_truth)\n",
    "    path_t = 'Segment'+str(i+1).zfill(5)+'.mat'\n",
    "    y = sio.loadmat(path_t)\n",
    "    ground_truth_testing  = y['S']\n",
    "    ground_truth1 = ground_truth_testing*255;\n",
    "    retval2, ground_truth_testing_threshold = cv2.threshold(ground_truth1,0,255,cv2.THRESH_BINARY)\n",
    "    ground_truth_testing_threshold1 = ground_truth_testing_threshold/255.0\n",
    "\n",
    "    mcm = confusion_matrix(np.ndarray.flatten(Segment_model_threshold1),np.ndarray.flatten(ground_truth_testing_threshold1))\n",
    "    tn = mcm[0, 0]\n",
    "    fp = mcm[0, 1]\n",
    "    fn = mcm[1, 0]\n",
    "    tp = mcm[1, 1]\n",
    "    \n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    accuracy = (tp + tn) / (tp + tn + fn + fp)\n",
    "    dice_score = 2*tp / (2*tp + fp + fn)\n",
    "    Jaccard = dice_score / (2-dice_score)\n",
    "    \n",
    "    sensitivity_t = sensitivity_t + sensitivity\n",
    "    specificity_t = specificity_t + specificity\n",
    "    accuracy_t = accuracy_t + accuracy\n",
    "    dice_score_t = dice_score_t + dice_score\n",
    "    Jaccard_t = Jaccard_t + Jaccard\n",
    "    \n",
    "    \n",
    "    df = df.append(pd.Series([sensitivity,specificity,accuracy,dice_score,Jaccard],index=df.columns), ignore_index=True)\n",
    "\n",
    "os.chdir(saveFolder)\n",
    "df.to_csv(saving_metrics,index=False)\n",
    "print(sensitivity_t/N_TESTING_SAMPLES)\n",
    "print(specificity_t/N_TESTING_SAMPLES)\n",
    "print(dice_score_t/N_TESTING_SAMPLES)\n",
    "print(accuracy_t/N_TESTING_SAMPLES)\n",
    "print(Jaccard_t/N_TESTING_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017634100353159925\n",
      "0.01466810706854283\n",
      "0.20678660329307358\n",
      "0.013801893203685243\n",
      "0.19604267073948728\n"
     ]
    }
   ],
   "source": [
    "print(np.std(df.sensitivity))\n",
    "print(np.std(df.specificity))\n",
    "print(np.std(df.dice_score))\n",
    "print(np.std(df.accuracy))\n",
    "print(np.std(df.Jaccard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9770216388739259\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(df.sensitivity)/342)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n"
     ]
    }
   ],
   "source": [
    "O1 = np.zeros((N_TESTING_SAMPLES,256,256))\n",
    "O2 = np.zeros((N_TESTING_SAMPLES,256,256))\n",
    "Ground_Truth = np.zeros((N_TESTING_SAMPLES,256,256))\n",
    "Segmentation_from_model = np.zeros((N_TESTING_SAMPLES,256,256))\n",
    "\n",
    "path = \"/tank/data/navchetan/Lars_Annotated_Datasets/Testing_Input\"\n",
    "os.chdir(path)\n",
    "for i in range(N_TESTING_SAMPLES):\n",
    "    if(i%10 == 0):\n",
    "        print(i)\n",
    "    pathr = 'Image'+str(i+1).zfill(5)+'.mat'\n",
    "    x = sio.loadmat(pathr)\n",
    "    O1[i,:,:] = x['U']/255\n",
    "    O2[i,:,:] = x['U']/255\n",
    "    \n",
    "path = \"/tank/data/navchetan/Lars_Annotated_Datasets/Testing_Inner_Without_Papillary\"\n",
    "os.chdir(path)\n",
    "for i in range(N_TESTING_SAMPLES):\n",
    "    if(i%10 == 0):\n",
    "        print(i)\n",
    "    pathr = 'Segment'+str(i+1).zfill(5)+'.mat'\n",
    "    x = sio.loadmat(pathr)\n",
    "    Ground_Truth[i,:,:] = x['S']\n",
    "    retval2, ground_truth_testing_threshold = cv2.threshold(Ground_Truth[i,:,:],0,255,cv2.THRESH_BINARY)\n",
    "    Ground_Truth[i,:,:] = ground_truth_testing_threshold/255.0\n",
    "    \n",
    "\n",
    "os.chdir(path_results_save)\n",
    "for i in range(N_TESTING_SAMPLES):\n",
    "    if(i%10 == 0):\n",
    "        print(i)\n",
    "    pathr = 'Segment_Output'+str(i+1).zfill(5)+'.mat'\n",
    "    x = sio.loadmat(pathr)\n",
    "    Segmentation_from_model[i,:,:] = x['S']\n",
    "    Segment_model1 = Segmentation_from_model[i,:,:]*255;\n",
    "    retval1, Segment_model_threshold = cv2.threshold(Segment_model1,100,255,cv2.THRESH_BINARY)\n",
    "    Segmentation_from_model[i,:,:] = Segment_model_threshold/255.0\n",
    " \n",
    "def concat_images(imga, imgb):\n",
    "    \"\"\"\n",
    "    Combines two color image ndarrays side-by-side.\n",
    "    \"\"\"\n",
    "    [ha,wa] = np.shape(imga)\n",
    "    [hb,wb] = np.shape(imgb)\n",
    "    max_height = np.max([ha, hb])\n",
    "    total_width = wa+wb\n",
    "    new_img = np.zeros(shape=(max_height, total_width))\n",
    "    new_img[:ha,:wa]=imga\n",
    "    new_img[:hb,wa:wa+wb]=imgb\n",
    "    return new_img\n",
    "\n",
    "\n",
    "h = 256\n",
    "w = 256\n",
    "img_array = []    \n",
    "for k in range(N_TESTING_SAMPLES):\n",
    "    print(k)\n",
    "    for i in range(h):\n",
    "        for j in range (w):\n",
    "            if Ground_Truth[k,i,j]>0:\n",
    "                O1[k,i,j]=Ground_Truth[k,i,j]\n",
    "            if Segmentation_from_model[k,i,j]>0:\n",
    "                O2[k,i,j]=Segmentation_from_model[k,i,j]\n",
    "    W1 = O1[k,:,:]\n",
    "    W2 = O2[k,:,:]\n",
    "    img = concat_images(W1,W2)\n",
    "    img_array.append(img)\n",
    "    \n",
    "    os.chdir(path_images_save) \n",
    "    pathr = 'Joint_Image'+str(k+1).zfill(5)+'.png'\n",
    "    cv2.imwrite(pathr, img*255) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
