{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, BatchNormalization, UpSampling2D, concatenate\n",
    "# from tensorflow.keras.utils import np_utils\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras import backend as Ks\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,CSVLogger\n",
    "import os\n",
    "import utilModels\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = 'U_Net_Dice_Loss_with_papilary'\n",
    "name_save_directory = \"U_Net_Dice_Loss_with_papilary\"\n",
    "saving_metrics = 'Metrics_U_Net_Dice_Loss_with_papilary.csv'\n",
    "\n",
    "\n",
    "results = \"_Results\"\n",
    "images = \"_Joint_Images\"\n",
    "parent_directory = \"/tank/data/navchetan/Lars_Annotated_Datasets/Results/\"\n",
    "saveFolder = os.path.join(parent_directory,name_save_directory)\n",
    "# os.mkdir(saveFolder)\n",
    "\n",
    "name_save_results_directory = name_save_directory+results\n",
    "path_results_save = os.path.join(saveFolder,name_save_results_directory)\n",
    "# os.mkdir(path_results_save)\n",
    "\n",
    "name_save_images_directory = name_save_directory+images\n",
    "path_images_save = os.path.join(saveFolder,name_save_images_directory)\n",
    "# os.mkdir(path_images_save)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customLoss(yTrue,yPred):\n",
    "    return (1e4)*(Ks.mean(Ks.square(yPred - yTrue), axis=-1))\n",
    "\n",
    "\n",
    "IMAGE_WIDTH = 256\n",
    "IMAGE_HEIGHT = 256\n",
    "N_TRAIN_SAMPLES = 1445\n",
    "CHANNELS = 1\n",
    "N_EVALUATE_SAMPLES = 475\n",
    "N_TESTING_SAMPLES = 342\n",
    "EVALUATE_FROM = 1\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.0001\n",
    "F = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomImgGenSC(path_train_input,path_train_output,indlst_train,\\\n",
    "                   df_train_input,df_train_output,\\\n",
    "                   H=256,W=256,onlyX=False,shuffle=False,BATCH_SIZE=16):\n",
    "    L_train = len(indlst_train)\n",
    "    while True:\n",
    "        if(shuffle):\n",
    "            random.shuffle(indlst_train)\n",
    "        ii = 0 # Current image index\n",
    "        left = L_train\n",
    "        while left>0:\n",
    "            BL = min(BATCH_SIZE,left)\n",
    "            X_BATCH = np.zeros((BL,H,W,1))\n",
    "            Y_BATCH = np.zeros((BL,H,W,1))\n",
    "            for bi in range(BL):\n",
    "                os.chdir(path_train_input)\n",
    "                imgIdx = indlst_train[ii] \n",
    "                pathr = 'Image'+str(imgIdx).zfill(5)+'.mat'\n",
    "                x = sio.loadmat(pathr)\n",
    "                X_BATCH[bi,:,:,0] = x['U']/255.0           \n",
    "                \n",
    "                if(not onlyX):\n",
    "                    os.chdir(path_train_output)\n",
    "                    pathr = 'Segment'+str(imgIdx).zfill(5)+'.mat'\n",
    "                    x = sio.loadmat(pathr)\n",
    "                    Y_BATCH[bi,:,:,0] = x['S']/255.0\n",
    "                ii+=1\n",
    "                \n",
    "            left = left - BL\n",
    "            if(not onlyX):\n",
    "                yield (X_BATCH,Y_BATCH)\n",
    "            else:\n",
    "                yield X_BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_input = \"/tank/data/navchetan/Lars_Annotated_Datasets/Training_Input\"\n",
    "path_train_output = \"/tank/data/navchetan/Lars_Annotated_Datasets/Training_Output_With_Papillary\"\n",
    "path = \"/tank/data/navchetan/Lars_Annotated_Datasets/\"\n",
    "os.chdir(path)\n",
    "indlist_train_input = list(pd.read_csv('Training_indices.csv')['Indices'].values)\n",
    "csvFileTrain_Input = 'Training_Input.csv'\n",
    "df_train_input = pd.read_csv(csvFileTrain_Input)\n",
    "csvFileTrain_Output = 'Training_Output.csv'\n",
    "df_train_output = pd.read_csv(csvFileTrain_Output)\n",
    "BS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = CustomImgGenSC(path_train_input,path_train_output,indlist_train_input\\\n",
    "                                 ,df_train_input,df_train_output\\\n",
    "                                 ,H=256,W=256,onlyX=False,shuffle=False,BATCH_SIZE=BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_validation_input = \"/tank/data/navchetan/Lars_Annotated_Datasets/Validation_Input\"\n",
    "path_validation_output = \"/tank/data/navchetan/Lars_Annotated_Datasets/Validation_Output_With_Papillary\"\n",
    "path = \"/tank/data/navchetan/Lars_Annotated_Datasets/\"\n",
    "os.chdir(path)\n",
    "indlist_validation_input = list(pd.read_csv('Validation_indices.csv')['Indices'].values)\n",
    "# indlist_validation_output = list(pd.read_csv('Training_indices.csv')['Indices'].values)\n",
    "csvFilevalidation_Input = 'Validation_Input.csv'\n",
    "df_validation_input = pd.read_csv(csvFilevalidation_Input)\n",
    "csvFilevalidation_Output = 'Validation_Output.csv'\n",
    "df_validation_output = pd.read_csv(csvFilevalidation_Output)\n",
    "BS = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator = CustomImgGenSC(path_validation_input,path_validation_output,indlist_validation_input\\\n",
    "                                 ,df_validation_input,df_validation_output\\\n",
    "                                 ,H=256,W=256,onlyX=False,shuffle=False,BATCH_SIZE=BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "batch_size = 8\n",
    "N_TRAIN=len(indlist_train_input)\n",
    "N_VALIDATE=len(indlist_validation_input)\n",
    "# N_TEST=len(indlist_testing_input)\n",
    "    \n",
    "t_steps = math.ceil(N_TRAIN/batch_size)\n",
    "v_steps = math.ceil(N_VALIDATE/batch_size)\n",
    "# tt_steps = math.ceil(N_TEST/batch_size)\n",
    "\n",
    "min_lr=0.0001\n",
    "epochs=300\n",
    "LR_patience=20\n",
    "LR_factor=0.1\n",
    "stop_patience=50\n",
    "retrainFlag=False\n",
    "\n",
    "modelSave = saveFolder + '/' + modelName + '.h5'\n",
    "trainGraphSave = saveFolder + '/' + modelName+ '_training_plot.png'\n",
    "\n",
    "callbacks = [EarlyStopping(patience=stop_patience, verbose=1),\n",
    "                 ReduceLROnPlateau(factor=LR_factor, patience=LR_patience, min_lr=min_lr, verbose=1),\n",
    "                 ModelCheckpoint(modelSave, verbose=1, save_best_only=True, save_weights_only=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/navchetan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "H = 256\n",
    "W = 256\n",
    "input_img = Input((H,W,1),name='img')\n",
    "model = utilModels.get_unet_large(input_img, n_filters = 32, dropout = 0.0, batchnorm = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 256, 256, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 256, 256, 32) 320         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 256, 256, 32) 9248        conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 256, 256, 32) 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 256, 256, 32) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 128, 128, 32) 0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 128, 128, 32) 0           max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 128, 128, 64) 18496       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 128, 128, 64) 36928       conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 64) 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 64) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 64)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64, 64, 64)   0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 64, 128)  73856       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 64, 128)  147584      conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 128)  512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 128)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 128)  0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32, 32, 128)  0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 256)  295168      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 256)  590080      conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 256)  1024        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 256)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 256)  0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 16, 16, 256)  0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 512)  1180160     dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 512)  2359808     conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 16, 16, 512)  2048        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 16, 16, 512)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 32, 32, 512)  0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 256)  524544      up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32, 32, 512)  0           conv2d_10[0][0]                  \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32, 32, 512)  0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 256)  1179904     dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 256)  590080      conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 256)  1024        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 256)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 256)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 64, 64, 128)  131200      up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64, 64, 256)  0           conv2d_13[0][0]                  \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 64, 64, 256)  0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 128)  295040      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 128)  147584      conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64, 64, 128)  512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 64, 64, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 128, 128, 128 0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 64) 32832       up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128, 128, 128 0           conv2d_16[0][0]                  \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 128, 128, 128 0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 64) 73792       dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 128, 64) 36928       conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128, 128, 64) 256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 128, 128, 64) 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 256, 256, 64) 0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 256, 256, 32) 8224        up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 256, 256, 64) 0           conv2d_19[0][0]                  \n",
      "                                                                 activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 256, 256, 64) 0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 256, 256, 32) 18464       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 256, 256, 32) 9248        conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 256, 256, 32) 128         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 256, 256, 32) 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 256, 256, 1)  33          activation_8[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 7,765,409\n",
      "Trainable params: 7,762,465\n",
      "Non-trainable params: 2,944\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dice_loss1(y_true, y_pred):\n",
    "#   y_true = tf.cast(y_true, tf.float64)\n",
    "#   y_pred = tf.math.sigmoid(y_pred)\n",
    "#   numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
    "#   denominator = tf.reduce_sum(y_true + y_pred)\n",
    "\n",
    "#   return 1 - numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "def generalized_dice_coefficient(y_true, y_pred):\n",
    "        smooth = 1.\n",
    "        y_true_f = K.flatten(y_true)\n",
    "        y_pred_f = K.flatten(y_pred)\n",
    "        intersection = K.sum(y_true_f * y_pred_f)\n",
    "        score = (2. * intersection + smooth) / (\n",
    "                    K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "        return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1 - generalized_dice_coefficient(y_true, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.3711 - acc: 0.9238\n",
      "Epoch 00001: val_loss improved from inf to 0.56655, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_with_papilary/U_Net_Dice_Loss_with_papilary.h5\n",
      "181/181 [==============================] - 35s 196ms/step - loss: 0.3708 - acc: 0.9239 - val_loss: 0.5665 - val_acc: 0.8760\n",
      "Epoch 2/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.2951 - acc: 0.9710\n",
      "Epoch 00002: val_loss improved from 0.56655 to 0.35124, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_with_papilary/U_Net_Dice_Loss_with_papilary.h5\n",
      "181/181 [==============================] - 26s 145ms/step - loss: 0.2949 - acc: 0.9710 - val_loss: 0.3512 - val_acc: 0.9386\n",
      "Epoch 3/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.2585 - acc: 0.9751\n",
      "Epoch 00003: val_loss improved from 0.35124 to 0.28896, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_with_papilary/U_Net_Dice_Loss_with_papilary.h5\n",
      "181/181 [==============================] - 28s 156ms/step - loss: 0.2584 - acc: 0.9751 - val_loss: 0.2890 - val_acc: 0.9549\n",
      "Epoch 4/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.2276 - acc: 0.9770\n",
      "Epoch 00004: val_loss improved from 0.28896 to 0.25846, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_with_papilary/U_Net_Dice_Loss_with_papilary.h5\n",
      "181/181 [==============================] - 32s 175ms/step - loss: 0.2274 - acc: 0.9770 - val_loss: 0.2585 - val_acc: 0.9598\n",
      "Epoch 5/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.2005 - acc: 0.9784\n",
      "Epoch 00005: val_loss improved from 0.25846 to 0.23056, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_with_papilary/U_Net_Dice_Loss_with_papilary.h5\n",
      "181/181 [==============================] - 35s 192ms/step - loss: 0.2004 - acc: 0.9784 - val_loss: 0.2306 - val_acc: 0.9635\n",
      "Epoch 6/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.1774 - acc: 0.9793\n",
      "Epoch 00006: val_loss improved from 0.23056 to 0.20005, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_with_papilary/U_Net_Dice_Loss_with_papilary.h5\n",
      "181/181 [==============================] - 36s 198ms/step - loss: 0.1773 - acc: 0.9793 - val_loss: 0.2001 - val_acc: 0.9676\n",
      "Epoch 7/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.1576 - acc: 0.9801\n",
      "Epoch 00007: val_loss improved from 0.20005 to 0.18644, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_with_papilary/U_Net_Dice_Loss_with_papilary.h5\n",
      "181/181 [==============================] - 36s 202ms/step - loss: 0.1575 - acc: 0.9801 - val_loss: 0.1864 - val_acc: 0.9662\n",
      "Epoch 8/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.1406 - acc: 0.9808\n",
      "Epoch 00008: val_loss improved from 0.18644 to 0.17666, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_with_papilary/U_Net_Dice_Loss_with_papilary.h5\n",
      "181/181 [==============================] - 37s 206ms/step - loss: 0.1405 - acc: 0.9808 - val_loss: 0.1767 - val_acc: 0.9672\n",
      "Epoch 9/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.1261 - acc: 0.9815\n",
      "Epoch 00009: val_loss improved from 0.17666 to 0.16716, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_with_papilary/U_Net_Dice_Loss_with_papilary.h5\n",
      "181/181 [==============================] - 37s 207ms/step - loss: 0.1260 - acc: 0.9815 - val_loss: 0.1672 - val_acc: 0.9657\n",
      "Epoch 10/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.1142 - acc: 0.9821\n",
      "Epoch 00010: val_loss improved from 0.16716 to 0.16244, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_with_papilary/U_Net_Dice_Loss_with_papilary.h5\n",
      "181/181 [==============================] - 38s 208ms/step - loss: 0.1142 - acc: 0.9821 - val_loss: 0.1624 - val_acc: 0.9649\n",
      "Epoch 11/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9824\n",
      "Epoch 00011: val_loss improved from 0.16244 to 0.13825, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_with_papilary/U_Net_Dice_Loss_with_papilary.h5\n",
      "181/181 [==============================] - 38s 211ms/step - loss: 0.1043 - acc: 0.9824 - val_loss: 0.1383 - val_acc: 0.9705\n",
      "Epoch 12/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0976 - acc: 0.9822\n",
      "Epoch 00012: val_loss improved from 0.13825 to 0.12836, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_with_papilary/U_Net_Dice_Loss_with_papilary.h5\n",
      "181/181 [==============================] - 38s 212ms/step - loss: 0.0975 - acc: 0.9822 - val_loss: 0.1284 - val_acc: 0.9707\n",
      "Epoch 13/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0899 - acc: 0.9827\n",
      "Epoch 00013: val_loss did not improve from 0.12836\n",
      "181/181 [==============================] - 38s 212ms/step - loss: 0.0898 - acc: 0.9828 - val_loss: 0.1293 - val_acc: 0.9691\n",
      "Epoch 14/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0847 - acc: 0.9828\n",
      "Epoch 00014: val_loss did not improve from 0.12836\n",
      "181/181 [==============================] - 38s 212ms/step - loss: 0.0846 - acc: 0.9828 - val_loss: 0.1286 - val_acc: 0.9690\n",
      "Epoch 15/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0797 - acc: 0.9830\n",
      "Epoch 00015: val_loss improved from 0.12836 to 0.11313, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_with_papilary/U_Net_Dice_Loss_with_papilary.h5\n",
      "181/181 [==============================] - 39s 216ms/step - loss: 0.0796 - acc: 0.9830 - val_loss: 0.1131 - val_acc: 0.9731\n",
      "Epoch 16/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9834\n",
      "Epoch 00016: val_loss improved from 0.11313 to 0.11089, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_with_papilary/U_Net_Dice_Loss_with_papilary.h5\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0748 - acc: 0.9834 - val_loss: 0.1109 - val_acc: 0.9725\n",
      "Epoch 17/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9837\n",
      "Epoch 00017: val_loss did not improve from 0.11089\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0708 - acc: 0.9837 - val_loss: 0.1291 - val_acc: 0.9677\n",
      "Epoch 18/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0673 - acc: 0.9840\n",
      "Epoch 00018: val_loss did not improve from 0.11089\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0673 - acc: 0.9840 - val_loss: 0.1232 - val_acc: 0.9673\n",
      "Epoch 19/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.9838\n",
      "Epoch 00019: val_loss improved from 0.11089 to 0.10656, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_with_papilary/U_Net_Dice_Loss_with_papilary.h5\n",
      "181/181 [==============================] - 39s 216ms/step - loss: 0.0654 - acc: 0.9838 - val_loss: 0.1066 - val_acc: 0.9718\n",
      "Epoch 20/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9843\n",
      "Epoch 00020: val_loss did not improve from 0.10656\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0621 - acc: 0.9843 - val_loss: 0.1187 - val_acc: 0.9694\n",
      "Epoch 21/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9842\n",
      "Epoch 00021: val_loss improved from 0.10656 to 0.10451, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_with_papilary/U_Net_Dice_Loss_with_papilary.h5\n",
      "181/181 [==============================] - 39s 216ms/step - loss: 0.0606 - acc: 0.9842 - val_loss: 0.1045 - val_acc: 0.9724\n",
      "Epoch 22/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9847\n",
      "Epoch 00022: val_loss did not improve from 0.10451\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0578 - acc: 0.9847 - val_loss: 0.1066 - val_acc: 0.9716\n",
      "Epoch 23/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9847\n",
      "Epoch 00023: val_loss did not improve from 0.10451\n",
      "181/181 [==============================] - 39s 216ms/step - loss: 0.0564 - acc: 0.9847 - val_loss: 0.1099 - val_acc: 0.9699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0548 - acc: 0.9849\n",
      "Epoch 00024: val_loss did not improve from 0.10451\n",
      "181/181 [==============================] - 39s 216ms/step - loss: 0.0548 - acc: 0.9849 - val_loss: 0.1384 - val_acc: 0.9606\n",
      "Epoch 25/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9851\n",
      "Epoch 00025: val_loss did not improve from 0.10451\n",
      "181/181 [==============================] - 39s 216ms/step - loss: 0.0532 - acc: 0.9851 - val_loss: 0.1143 - val_acc: 0.9691\n",
      "Epoch 26/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9852\n",
      "Epoch 00026: val_loss did not improve from 0.10451\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0521 - acc: 0.9853 - val_loss: 0.1152 - val_acc: 0.9694\n",
      "Epoch 27/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9854\n",
      "Epoch 00027: val_loss did not improve from 0.10451\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0508 - acc: 0.9855 - val_loss: 0.1377 - val_acc: 0.9640\n",
      "Epoch 28/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9855\n",
      "Epoch 00028: val_loss did not improve from 0.10451\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0498 - acc: 0.9855 - val_loss: 0.1142 - val_acc: 0.9673\n",
      "Epoch 29/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9855\n",
      "Epoch 00029: val_loss did not improve from 0.10451\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0491 - acc: 0.9856 - val_loss: 0.1083 - val_acc: 0.9697\n",
      "Epoch 30/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9858\n",
      "Epoch 00030: val_loss did not improve from 0.10451\n",
      "181/181 [==============================] - 39s 216ms/step - loss: 0.0478 - acc: 0.9858 - val_loss: 0.1264 - val_acc: 0.9657\n",
      "Epoch 31/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9859\n",
      "Epoch 00031: val_loss improved from 0.10451 to 0.09795, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_with_papilary/U_Net_Dice_Loss_with_papilary.h5\n",
      "181/181 [==============================] - 39s 217ms/step - loss: 0.0472 - acc: 0.9859 - val_loss: 0.0980 - val_acc: 0.9725\n",
      "Epoch 32/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9861\n",
      "Epoch 00032: val_loss did not improve from 0.09795\n",
      "181/181 [==============================] - 39s 213ms/step - loss: 0.0463 - acc: 0.9861 - val_loss: 0.1128 - val_acc: 0.9688\n",
      "Epoch 33/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9861\n",
      "Epoch 00033: val_loss did not improve from 0.09795\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0460 - acc: 0.9861 - val_loss: 0.1109 - val_acc: 0.9691\n",
      "Epoch 34/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9859\n",
      "Epoch 00034: val_loss improved from 0.09795 to 0.09603, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_with_papilary/U_Net_Dice_Loss_with_papilary.h5\n",
      "181/181 [==============================] - 39s 216ms/step - loss: 0.0463 - acc: 0.9859 - val_loss: 0.0960 - val_acc: 0.9726\n",
      "Epoch 35/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9863\n",
      "Epoch 00035: val_loss did not improve from 0.09603\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0445 - acc: 0.9864 - val_loss: 0.1014 - val_acc: 0.9711\n",
      "Epoch 36/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9864\n",
      "Epoch 00036: val_loss did not improve from 0.09603\n",
      "181/181 [==============================] - 39s 213ms/step - loss: 0.0442 - acc: 0.9864 - val_loss: 0.1047 - val_acc: 0.9703\n",
      "Epoch 37/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9866\n",
      "Epoch 00037: val_loss did not improve from 0.09603\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0432 - acc: 0.9866 - val_loss: 0.1135 - val_acc: 0.9679\n",
      "Epoch 38/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9867\n",
      "Epoch 00038: val_loss did not improve from 0.09603\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0427 - acc: 0.9867 - val_loss: 0.1074 - val_acc: 0.9691\n",
      "Epoch 39/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9867\n",
      "Epoch 00039: val_loss improved from 0.09603 to 0.09364, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_with_papilary/U_Net_Dice_Loss_with_papilary.h5\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0424 - acc: 0.9867 - val_loss: 0.0936 - val_acc: 0.9730\n",
      "Epoch 40/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9869\n",
      "Epoch 00040: val_loss did not improve from 0.09364\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0419 - acc: 0.9869 - val_loss: 0.0948 - val_acc: 0.9720\n",
      "Epoch 41/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9871\n",
      "Epoch 00041: val_loss did not improve from 0.09364\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0411 - acc: 0.9871 - val_loss: 0.0991 - val_acc: 0.9711\n",
      "Epoch 42/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9871\n",
      "Epoch 00042: val_loss did not improve from 0.09364\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0409 - acc: 0.9871 - val_loss: 0.1116 - val_acc: 0.9685\n",
      "Epoch 43/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9873\n",
      "Epoch 00043: val_loss did not improve from 0.09364\n",
      "181/181 [==============================] - 39s 213ms/step - loss: 0.0402 - acc: 0.9873 - val_loss: 0.1053 - val_acc: 0.9692\n",
      "Epoch 44/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9874\n",
      "Epoch 00044: val_loss improved from 0.09364 to 0.08886, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_with_papilary/U_Net_Dice_Loss_with_papilary.h5\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0396 - acc: 0.9875 - val_loss: 0.0889 - val_acc: 0.9732\n",
      "Epoch 45/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9876\n",
      "Epoch 00045: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0389 - acc: 0.9876 - val_loss: 0.0984 - val_acc: 0.9707\n",
      "Epoch 46/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9878\n",
      "Epoch 00046: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0382 - acc: 0.9878 - val_loss: 0.1038 - val_acc: 0.9697\n",
      "Epoch 47/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9878\n",
      "Epoch 00047: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0381 - acc: 0.9878 - val_loss: 0.0968 - val_acc: 0.9714\n",
      "Epoch 48/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9877\n",
      "Epoch 00048: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0385 - acc: 0.9877 - val_loss: 0.0905 - val_acc: 0.9731\n",
      "Epoch 49/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9880\n",
      "Epoch 00049: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0375 - acc: 0.9880 - val_loss: 0.1060 - val_acc: 0.9697\n",
      "Epoch 50/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9883\n",
      "Epoch 00050: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0366 - acc: 0.9883 - val_loss: 0.0937 - val_acc: 0.9712\n",
      "Epoch 51/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9885\n",
      "Epoch 00051: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0358 - acc: 0.9885 - val_loss: 0.1042 - val_acc: 0.9697\n",
      "Epoch 52/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9886\n",
      "Epoch 00052: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0354 - acc: 0.9886 - val_loss: 0.1153 - val_acc: 0.9674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9886\n",
      "Epoch 00053: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0354 - acc: 0.9886 - val_loss: 0.1192 - val_acc: 0.9666\n",
      "Epoch 54/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9890\n",
      "Epoch 00054: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0344 - acc: 0.9890 - val_loss: 0.1159 - val_acc: 0.9669\n",
      "Epoch 55/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9891\n",
      "Epoch 00055: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0339 - acc: 0.9891 - val_loss: 0.1153 - val_acc: 0.9673\n",
      "Epoch 56/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9893\n",
      "Epoch 00056: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0332 - acc: 0.9893 - val_loss: 0.1168 - val_acc: 0.9670\n",
      "Epoch 57/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9895\n",
      "Epoch 00057: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 38s 212ms/step - loss: 0.0327 - acc: 0.9895 - val_loss: 0.1404 - val_acc: 0.9615\n",
      "Epoch 58/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9898\n",
      "Epoch 00058: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 38s 212ms/step - loss: 0.0315 - acc: 0.9899 - val_loss: 0.1384 - val_acc: 0.9621\n",
      "Epoch 59/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9896\n",
      "Epoch 00059: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 213ms/step - loss: 0.0322 - acc: 0.9897 - val_loss: 0.0992 - val_acc: 0.9700\n",
      "Epoch 60/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9901\n",
      "Epoch 00060: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0308 - acc: 0.9901 - val_loss: 0.1276 - val_acc: 0.9647\n",
      "Epoch 61/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9902\n",
      "Epoch 00061: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0305 - acc: 0.9902 - val_loss: 0.1063 - val_acc: 0.9689\n",
      "Epoch 62/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9906\n",
      "Epoch 00062: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0293 - acc: 0.9906 - val_loss: 0.1045 - val_acc: 0.9684\n",
      "Epoch 63/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9905\n",
      "Epoch 00063: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0293 - acc: 0.9905 - val_loss: 0.1054 - val_acc: 0.9687\n",
      "Epoch 64/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9909\n",
      "Epoch 00064: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0282 - acc: 0.9909 - val_loss: 0.1231 - val_acc: 0.9653\n",
      "Epoch 65/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9909\n",
      "Epoch 00065: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0282 - acc: 0.9909 - val_loss: 0.0976 - val_acc: 0.9702\n",
      "Epoch 66/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9913\n",
      "Epoch 00066: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0268 - acc: 0.9913 - val_loss: 0.1057 - val_acc: 0.9685\n",
      "Epoch 67/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9914\n",
      "Epoch 00067: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0264 - acc: 0.9915 - val_loss: 0.1020 - val_acc: 0.9706\n",
      "Epoch 68/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9913\n",
      "Epoch 00068: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0269 - acc: 0.9913 - val_loss: 0.1027 - val_acc: 0.9697\n",
      "Epoch 69/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9911\n",
      "Epoch 00069: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0273 - acc: 0.9912 - val_loss: 0.0972 - val_acc: 0.9713\n",
      "Epoch 70/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9914\n",
      "Epoch 00070: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 216ms/step - loss: 0.0265 - acc: 0.9914 - val_loss: 0.1170 - val_acc: 0.9663\n",
      "Epoch 71/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9918\n",
      "Epoch 00071: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0253 - acc: 0.9918 - val_loss: 0.1312 - val_acc: 0.9640\n",
      "Epoch 72/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9918\n",
      "Epoch 00072: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0253 - acc: 0.9918 - val_loss: 0.1105 - val_acc: 0.9677\n",
      "Epoch 73/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9920\n",
      "Epoch 00073: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0246 - acc: 0.9920 - val_loss: 0.1022 - val_acc: 0.9695\n",
      "Epoch 74/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9925\n",
      "Epoch 00074: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0231 - acc: 0.9925 - val_loss: 0.1227 - val_acc: 0.9659\n",
      "Epoch 75/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9926\n",
      "Epoch 00075: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0229 - acc: 0.9926 - val_loss: 0.1186 - val_acc: 0.9666\n",
      "Epoch 76/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9930\n",
      "Epoch 00076: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0218 - acc: 0.9930 - val_loss: 0.1140 - val_acc: 0.9662\n",
      "Epoch 77/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9927\n",
      "Epoch 00077: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 216ms/step - loss: 0.0226 - acc: 0.9927 - val_loss: 0.1124 - val_acc: 0.9663\n",
      "Epoch 78/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9930\n",
      "Epoch 00078: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0216 - acc: 0.9930 - val_loss: 0.1144 - val_acc: 0.9666\n",
      "Epoch 79/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9930\n",
      "Epoch 00079: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0216 - acc: 0.9930 - val_loss: 0.1281 - val_acc: 0.9638\n",
      "Epoch 80/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9931\n",
      "Epoch 00080: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0213 - acc: 0.9931 - val_loss: 0.1292 - val_acc: 0.9635\n",
      "Epoch 81/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9934\n",
      "Epoch 00081: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 216ms/step - loss: 0.0206 - acc: 0.9934 - val_loss: 0.1162 - val_acc: 0.9661\n",
      "Epoch 82/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9935\n",
      "Epoch 00082: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0201 - acc: 0.9935 - val_loss: 0.1407 - val_acc: 0.9612\n",
      "Epoch 83/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9938\n",
      "Epoch 00083: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0193 - acc: 0.9938 - val_loss: 0.1515 - val_acc: 0.9586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9936\n",
      "Epoch 00084: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0199 - acc: 0.9936 - val_loss: 0.1324 - val_acc: 0.9624\n",
      "Epoch 85/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9935\n",
      "Epoch 00085: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0201 - acc: 0.9935 - val_loss: 0.1222 - val_acc: 0.9645\n",
      "Epoch 86/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9934\n",
      "Epoch 00086: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0205 - acc: 0.9934 - val_loss: 0.1093 - val_acc: 0.9677\n",
      "Epoch 87/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9937\n",
      "Epoch 00087: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0194 - acc: 0.9937 - val_loss: 0.1266 - val_acc: 0.9632\n",
      "Epoch 88/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9941\n",
      "Epoch 00088: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0182 - acc: 0.9941 - val_loss: 0.1149 - val_acc: 0.9670\n",
      "Epoch 89/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9943\n",
      "Epoch 00089: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 213ms/step - loss: 0.0177 - acc: 0.9943 - val_loss: 0.1010 - val_acc: 0.9700\n",
      "Epoch 90/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9940\n",
      "Epoch 00090: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0183 - acc: 0.9941 - val_loss: 0.1192 - val_acc: 0.9659\n",
      "Epoch 91/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9943\n",
      "Epoch 00091: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 215ms/step - loss: 0.0176 - acc: 0.9943 - val_loss: 0.1351 - val_acc: 0.9624\n",
      "Epoch 92/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9944\n",
      "Epoch 00092: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0173 - acc: 0.9944 - val_loss: 0.1242 - val_acc: 0.9649\n",
      "Epoch 93/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9944\n",
      "Epoch 00093: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0174 - acc: 0.9944 - val_loss: 0.1173 - val_acc: 0.9662\n",
      "Epoch 94/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9945\n",
      "Epoch 00094: val_loss did not improve from 0.08886\n",
      "181/181 [==============================] - 39s 214ms/step - loss: 0.0169 - acc: 0.9945 - val_loss: 0.1165 - val_acc: 0.9665\n",
      "Epoch 00094: early stopping\n"
     ]
    }
   ],
   "source": [
    "adam = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer=adam,loss= dice_loss, metrics=[\"accuracy\"]) \n",
    "results = model.fit_generator(train_generator, steps_per_epoch=t_steps,  epochs=epochs,use_multiprocessing=False, \n",
    "                                  workers=0,validation_data=validation_generator,validation_steps=v_steps,callbacks=callbacks, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfYAAAHwCAYAAABUsk2hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xd4lFX6//H3SYcktARC76EmdBAsICqCva6Lve9i36KrrqvLWr7r6m/Xbdh2xbWLK7qiIFYEUUSKIEV6Tei9xJB2fn+cGTIJk2QmyWQyw+d1XV7DPPPMzJmYzP3c59znHGOtRURERKJDTLgbICIiIrVHgV1ERCSKKLCLiIhEEQV2ERGRKKLALiIiEkUU2EVERKKIAruIYIz50BhzbbjbISI1ZzSPXSR8jDEbgJustZ+Guy0iEh2UsYtEOWNMXLjbUFPR8BlE6ooCu0g9ZYw51xizyBizzxjztTGmj89j9xlj1hpjDhpjlhtjLvJ57DpjzFfGmKeMMXuA8Z5js40x/88Ys9cYs94Yc5bPc74wxtzk8/zKzu1kjJnlee9PjTETjDGvVvI5LvB8jgOeNo/xHN9gjDnD57zx3tcxxnQ0xlhjzI3GmE3A58aY6caY28u99mJjzMWef/cwxnxijNljjFlpjLms+j99kcilwC5SDxljBgATgZ8DacBzwBRjTKLnlLXAKUBj4A/Aq8aYVj4vcQKwDmgBPOZzbCWQDjwBvGCMMRU0obJzXwe+9bRrPHB1JZ9jCPAycA/QBBgObKjq8/sYAfQERnve93Kf1+4FdACmGmOSgU8857TwnPe0MaZ3EO8lEhUU2EXqp5uB56y1c621xdbal4AjwFAAa+1/rbVbrLUl1tpJwGpgiM/zt1hr/2GtLbLW/ug5ttFa+y9rbTHwEtAKyKjg/f2ea4xpDwwGHrLWFlhrZwNTKvkcNwITrbWfeNqaa61dEcTPYby19rDnM7wL9DPGdPA8diXwjrX2CHAusMFa+6LnMy8EJgOXBvFeIlFBgV2kfuoA/NrTDb/PGLMPaAe0BjDGXOPTTb8PyMJl116b/bzmNu8/rLV5nn+mVPD+FZ3bGtjjc6yi9/Jqh+tdqK6jr22tPQhMBcZ6Do0FXvP8uwNwQrmf15VAyxq8t0hEUkGKSP20GXjMWvtY+Qc8Geu/gNOBOdbaYmPMIsC3Wz1U0122As2MMQ19gnu7Ss7fDHSp4LHDQEOf+/6CcPnP8Qbwe2PMLKABMMPnfWZaa0dV1niR44EydpHwizfGJPn8F4cL3OOMMScYJ9kYc44xJhVIxgW8nQDGmOtxGXvIWWs3AvNxBXkJxphhwHmVPOUF4HpjzOnGmBhjTBtjTA/PY4uAscaYeGPMIALrNp+Gy84fBiZZa0s8xz8Auhljrva8XrwxZrAxpmd1PqdIJFNgFwm/acCPPv+Nt9bOx42z/xPYC6wBrgOw1i4H/gzMAbYD2cBXddjeK4FhwG7gUWASbvz/GNbab4HrgaeA/cBMXGAGeBCXze/FFQC+XtUbe8bT3wHO8D3f001/Jq57fgtuKOFPQKKflxGJalqgRkRqxBgzCVhhrf19uNsiIsrYRSRIni7uLp6u9THABcD/wt0uEXFUPCciwWqJ6w5PA3KAW6y134W3SSLipa54ERGRKKKueBERkSiiwC4iIhJFInKMPT093Xbs2DHczRAREakTCxYs2GWtbR7IuREZ2Dt27Mj8+fPD3QwREZE6YYzZGOi56ooXERGJIgrsIiIiUUSBXUREJIpE5Bi7iIjUrcLCQnJycsjPzw93U6JaUlISbdu2JT4+vtqvocAuIiJVysnJITU1lY4dO2KMqfoJEjRrLbt37yYnJ4dOnTpV+3XUFS8iIlXKz88nLS1NQT2EjDGkpaXVuFdEgV1ERAKioB56tfEzVmAXEZGIkJKSEu4mRAQFdhERkSiiwC4iIhHFWss999xDVlYW2dnZTJo0CYCtW7cyfPhw+vXrR1ZWFl9++SXFxcVcd911R8996qmnwtz60FNVvIiIBOUP7y9j+ZYDtfqavVo34vfn9Q7o3HfeeYdFixaxePFidu3axeDBgxk+fDivv/46o0eP5oEHHqC4uJi8vDwWLVpEbm4uS5cuBWDfvn212u76SBm7iIhElNmzZ3P55ZcTGxtLRkYGI0aMYN68eQwePJgXX3yR8ePHs2TJElJTU+ncuTPr1q3jjjvuYPr06TRq1CjczQ85ZewiIhKUQDPrULHW+j0+fPhwZs2axdSpU7n66qu55557uOaaa1i8eDEfffQREyZM4K233mLixIl13OK6pYxdREQiyvDhw5k0aRLFxcXs3LmTWbNmMWTIEDZu3EiLFi24+eabufHGG1m4cCG7du2ipKSESy65hEceeYSFCxeGu/khp4xdREQiykUXXcScOXPo27cvxhieeOIJWrZsyUsvvcSTTz5JfHw8KSkpvPzyy+Tm5nL99ddTUlICwB//+Mcwtz70TEVdGvXZoEGDrPZjFxGpOz/88AM9e/YMdzOOC/5+1saYBdbaQYE8X13xRUcgb0+4WyEiIlIrFNi/+hs80QmKi8LdEhERkRpTYI9NcLfFR8LbDhERkVqgwB6X6G6LFNhFRCTyKbArsIuISBRRYI/1BHZ1xYuISBRQYD+asReEtx0iIiK1QIFdxXMiIlGnsr3bN2zYQFZWVh22pm4psMcluVuNsYuISBTQkrJxnoxdgV1EJDAf3gfbltTua7bMhrMer/Dhe++9lw4dOnDrrbcCMH78eIwxzJo1i71791JYWMijjz7KBRdcENTb5ufnc8sttzB//nzi4uL4y1/+wsiRI1m2bBnXX389BQUFlJSUMHnyZFq3bs1ll11GTk4OxcXFPPjgg/z0pz+t0ccOBQV2Fc+JiNR7Y8eO5Re/+MXRwP7WW28xffp0fvnLX9KoUSN27drF0KFDOf/88zHGBPy6EyZMAGDJkiWsWLGCM888k1WrVvHss89y1113ceWVV1JQUEBxcTHTpk2jdevWTJ06FYD9+/fX/getBQrsRzN2Fc+JiASkksw6VPr378+OHTvYsmULO3fupGnTprRq1Ypf/vKXzJo1i5iYGHJzc9m+fTstW7YM+HVnz57NHXfcAUCPHj3o0KEDq1atYtiwYTz22GPk5ORw8cUXk5mZSXZ2NnfffTf33nsv5557LqecckqoPm6NaIxdGbuISES49NJLefvtt5k0aRJjx47ltddeY+fOnSxYsIBFixaRkZFBfn5+UK9Z0UZoV1xxBVOmTKFBgwaMHj2azz//nG7durFgwQKys7O5//77efjhh2vjY9U6ZewqnhMRiQhjx47l5ptvZteuXcycOZO33nqLFi1aEB8fz4wZM9i4cWPQrzl8+HBee+01TjvtNFatWsWmTZvo3r0769ato3Pnztx5552sW7eO77//nh49etCsWTOuuuoqUlJS+M9//lP7H7IWKLCreE5EJCL07t2bgwcP0qZNG1q1asWVV17Jeeedx6BBg+jXrx89evQI+jVvvfVWxo0bR3Z2NnFxcfznP/8hMTGRSZMm8eqrrxIfH0/Lli156KGHmDdvHvfccw8xMTHEx8fzzDPPhOBT1pz2Yz+4Hf7cDc75Mwy+qXZeU0Qkymg/9rqj/dhrSsVzIiISRdQV7y2eKwqu4EJEROq3JUuWcPXVV5c5lpiYyNy5c8PUorqhwO5dK75YGbuISDTJzs5m0aJF4W5GnVNXfEwsxMSpeE5ERKKCAju47nhl7CIiEgUU2MEV0CljFxGRKKDADm6RGhXPiYjUW7W11eoXX3zB119/XQstqvp9zj333BqfUx0K7OD2ZFdXvIhI7XjiCZgxo+yxGTPc8TCrq8AeTgrs4Crj1RUvIlI7Bg+Gyy4rDe4zZrj7gwfX6GWLioq49tpr6dOnD5deeil5eXkALFiwgBEjRjBw4EBGjx7N1q1bAfj73/9Or1696NOnD2PHjmXDhg08++yzPPXUU/Tr148vv/yyzOuPHz+ea6+9ljPPPJOOHTvyzjvv8Jvf/Ibs7GzGjBlDYWEhAJ999hn9+/cnOzubG264gSNHXPyYPn06PXr04OSTT+add945+rqHDx/mhhtuYPDgwfTv35/33nuvRj+Hqmi6G6h4TkQkGL/4BVQ1jax1axg9Glq1gq1boWdP+MMf3H/+9OsHf/1rpS+5cuVKXnjhBU466SRuuOEGnn76ae666y7uuOMO3nvvPZo3b86kSZN44IEHmDhxIo8//jjr168nMTGRffv20aRJE8aNG0dKSgp333233/dYu3YtM2bMYPny5QwbNozJkyfzxBNPcNFFFzF16lTGjBnDddddx2effUa3bt245ppreOaZZxg3bhw333wzn3/+OV27di2zT/tjjz3GaaedxsSJE9m3bx9DhgzhjDPOqPznVwPK2MFTPKcxdhGRWtO0qQvqmza526ZNa/yS7dq146STTgLgqquuYvbs2axcuZKlS5cyatQo+vXrx6OPPkpOTg4Affr04corr+TVV18lLi6wPPass84iPj6e7OxsiouLGTNmDODmxG/YsIGVK1fSqVMnunXrBsC1117LrFmzWLFiBZ06dSIzMxNjDFddddXR1/z44495/PHH6devH6eeeir5+fls2rSpxj+PiihjB0/xnDJ2EZGAVJFZA6Xd7w8+CM88A7//PYwcWaO3NcYcc99aS+/evZkzZ84x50+dOpVZs2YxZcoUHnnkEZYtW1bleyQmukXLvBu9eN8zJiaGoqKiCrd59dc+L2stkydPpnv37mWOb9++vcr2VIcydvAUz2mMXUSkVniD+ltvwcMPu1vfMfdq2rRp09EA/sYbb3DyySfTvXt3du7cefR4YWEhy5Yto6SkhM2bNzNy5EieeOIJ9u3bx6FDh0hNTeXgwYPVbkOPHj3YsGEDa9asAeCVV15hxIgR9OjRg/Xr17N27dqj7fMaPXo0//jHP45eFHz33XfVfv9AKLCDiudERGrTvHkumHsz9JEj3f1582r0sj179uSll16iT58+7Nmzh1tuuYWEhATefvtt7r33Xvr27Uu/fv34+uuvKS4u5qqrriI7O5v+/fvzy1/+kiZNmnDeeefx7rvv+i2eC0RSUhIvvvgiP/nJT8jOziYmJoZx48aRlJTE888/zznnnMPJJ59Mhw4djj7nwQcfpLCwkD59+pCVlcWDDz5Yo59DVbRtK8Ckq2HXKrgtujcGEBGpLm3bWne0bWttiEtU8ZyIiEQFBXbwBHYVz4mISORTYAfPPHaNsYuISORTYAdl7CIiAYjEmqxIUxs/YwV20HQ3EZEqJCUlsXv3bgX3ELLWsnv3bpKSkmr0OlqgBtwCNcUFUFICMbrWEREpr23btuTk5LBz585wNyWqJSUl0bZt2xq9hgI7uCVlwQX3mJpdKYmIRKP4+Hg6deoU7mZIAJSegiueA3XHi4hIxFNgB1c8ByqgExGRiKfADq54DrRIjYiIRDwFdnDFc6A92UVEJOIpsENp8Zw2ghERkQinwA4qnhMRkaihwA4+Gbu64kVEJLIpsENpxq7iORERiXAhD+zGmDHGmJXGmDXGmPv8PH6dMWanMWaR57+bQt2mY6h4TkREokRIV54zxsQCE4BRQA4wzxgzxVq7vNypk6y1t4eyLZVS8ZyIiESJUGfsQ4A11tp11toC4E3gghC/Z/BUPCciIlEi1IG9DbDZ536O51h5lxhjvjfGvG2MaRfiNh1LGbuIiESJUAd24+dY+T3/3gc6Wmv7AJ8CL/l9IWN+ZoyZb4yZX+u7C3nH2BXYRUQkwoU6sOcAvhl4W2CL7wnW2t3WWm9E/Rcw0N8LWWuft9YOstYOat68ee228mhXvIrnREQksoU6sM8DMo0xnYwxCcBYYIrvCcaYVj53zwd+CHGbjqWueBERiRIhrYq31hYZY24HPgJigYnW2mXGmIeB+dbaKcCdxpjzgSJgD3BdKNvkl4rnREQkSoQ0sANYa6cB08ode8jn3/cD94e6HZWKjXe3ythFRCTCaeU5AGNcAZ0Cu4iIRDgFdq/YRBXPiYhIxFNg94pLUMYuIiIRT4HdSxm7iIhEAQV2r7gE7e4mIiIRT4HdS8VzIiISBRTYvWIT1BUvIiIRT4HdKy5RGbuIiEQ8BXavWFXFi4hI5FNg94pL0pKyIiIS8RTYveISoUhj7CIiEtkU2L1iE5Sxi4hIxFNg91LGLiIiUUCB3StWC9SIiEjkU2D3UvGciIhEAQV2r7gEdcWLiEjEU2D3ik10Gbu14W6JiIhItSmwe8Ulgi2BkqJwt0RERKTaFNi94hLdrVafExGRCKbA7hXrCezaCEZERCKYArtXXIK7VcYuIiIRTIHd62jGrsAuIiKRS4HdS2PsIiISBRTYvRTYRUQkCiiwe6l4TkREooACu5eK50REJAoosHupeE5ERKKAAruXMnYREYkCCuxecUnuVoFdREQimAK7l4rnREQkCiiwe6krXkREooACu5c3Yy/KD287REREakCB3StOXfEiIhL5FNi9tPKciIhEAQV2LxXPiYhIFFBg94qJgZg4ZewiIhLRFNh9xSYqsIuISERTYPcVl6glZUVEJKIpsPuKU8YuIiKRTYHdV2yCiudERCSiKbD7ikvUAjUiIhLRFNh9xSZCkTJ2ERGJXArsvlQ8JyIiEU6B3VecMnYREYlsCuy+YhOUsYuISERTYPel4jkREYlwCuy+1BUvIiIRToHdV6yK50REJLIpsPuKS1DGLiIiEU2B3ZcydhERiXAK7L60VryIiEQ4BXZfCuwiIhLhFNh9xSZCSSGUlIS7JSIiItWiwO4rLsHdaoc3ERGJUArsvmIT3a0WqRERkQilwO4rzhPYlbGLiEiEUmD35Q3sKqATEZEIpcDuK1YZu4iIRDYFdl/e4jll7CIiEqEU2H2peE5ERCKcArsvFc+JiEiEU2D3peI5ERGJcArsvlQ8JyIiES7kgd0YM8YYs9IYs8YYc18l511qjLHGmEGhblOFjhbPaYxdREQiU0gDuzEmFpgAnAX0Ai43xvTyc14qcCcwN5TtqVKsuuJFRCSyhTpjHwKssdaus9YWAG8CF/g57xHgCSC8qbKK50REJMKFOrC3ATb73M/xHDvKGNMfaGet/aCyFzLG/MwYM98YM3/nzp2131JQ8ZyIiES8UAd24+eYPfqgMTHAU8Cvq3oha+3z1tpB1tpBzZs3r8Um+lDxnIiIRLhQB/YcoJ3P/bbAFp/7qUAW8IUxZgMwFJgStgI6Fc+JiEiEC3VgnwdkGmM6GWMSgLHAFO+D1tr91tp0a21Ha21H4BvgfGvt/BC3y7+jxXPK2EVEJDKFNLBba4uA24GPgB+At6y1y4wxDxtjzg/le1dLbDxgoFhj7CIiEpniQv0G1tppwLRyxx6q4NxTQ92eShnjCuhUPCciIhFKK8+VF5uo4jkREYlYCuzlxSWoeE5ERCKWAnt5cUkqnhMRkYilwF5ebIKK50REJGIpsJen4jkREYlgCuzlxSYosIuISMRSYC8vLlFd8SIiErEU2MuLS1TxnIiIRCwF9vJilbGLiEjkUmAvTxm7iIhEMAX28mK1QI2IiEQuBfby4rSkrIiIRC4F9vI0j11ERCKYAnt5Kp4TEZEIdtwH9lXbDzJ5QU7pgTgtUCMiIpHruA/snyzfzq//u5j8wmJ3INbTFW9teBsmIiJSDcd9YG+WnADAnsOegrm4JMBCSVH4GiUiIlJNx31gb9qwfGB399UdLyIikei4D+xpKeUCe2yiu9WUNxERiUDHfWD3Zux788pn7FqkRkREIs9xH9jTPGPsuw+Vy9jVFS8iIhHouA/sjRvEE2N8M3Z1xYuISOQ67gN7TIyhacMEdh8uF9iVsYuISAQ67gM7QNPkBPaWL55TYBcRkQikwI6by37MdDctKysiIhFIgR1o1jDh2OluythFRCQCKbDj6YpX8ZyIiEQBBXbclLe9eYWUlFgVz4mISERTYMdl7MUllgP5heqKFxGRiKbAjs8iNYcLID7JHSz6MYwtEhERqR4FdlzGDrgpbw3T3cFDO8PYIhERkepRYMdPxp7UGA5tC3OrREREgqfATrmMHSClJRzaHsYWiYiIVI8CO24eO8Ae75S31Aw4qMAuIiKRR4EdaJAQS4P4WPYc8s3Y1RUvIiKRR4Hdo1lywrEZu7XhbZSIiEiQFNg9yqwXn9LSrRWfvy+8jRIREQmSArtHmR3eUlu6W42zi4hIhFFg90hL9tmTPSXD3WqcXUREIowCu0fThsrYRUQk8imwe6SlJHC4oJj8wmJl7CIiErEU2D2aeuay780rgMRUiG+ojF1ERCKOArtHM++ysocKwBiXtStjFxGRCKPA7uEN7HvzfMbZlbGLiEiEUWD3aJYcD+Azl10Zu4iIRB4Fdo9myYmAT2BXxi4iIhFIgd2jcYN4jPHd4a0FFByEgsPhbZiIiEgQFNg9YmMMTRv6LlLjmcuu7VtFRCSCKLD7aNow3qd4zjOXXd3xIiISQRTYfaQlJ7rpbuCTsauATkREIocCu4+myfFlp7uBMnYREYkoCuw+miUnllbFN2gGMXHK2EVEJKIosPtolhzP3rxCSkosxMS4uezK2EVEJIIosPto2jCB4hLLgfxCd0CL1IiISISpVmA3xsQYYxrVdmPCLS3FLSurRWpERCRSBRzYjTGvG2MaGWOSgeXASmPMPaFrWt0rs8MbKGMXEZGIE0zG3staewC4EJgGtAeuDkmrwiTNs6zs0SlvqS0hbzcUFYSxVSIiIoELJrDHG2PicYH9PWttIWBD06zwaOrZCKZMxg5weEeYWiQiIhKcYAL7c8AGIBmYZYzpABwIRaPC5WjGflhz2UVEJDLFBXqitfbvwN99Dm00xoys/SaFT4OEWJLiY3w2gvFk7FovXkREIkQwxXN3eYrnjDHmBWPMQuC0ELYtLNKSE4/N2FVAJyIiESKYrvgbPMVzZwLNgeuBx0PSqjBqmhxfmrEntwCMuuJFRCRiBBPYjef2bOBFa+1in2NRo2nDhNJ57LFxkJyujF1ERCJGMIF9gTHmY1xg/8gYkwqUVPUkY8wYY8xKY8waY8x9fh4fZ4xZYoxZZIyZbYzpFUSbal1acgJ78nymt2lZWRERiSABF88BNwL9gHXW2jxjTBquO75CxphYYAIwCsgB5hljplhrl/uc9rq19lnP+ecDfwHGBNGuWtU0OYE9h8oFdmXsIiISIYKpii8xxrQFrjDGAMy01r5fxdOGAGustesAjDFvAhfgVq7zvq7vlLlkwjw3Pi05gcMFxeQXFpMUH+sK6Hb8EM4miYiIBCyYqvjHgbtwQXk5cKcx5o9VPK0NsNnnfo7nWPnXvs0YsxZ4Arizgvf/mTFmvjFm/s6dOwNtdtCaJrtlZffl+WwEc3gHlFQ56iAiIhJ2wYyxnw2MstZOtNZOxHWXn1PFc/wV1x2TkVtrJ1hruwD3Ar/z90LW2uettYOstYOaN28eRLODk+YJ7LsPH3EHUltCSZFbWlZERKSeC3Z3tyY+/24cwPk5QDuf+22BLZWc/yZuydqwOboRzGGfjB00zi4iIhEhmMD+R+A7Y8x/jDEvAQuA/6viOfOATGNMJ2NMAjAWmOJ7gjEm0+fuOcDqINpU67xbt5bJ2EGV8SIiEhGCKZ57wxjzBTAY18V+r7W20jTWWltkjLkd+AiIBSZaa5cZYx4G5ltrpwC3G2POAAqBvcC11fsotaNl4wYA5Oz90R1Qxi4iIhGkysBujBlQ7lCO57a1Maa1tXZhZc+31k7DbfPqe+whn3/fFWBb60RKYhytGyexevtBd+Boxq7ALiIi9V8gGfufK3nMEoXrxXfNSGXV9kPuTnwDSGysjWBERCQiVBnYrbUB7eBmjBllrf2k5k0Kv24tUpi7bjfFJZbYGAOpGQrsIiISEYKtiq/Mn2rxtcIqMyOFI0UlbN6T5w40agN7N4a3USIiIgGozcAeNRvCZGakArB6h6c7vmU27FgORQWVPEtERCT8ajOwh3Up2NqU2SIFgFXeArrW/aC4AHZqaVkREanfajOwR43UpHhaNU5ijTdjb9XP3W5ZFL5GiYiIBKA2A/uGWnytsMvMSC3N2Jt1dpXxWxXYRUSkfgt4gRpjzMV+Du8Hllhrd1hr/T0esTLLV8a36qOMXURE6r1gMvYbgX8DV3r++xfwK+ArY8zVIWhbWHXzVMbn7PVUxrfqC9uXQXFheBsmIiJSiWACewnQ01p7ibX2EqAXcAQ4AbcrW1Tp2sJTGe9dqKZ1fyg+or3ZRUSkXgsmsHe01vqu0rID6Gat3YNb5z2qZGZ4KuN3eMbZvQV0WxeHqUUiIiJVC3iMHfjSGPMB8F/P/UuBWcaYZGBfrbcszBolxdOyURJrvBl7s86QkOopoIu6kQcREYkSwQT224CLgZNxi9G8BEy21logoGVnI01mRkppxh4T48bZVUAnIiL1WMBd8Z4APhv4HPgUmOU5FrUyW6SyZschSko8H7N1P9i+FIqLwtswERGRCgQc2I0xlwHf4rrgLwPmGmMuDVXD6oNuGSnkF5aU7s3eqh8U5cPOFeFtmIiISAWC6Yp/ABhsrd0BYIxpjsvc3w5Fw+oDbwHd6h0HaZ/W0GXs4MbZW2aFsWUiIiL+BVMVH+MN6h67g3x+xPFOeTu6N3uzLq6ATuPsIiJSTwWTsU83xnwEvOG5/1NgWu03qf5o3CCejEaJrC5TQNdHS8uKiEi9FUzx3D3A80AfoC/wvLU26hamKa9bRmrpIjXgKuO3qYBORETqp2Aydqy1k4HJIWpLvZTZIpU3vt1ESYklJsZ4Cuh+hF0rIaN3uJsnIiJSRpUZuzHmoDHmgJ//DhpjDtRFI8MpMyOFHwuLyd3nqYxvrS1cRUSk/qoysFtrU621jfz8l2qtbVQXjQynbj6V8QCkdYX4ZI2zi4hIvRTVVe214ZjK+JhYTwGd1owXEZH6R4G9Ct7K+FXbD5YebNUPti2BkuLwNUxERMQPBfYAdMtIZeU2n8Deuj8U5mkFOhERqXcU2APQu3VjVm0/yJEiT4bedpC7zZkfvkaJiIj4ocAegKw2jSgstqXz2Zt1hgYwN0jpAAAgAElEQVRNIWdeeBsmIiJSjgJ7ALJaNwZgae5+d8AYaDNIGbuIiNQ7CuwBaN+sIamJcSzdsr/0YNvBbow9P+qn8ouISARRYA9ATIyhd5tGLM31CeJtBwEWtiwMW7tERETKU2APUFbrxvyw9QBFxSXuQJuB7lbj7CIiUo8osAcoq01jjhSVsHbnYXegQRNI7wY5C8LbMBERER8K7AHKauNWz12SW26cPWceWBumVomIiJSlwB6gTukpNIiPLa2MB9cdn7cL9m4IW7tERER8KbAHKDbG0Kt1I5aVr4wHyFV3vIiI1A8K7EHIat2IZVsOUFLi6Xpv0QviG6qATkRE6g0F9iBktWlMXkEx63d7Cuhi49y68QrsIiJSTyiwByGrTbkV6MDNZ9+2BIqOhKlVIiIipRTYg9C1RQoJcTEs2+K7UM1gKC6Ard+Hr2EiIiIeCuxBiI+NoWfLVJbk+FbGe3Z6y9W68SIiEn4K7EHq3aYxS7fsx3rnrjdqBY3aapxdRETqBQX2IGW1bszB/CI27/mx9GDbgQrsIiJSLyiwB8m7At0xO73t2wSHdoSpVSIiIo4Ce5C6ZaQSF2PKVcZ7FqrR/uwiIhJmCuxBSoqPpVtGKkt9K+Nb9QUTqwI6EREJOwX2ashq04hluT4FdPENoEVP2LIovA0TEZHjngJ7NWS1aczuwwVs2Z9ferBVP9i6WDu9iYhIWCmwV0O/dk0AWLhxb+nB1v3cTm8HcsPUKhEREQX2aunZqhFJ8TEs3OQT2Fv1c7fqjhcRkTBSYK+G+NgY+rRtwsJN+0oPtsxyBXRbFdhFRCR8FNiraUD7pizL3U9+YbE7EN8AmvdQxi4iImGlwF5NA9o3oajEssR3Pnvrfi5jVwGdiIiEiQJ7NQ3o0BQoV0DXqh8c3gkHtoSpVSIicrxTYK+m9JREOqQ1ZEH5ynjQOLuIiISNAnsNDGjflIWb9pUuVJORBSZG4+wiIhI2Cuw1MKBDU3YdOkLOXs9ObwkNIb27MnYREQkbBfYaGNDeLVRzTHf81sVhapGIiBzvFNhroHtGKg0TYo9dqObQdjiwNXwNExGR45YCew3ExcbQr12TsoFdBXQiIhJGCuw1NKB9U37YepC8giJ3oGW2CuhERCRsFNhraECHJhSXWBZv9ixUk5AM6d2UsYuISFgosNdQ/3aehWrKj7MrYxcRkTBQYK+hpskJdG6efOwWroe2wcFt4WuYiIgcl0Ie2I0xY4wxK40xa4wx9/l5/FfGmOXGmO+NMZ8ZYzqEuk21bUD7pny32WehGm3hKiIiYRLSwG6MiQUmAGcBvYDLjTG9yp32HTDIWtsHeBt4IpRtCoUB7Zuy53ABG3bnuQMtswGjcXYREalzoc7YhwBrrLXrrLUFwJvABb4nWGtnWGs9EZFvgLYhblOtG+jZEOboQjWJKa6AThm7iIjUsVAH9jbAZp/7OZ5jFbkR+NDfA8aYnxlj5htj5u/cubMWm1hzmS1SaNwgnrnrdpcebDcENsyG/APha5iIiBx3Qh3YjZ9jfjcrN8ZcBQwCnvT3uLX2eWvtIGvtoObNm9diE2suJsYwrHMaX63ZVTrOPugGKDgI370S3saJiMhxJdSBPQdo53O/LXDMZuXGmDOAB4DzrbVHQtymkDipaxpb9ueXjrO3GQAdToJvnoXiovA2TkREjhuhDuzzgExjTCdjTAIwFpjie4Ixpj/wHC6o7whxe0LmxK7pAHy1ZlfpwWG3wf5NsOL9MLVKRESONyEN7NbaIuB24CPgB+Ata+0yY8zDxpjzPac9CaQA/zXGLDLGTKng5eq1zunJtGqcxNdrfQJ7tzHQrDPMmRC+homIyHElLtRvYK2dBkwrd+whn3+fEeo21AVjDCd2SeezFdspKbHExBiIiYWht8K0u2Hzt66gTkREJIS08lwtOqlrGvvyClm+1acSvt8VkNQE5vwzfA0TEZHjhgJ7LTrJ3zh7QjIMuh5+eB/2bghPw0RE5LihwF6LMhol0bVFCl+t3V32gSE/c1u5zn0uPA0TEZHjhgJ7LTupSxrz1u/hSFFx6cFGrSHrElj4Mvy4t+Ini4iI1JACey07sWs6PxYW892mfeUeuBMK8+CzR8LTMBEROS4osNeyoZ3TiDHwte84O0DLLBjyc5g/EXLmh6dxIiIS9RTYa1njBvFkt21y7Dg7wMjfQmpLeP8XWo1ORERCQoE9BE7qksbizfs4dKRc8E5qBGf9CbYvgbnPhqdxIiIS1RTYQ+CkrukUlVi+Xe8na+95PmSOhhn/B/s2H/u4iIhIDSiwh8DADk1JiIth9mo/gd0YOPtJsCXw4b113zgREYlqCuwhkBQfy+COTZm9poJ945t2gFPvg5VTYcXUum2ciIhENQX2EBnZvQWrth9i8548/ycMuw1a9Iapd0P+Af/niIiIBEmBPURG9coA4OPl2/2fEBsP5/8dDm6Fzx6uw5aJiEg0U2APkQ5pyWS2SOHTigI7QNtBcMI4mPdv2DS37honIiJRS4E9hEb1yuDbDXvYl1dQ8Umn/Q4at4Upd0DRkbprnIiIRCUF9hAa1SuD4hLLjJU7Kj4pMQXOfQp2rYTZT9Vd40REJCopsIdQ37ZNaJ6ayCeVdccDZI6C7J/Al3+GnSvrpnEiIhKVFNhDKCbGcEbPFsxcubPsbm/+jP6j27v93Z/DYT/z30VERAKgwB5io3plcLigmDn+1o73ldIcLpgA25fD8yMgd0HdNFBERKKKAnuIndglnYYJsVV3xwP0OAdu/AgwMHEMzH8RrA15G0VEJHoosIdYUnwswzOb8+kP27GBBOnW/eHnM6HTcPjgF/DebVBcGPqGiohIVFBgrwNn9Mpg+4EjLMndH9gTGjaDK96CU+6GRa/B8vdC20AREYkaCux14LQeLYgxBNYd7xUT6/ZvT2wM62eFrnEiIhJVFNjrQLPkBAZ1bBZcYAcX3DucCBu+DE3DREQk6iiw15FRPTNYse0gm3ZXsClMRTqeDHvWwf7c0DRMRESiigJ7HRmT1RKA97/fEtwTO57sbjd+VcstEhGRaKTAXkfaNWvIoA5N+d93uYFVx3u1zIakxuqOFxGRgCiw16EL+rdh9Y5DLN8axP7rMbHQ4STYMDt0DRMRkaihwF6HzsluRVyM4b1F1eiO1zi7iBwvNs+DldPD3YqIpcBeh5olJzCiW3OmLNpCcUkQ3fHecXZl7SIS7QoOw6SrYPJN2sq6mhTY69gF/duw7UA+c9cHsdFLRpbG2UXk+PD1P+DQNig4qDU8qkmBvY6N6plBckIs730XRHd8TCx0OFkZu4hEtwNb4Ku/QfdzICEVfng/3C2KSArsdaxBQiyjs1oybelW8gur2MrVV8eTYe962J8TusaJiITT549CSRGM+T/IHAUrpkJJEN+TAiiwh8WF/dpwML+IL1buCPxJGmcXkWi2ZREseh1OGAdNO0LPcyFvF2yeG+6WRRwF9jA4sUsa6SmJ/C+Y7viMLEhqonF2EYk+1sLHv3MbYJ3ya3es6yiITYAfPghv2yKQAnsYxMXGcF7fVny+Ygf7fwxwS9aYGM1nF5HotHKaS1pOvR8aNHHHkhpB51Nhxfsu8EvAFNjD5MJ+bSgoLmH60q2BP6nTKbB3A+zbHLJ2iUS0khKY87TL8grzw90aCURJCXzyEKR3h4HXl32s53mwbxNsWxKetkUoBfYw6dO2MV2aJ/PGt0EEaY2zi1QudwF8dD9MuhKe7AJv36ggX9/tWA6718BJd0JsXNnHup8NJkbV8UFSYA8TYwxXD+3Aos37+D5nX2BPatEbGjR13VbR6vBueLwDrPoo8OdYC988A7vXhq5dEhnWz3S3P3kJsi6GtZ+7ID/5xvC2Syrm3eCq0/BjH0tOh/bDYIXG2YOhwB5GFw9sS8OEWF6eszGwJ8TEwKAb4Ycp0Zu1b/4G8ve5zxio3Wth+n0w55+ha5dEhg1fugvg3hfC+f+Au1dBj3Mhd2G4WyYV2TAbGreHJu39P97jXE9Wrwv3QCmwh1GjpHgu6t+G9xdvYe/hgsCedMqv3R/AB7+CogCfE0lyF7jbYC5c1s1wt1ql6vhWdAQ2fVM284uNh9b94eAWOHIwfG0T/6yFjV9Dx5MqPqfHOe42XFn7wpdh/ovhee9qUmAPs2uGdeRIUQlvzQ9wrD2hIZz9/2DXyujMUHPmu9tgigTXegL77jXaKOd4ljMPivKP7dJN7+Zud6+p+zZJ5XaudHPVvfVD/jTtAC37hGfaW0EeTP8tTLsnonoMFNjDrHvLVE7o1IxXvtkY+MYw3Ua77qmZT8DeALvxI0FJCWz5Dtqd4O4HkrUXF7nu1zaD3H1l7cev9V+6QqsOJ5Y9np7pbnetrvs2SeW863J0qCRjB1cdn/Nt3c8I+mGKW7PelsCn4+v2vWtAgb0euGZYR3L2/hjcSnRn/cl9iX14b+gaVtd2r4YjB6D/1dCgWWCL8eQucM858Xb3HAX249f6WdCqb+k8aK9mnd3fSrQG9qIj8PKFwRWc1hcbv4JGbdxKc5Xpc5lbrOazh+ukWUd996r7/RnxGxfkN35dt+9fTQrs9cCZvTPIaJQYeBEdQOO2cOp9sOpDt57yoZ0uw533Anz0AMx8Eha94Y7t3eCy4WDs3QA5C4J7Tk15u+HbDXFjboEE9nUzAAOdRrh5/utnaTGL41FBnuuK91dZHZcITTrArlV13666sOxd93dQ38aBi47A1u/d99CW74593FrY8JXL1o2p/LWadoQT74Qlb9VdcN2zzn0H9bsSTrwDUlu579Zgv0vDIK7qUyTU4mNjuHxIe/766WrW7zpMp/TkwJ449BZY/Aa8eSXgE8xiE6G43D7GWZfApRMDb9S749wazRc+C31/GvjzaiJ3ASQ2grRM6HiKm7u6d6MbY6vI2hmuOKphM/elvvw99weZ1qVu2iz1w+ZvoKQQOvoJ7ODG2SsbY9++DJp2cjUskcQ71RPcVL/CfIhPCl97igrgkwfdBfauVW5DF3BB8a7F7iLLa/caOLyj8sI5X6f8Gr6f5Ma7fzbz2DnvtW3R666np+/lkJAMpz0I790KSydDn5+E9r1rSBl7PXHFkPbExRhe/SaIrD02Hi7+Fwy5GcY8Dle9A79cBr/bDg9sg9sXwNX/c7+YSye7LDwQB7bApjmQkALv/tz9gteF3PkuSMfEuMAOlY+z5x9wWVqXke5+pxHuNtDu+HVfaBW/aLF+FsTEQfuh/h9Pz3SBxF+29eM+eG4EfPnn0LYxFHLmwdZFbiGXwrzwT4NdOhnmPgspGXDSXS6ZuOBpOLgVvn+r7LneHjnv33pVEhrC6Mdg+1JYEEDvRMFht6Ld4V3BfQZwO8oteh26nA6N27hjfS+Hltnw2R+g8MfgX7MOKbDXEy0aJTEmqyVvzdvM/rwA148HaJkFZz/psveup7suemMgvgGkd3VB77TfuSvP714N7DWXe+aQX/cBdB4B/7vVTfkIpcIfXdbU1lME17wHNEyrvDt+w2ywxdDZE9jTurrMIJDAnrcHXr0UZvxfzdsu4bd+liugTEzx/3h6pquY3+/nQi53gcv2I3GMeu6zkNgYLpgA8Q1h1fTwtcVamDPB/e1e/S6c/pDrKex3hatq/+pvZbdg3fAVpLR0Y9iB6nm+Wz/+80eqDthLJ7v3nPFY8J9l3Qw4kAv9ryo9FhMDZz7mfoe8vST1lAJ7PXLrqV05eKSIf89eV7sv3LgtdD0DvnvNVZFXZfn/oEUvV4h0+ZvugmHKHaEdw9v6veu2azPQ3Y+JcVNgNsyueMx83Qz3ZdZuiLtvjOuOXz+r6nGwldPcl3nu/Nr7DBIe+fvdGG6nSjI/75Q3fwV03sVrti+BA0Hs3VBeXdd2HNjihp4GXO0ZihoBqz8KX43J+lnuZzjstrJj5sbAyb90xbErprpj1rrCuY4BjK/7MgbOesJl45/9ofJzl/zX3S58GfasD+6zfPeqW+Wz+1llj3ceAd3GwJd/qdfT3xTY65FerRtxdnZLJs5ez55AF6wJ1IBr3CIdaz6t/LwDW90iH70vcvfjG8BPX4PM0fDBL/wXwdQGb4D1BnZwXXT7N1c8hLB2hiu88R236zTCzYvd+UPl77fsXXe7a5XripXItXGOm47kr3DOK80z5W23v8A+HxJS3b+r+vuozKwnYcLQugus8ye6DHjIze5+tzPdhik7Vwb2/Km/hi/+VHsLXX3zNDRMh+zLjn2s1wWuhuGrv7qfz551rnu+qmlu/jTv7nooF75ScYHvga1u+uOAa9wQzcw/Bf76eXvcBUifn5b9bvEa87gbBn3tJ24J7HpIgb2e+cUZ3cgrLOa5WbV8NdhtDCQ3r7pL/YcpgIVeF5Yei0+CS/4FcUku6w+FnPnQqC2ktiw9Vtk4+/4c9yXtHV/38mZtlXXH5+1x4+ut+rn7obpYkbqxfpYrGG07pOJzktMhqcmxlfHWut+9nudCamtY80n12lBSAgv+4y4o99Ryj5s/hfmuB6372aVTxTJHu9vVAQwp7FkH8/4NX/wf/Pt0NwxWE7tWu2GAITf7L96LiXWV5bkL3PCad334yhamqcyIe9332ecVTH9b9i5gYdgdMPgmV3QX6AXPkrehuKBsN7yvZp1cT+aBXHhjbL0cb1dgr2e6ZaRyQd/WvPz1RnYePFL1EwIVG+/GulZNh4PbKj5v2buuG755t7LHkxq7L5Glk0OzlG3uAmg7sOyx5t3dH6+/cXbvanOdywX2Ju1dZlBZYF8x1XX7j/J05ak7vn7I21O9uebrZ7nhmMqqwY1x4+zlX3/fJtfD03aQG3Ja+0Vgw1XlbZ7rvuihbqZjLXvHtfuEn5cea9wGMrIDqxVY9bG7HfO4y5yfPxVmP1V2DDwY3zztLq4GVbLZTr8rIbkFzP6ru1hPbl46RBKsxFS3dsW6L/zvA7Dkv25cv3k3OPlXbsgu0Hqa715xw5Atsys+p/0JrnA5Zx5Mvqn6P7cQUWCvh+46oxsFxSU880UtZ+0DrnXFZhVVuXu74X2zdV99x8KPeyrOaua/CG9cHvw8z8O7YN/G0tXjvIypeJx93QxXeNOi57Gv12m4e05FX9DL3nVZTqcRrou2rufrS1nWut/JfwyAfw5y0zd3Bbj8a94eN67rnRFRmfRuxwZ23yGgzFFwZL9b4SxYy95xPVpJTdyMklDyTnFr3vPY4YduZ7q/4R/3Vv4aqz9yv/tDb4Fbv3E9ep+OhxfPCn6myOHdbq56n8sgpXnF58UnwdBxsPYzWDEtsPnrlRl4vUs4Zj9V9vjutbBlIWR7pqQlp8HQW13t0NbFlb/m9uWw7Xt3EVKVXufDmD+6New/eqB6nyFEFNjroU7pyVzcvw2vzt3I1v212M2T1gU6nOy64/2NA3q74XtXENi7nOaushe/eexj+fvdF8PKaaWbsgTKu/FLm4HHPtbxFJcJ+XZvlpS4K/XOp/r/Yug03K1Gt83PH3HeHjfft9eF7rltB7n316I24bF3A7xyEfzvFldNPfw37v/t0ye4VRUrGsO01n2Bf/0Pd7+y8XWvtK5waJubJumVu9AF5Iwsz+9TLKwOsju+uMhdLHYb7YJVqAP75rku+Jzw82N//7uNcRfvaz+v+PlHDrkL326ervvkdLjsZZeBbl8Gzw2H1UHUGiyYCEU/uqK5qgy60dUzFBysfje8V1IjGHyzW+/C94Jt6WR3m3Vx6bETb3cXXZ9XUSG/7F03g8hbY1SVobfA0Ntg7jPwz8Hw+k/hw/tg7nOw5rPgPk8tUmCvp+48PZOSEsuEGbW8ccWAa2Dvev/d28v+57KA5t39Pzc2HrIudd355TOCuc+77VYTG7lf6mDkzHdfqK37HfuY7zh7cZH7Ml/0KuTtPnZ83cv7Je+vO37FB64b3vuH22agWyTD3zQoCa15L8DTw9z//3P+DNdNg9MegDu/c8sKf/s8/K2Pm2P+6qXw7i3w8YPw1rXw5+4uw5/9F2jWBdoMqPr9jm4G4xMEcua7btfYeJf9tTsh+HH2jbPh8E43tavDME9h2PbgXiMYSydDXAOXIZfXZqCbJlpZd/y6L9wYcrcxpceMca/3s5luyuhrl8Lnj1bdxVx0BL79l5vv7a/3rLwGTWDQ9e7fgc5fr8wJ41yB21d/dfetdd3wHU5ys4G8khq7efWrP4LNFfTIWOsCe4eTIKVF4G0481E4Y7z7/dqf4xKnD38D0++v7qeqMQX2eqpds4ZcNrgdk+ZtZvOevNp74V7nu1/y8kV0B7e5TKOqK9W+Y92XgreqHFy2Puefbgx+6K2w+uPgpoLkznfj+gl+VtxLz3SLXXz8O3ispfsyn3KH+2Lrcpr/10tp4S5QVk4/dlhg2btuDL5VX3ff20uQU4fj7NZWb9GMaLJtiavKbncC3PaNK3CK8XwdpbSA8/4Kt8xx3anJzV3gXD/TzdvOme8u3s59Cm6dC7fPd4G5KuWnvBUXusVdfHuKMs9wbausDqW8pZPdYk6ZZ0J7zwY0m0I0zm6t68buMtL/30tMrJvauvqTioPyqulu7ru/xXzSu8JNn7qu6FlPwssXuJqU8hfyhfmuHW/fAIe2B5ate438LVw1GVr0CPw5FUlp7pKVxZPczo7blrgCyaxLjj33hJ+736WKKuR3LHcXfRX1WFYkJsZN5xv7GtzyFfw2F+5eAz8NcN2QEFBgr8fuOK0rcTEx/OH95bX3ovEN3DSO5e+5caFtS93x5VV0w3u16uu6TH274+c+57L1Efe6q/GYWFdxGwhrXVd4RRmXMTD8Hnd1P+w2txDHjZ/A3Ssrv6oeeK1bZnTK7aVfcHl7YN1M9xm9XZgZWa7oJ9fPOHv+fnjvNpjzdOCr9gXiiz/Ck11DN8MgEnzye3eB+ZMXy2ZWvlr0cAH+qrfh5zPhV8vhdzvgV8vgkn/DoBvcOTEBfo017eh6hryBfcdyt2iNb2DvOsrdBjrtrajA/e30OMf9bbXq4wq1NoaoO37bEjiQ4y6iK5J5pquF8fc7XVLign7X0yq+GEpoCBdOcH9rW76DN6+AP3VyXfTTf+uC+ZNd4M3LXU/asNsrvsj2J76Bu/ioLcNud9Md50yApW+76W3+6oQSkmHIz93/W38V8sv+57rhe55fs/YY4y44yhcg1yGtFV+PtWrcgF+ckckfP1zBx8u2cWbvllU/KRCn3O0Wt5j7nMu0M7Kh4FDl3fBexris/dPxrsuxYVpptu7tSu99kVvgYeRvXfVqZXavdQG07aCKzxlyc+lc3UCdMM7NT5/5uAvsFz7txuJscdleibgEd7Hi70tw4culq/V9dL+7COh+Ngy+sey0vGBsnucyocRUd9EQE1d7a/Evnez2rD7/71X/3MNp3ReugOrMR90iIMGoSbFVXIKbquSd8pbjZ+2EltmuKHP1JxVPd/K17gt3UdvbM54bGw9tB4cuY1/5IWBKx8f96Xq6u4BZNb108SavbYtdnUFmJc/36n+V6zHJme+G7tZ/CfP+5X63si5xc9M7DQ+stySUmnZw7VzwohsK7HKaK5jzZ9D17u9v7rOux8erut3w9ZQy9nruhpM70aNlKuOnLOPwkWpMw/EnNcN1G/16JZz9/9wX3t71/sfs/Mm+DDCu++ubZ11gPvW+0sdPGOeK1/wV2fmyFlZ6VqLyVzhXE8bAyPth5O/g+zfdmvdLJ7vlK1v2KXtum4GwZVHZKvqSErcASLuhbsz3zMdchvnl/4O3K5nSU5mCPNeORm1d93GnU+B/49y82Zoq/NEV7Sx7B14f696rPiopcet3N27nCp/qWlpm6WYwuQvchanvlqHGuGxy3YzApr0tnex+L3wz1g4nup6w/P212nTA/b20G1J58GnQ1HWzL5187O/Bqo8B42YABCIu0a0Od+p9cP1UuD8X7l7tLh67nh7+oO518i/cWvmHtpVWw/uTnO42cFn8puvB86puN3w9pcBez8XHxvDYRVls2Z/P3z6r5f2kk9NcJnzz5/CrFW5bxEA0buOu1Be9Dt9MgO7nlI5Zg8u+2wx0PQIVTX3LXQgvnee+5L3d+6Ew4h44/feuoMa3Gt5X20GuqneHz5DH+pmuR2Lwje5i4MTb4fpp7rU2zoYdK4JvyycPwZ61rvcgNcMtctH+RHjnZlj6Ts0+58JXXBHg0Fvd4h+TrnSFTfXNsnfclKPTfheeXcjSM10vUUmxZwho0LG/D5lnuKCcM6/y1yr80Y0/9zzfXRx7tR8G2IqLtKprf6772ZVf5tSf4fe44aOPflv2+KrprkchOb16bYhLcENt9U2Lnu57KD658mEKgBNucRcBvnVGtdUNX08osEeAgR2acfmQdrwwez0/bD1Q9ROqo1Gr4LZB7DsW9m/yZOv3Hvv4kJ+7K+B15abd7F7rMt5/jYQdP7geg5s+C+2XxSm/ct2+iY1dfUF53vF934Vq5r8ADZod+4fe/yqITQhsdylfaz5z3ZhDbytdHS8hGa6Y5ArIJt9UuuhOsIoKXFVw+xPdvNrz/+GmO/33OlcgVl8UHYHPHnZDP/6WHa0L6ZluS+Pty9w4q7+eos4jXVd2VdXxqz9x07bKF2q1HeyGWGp7oZqV09xt93OqPrfLSFcFvuDF0k2dDu1w87u7nVm77aovLvinK/yraCMgr5ZZrmbn23+5Xpko64YHBfaIce+YHjRuEM8D7y6hpKQezLnueZ6rBO5xbtls3av3hW6VqbnPu/H8OU/Dv053Ve0rprpx/ju/cz0GddGdd+IdcO96/5W4TTu5LlnvOPuBra7it/9Vx2aVyelubHHRG4F3d/+4F967HdK7w+kPln0sMQWu/C80aVe9XagAFr/h5voPv9vdH3C1u2BaOQ3e+VnoV8UqLnIXa+tmVn7e/IluIaJR4wMveKtt3sr47ycB9tjVDsFNyWp3gvvdfXecKzQ9crD08YmPbEgAAB8JSURBVKICV4C38CVXZV1+2lZCQ7dcsb/57Mvfc7M1qrNuwsoP3dS+9MzAzh/5O2g9wBWQ7tvsZqtA2Wlu0aRhM8joFdi5Q291RYgr3vfphg9w7noEUPFchGjSMIEHzu7Jr/+7mNe+3cTVQzuEt0GJqfCzLyq+wo1LdFXLMx+Hv/QCrCtMOmM89BnregjqWkW9Asa4zM27At13r7giu4HX+T9/0A2ua3/ZO1UXWO1Y4aZ1Hd4Bl7/uKoLLS0x1XzQf/sYV17UbHPBHorjIzeVuPaDsOO+Qm11B5KfjXddtoPUT4IKAd/vfQKz51FUjb18Gt3ztP2jn74eZT7gV4rqcHnhbapt3Mxjvzl+tK5iNce5Tbn/2lR+6C6fYBGjd32W9+za53w9w3br+ero6DHNDUYX5pReHa2e4OfhY9zM4+0m3aFQg8g+4dRmGjgv8/0tcAlz6Ajx7irvAa9AUGrVxRaDHu26jXW3FN8+438ko6oaHOsjYjTFjjDErjTFrjDH3+Xl8uDFmoTGmyBhzaajbE8kuHtCGUzLTefj9ZXy1ph7Mg07PdIVDFRlysxvvOvV+Vyw2brab7xmOoF6VNgNh5wpXSb/gPy5IVvSl236YqwmY90LFr3dgi5tv/8wwt0rYuX91gaEi/a50QwXfTAiu3UvfdmOpw+859gv/xLvc2vkVLSFcnrUumP01C96/K/CscuHLrut65w+e1Qv9mPWk67kY9XDNKttrKjnNDbEc2u6y34bN/J/Xoofb+OietXDdVBjyM8C4/4en/Aoueg5u/BTOfMT/89uf6NZ72OJZx/zgNldL0by7K8TMmQdPD3VDEwWHPUvjLnMrvi15u+zqeOBmEZQUVj1+XF6zznDOX1yV/sqpbipcOH/+9UVMrCvy3TzXDZF1OKny5XAjTEgzdmNMLDABGAXkAPOMMVOstb4TszcB1wF3h7It0cAYwz+vGMBlz85h3CsLeGvcMHq2ahTuZlUsOR0ufyPcrQhMm0GAdYtXHMiFsyrZ5tEYl7V/+Bs3z9c3YBcXwhePuzm1JUXuy+OUuyuefuOVmOLm3s+Z4DLmJu2qbnNJsQvEGVn+u1djYqDv5S5T3p/rih4re61p97jagvTurpu5WSd3IVaZg9tcQdawW10X86wnXebjm7XvWuNmT/S/0v/qgnUtvZtb46CyKZZesXFu6dNglz/1Lv6y8evSGoqCw3DtB+6iIfsnrpjyyz+7tc5tuSLTFr3d/P1Grd39lR+6C5LKdrCrSN+fupqL79+sfJrc8abflW6J2R/3RlU3PIQ+Yx8CrLHWrrPWFgBvAhf4nmCt3WCt/R4IcueQ41PjBvG8eP1gkhPjuO7Fb8ndV/+2DIxI3gK6uc+6JTW7VVF53HesW4hkvk8R3ZGD8Pplbkpcj3PgjvmumK2qoO7l3anr2wCX5P1hipuTfcqvKx6z7vNTwMKStyp+ncIf4a1rXFA/6S64dY5bOvjT8aXrbldk0WueYYvrXa/B9qWlUxi9PrrfDUGc/vvAPleopXd1t7U9xdJXw2ZuXYhNc9zF4oYv3bK53hqP1Ay4+Dm4/kM3G+XMx+DSF+GGj9yKZfs2wr9HuQK/4kK3RGy3McEVuPo69y9w4TMuYxcnqZG7mI5NiKpueAh9YG8D+C7CneM5FjRjzM+MMfONMfN37txZK42LVK2bNOA/Nwwm70gx1038lv159ajyOVI1bOa6LW2J2wWvqi/QpMauGnrJ2278+OB2ePFsV0B2/j/c2Kbv/OhANG7rCvMWvFy2WMufvRvdWt5pme45FUnr4ubiL3rDf9d63h546XxX0HjWE66rPCbWTclrP8ytz77pG/+vXVLiptl1ONm9T9Yl7mc480+l77XqY1e0NeI39afi2FtAV343wdrWYZhb1GXmEy477HeFn3NOdNsHn3i727Sk/VBXmHr9NNeV/8KZ8NXf3CI4gUxzq0hCsnv/+jhVLZxOf8jVhURRNzyEPrD7G8ypVkm3tfZ5a+0ga+2g5s2j639CdfRo2YjnrhnIht2HufmV+RQUqcOjxtoOdmPFA68N7PxBN0DhYfjiT/DCGW7hk8vfdGtXV9ew29zWoZUtN/v9f+HZk10h19lPVv1l3Xcs7Frphg3Km3KHWy/9spfK7u0dlwhjX3cXG29c7n/t/42z3cJG3s8bG+ey9m1LXNdxUYHL1tMy3fTH+iL7Mjj1t6EfFmh/opta17y7+/8UjFZ94aZP3HDW54+4ZY+DWbZVAhOXGPgsgwgS6sCeA/gOFrYFtoT4PY8bJ3ZJ58lL+/Lt+j088kEtrid/vDr1Prcin3dcsyptBrhpTd9McFPfrvug5nOE2w7yTLV65thpavn7XXXzOze5TXPGza54hztfvS9ygaH8SoA/vO92uxv5gP+sv2EzNxXPGLe16v7cso8vfNn1XPTy6cbMvsz1VMx83H2G3WvccITvAi7h1qiVW3sh1Nlr5hnu53rZy/43bKlK045ww8euarvv2KrnZ4t4hDqwzwMyjTGdjDEJwFiggrJZqY4L+7fh5lM68co3G/nvfG09WiPNOgff3fn/27vzKLnK+8zj31/tVV1d1dWretPeQpYEYpHNYryAHcfYBMiYzeM4PmN7Ejw22DOTsZkkJ+fESc6xczJm4pjBwYax4zjYMl6GOA4xAzpglgEBQqAFgdDaUku9b1VdXds7f9xSoxW0dKnV1c/nnDpddeue0tv3XPVT773v+3uv+hNvHvNnH5m+e7aX/SdvpPvWh7zSpC894C3Yc8+V3qX/9/+xN1I7dZJTHqN1sPwj3hSvQs7blh3xBsu1nP/WK3M1LPHCPTMI/3Ddm8uRZga9wicX3HLkFD5/wBss2LPRG/Hd9dsnX7602kRTXqi/3foLb6WmAT71kFfCVeQkVTTYnXMF4AvAvwFbgbXOuc1m9lUzuw7AzN5pZt3ATcDfm9nmSrapGn3lw8u5YkkDf/KLTbzSXYH61HJiyz7k9dTrF0/fZy6/FpLzvcpx3363V09+/Xe9KwmfftjrbZ7qIKrVH/dW/DpUTe3Rr3oj2q/727cvENR+iRfuo/vhBzdAegBeXutdZj7ebYfVt3rT7DCvty4iZ5W506mANMPWrFnjnn/+LK6fPQsMjE/yO3/3JGbGQ194Nw3x8Ew3Sc7Ezie8KUotq7zCPvVLTn9ENMDXvgY7/w6uvgouvx3u/22IXAO+i+DLXz65z9jxuDfqv3GZVx42FPOKFB3PgU3eXPGlM1iMRqSKmNkLzrmTGvGpkrJVoiEe5tufvIS+8Uluf2ADhaIG081qi97rVek7/0bvUu6ZhDrApZfCj4bg4X/2lovtrYP/8Qi88xSq3C1+H9zyQ6/Gf/+2tx4kOG+VQl1khijYq8gFHXX85Q2rePqNAT7/Ty8ykatwjXCZPa66Cu77Jqwdgwdf8UJ+7U+87aei64Nwyw+8CmirVChS5FykYK8yN6/p5M+uXcGvtxzk1nufoXcsO9NNknPFjZ+FDyyGJ3LwhS+eeqgfct41XkXByDlc9VBkDlOwV6FPX7mIez+5htcOjvO7dz/NtgNvU+xE5oZ16+CZEfjTP4V77vFei0jVUbBXqd9a0cLaP7ycfLHEjfc8zeOvze1qfXPeunVw882wdi38xV94P2++WeEuUoUU7FXs/I4kv/j8u+moj/Hp763ne0/tZDbOgpBpsH69F+aHLr9fdZX3ev36mW2XiEw7TXebA9KTBb7045d4ZMtB/v2l8/nz61YS9Os7nYjIbKHpbnKEmnCAv/+9S/jc+5fwT8/u4ffve46hdG6mmyUiIhWgYJ8jfD7jKx9ezjduXs0Lu4e44X89pUF1IiJVSME+x/y7izt44A8uI5MrcsPdT/HPG7Umj4hINVGwz0GXLEjxy9uvZEVbgtsf2MBf/csWVaoTEakSCvY5qiUR4YH/eBm/f/kCvvObnXzyvufoG5uc6WaJiMgZUrDPYaGAj69ev4q/uWk1L+4Z4kN3Pc5DG/drSpyIyCymYBduvKSDX95+JfMbarjjgQ3c9o8vqBStiMgspWAXALpaavnpbZdz5zXLWbetjw/d9QQ/e7FbvXcRkVlGwS5TAn4ft71vCb+64z0saqzhv6zdyI3ffoaXu4dnumkiInKSFOxyjKXNcX562xX89ccuYPdAmuu+9RR/9JON9I7q8ryIyLlOJWXlLY1l83zrse3c/9ROAj4fVy1v4rdWtHDVec3UxUIz3TwRkTnhVErKKtjlpOzsT/Od3+zgkS0H6RubxO8z3rkwxScuXcC1F7RiZjPdRBGRqqVgl4oplRwbu4f5v1sP8q+vHGBHf5rVHUnuvOYdXL6kYaabJyJSlRTsclYUS46fb9jHN369jf0jWa5e3syXPtjFqrYkPp968CIi00XBLmdVNl/ke0/v4u512xnLFqiLBVmzoJ5LF9Vz6eJ6zm9P6lK9iMgZULDLjBjO5Hhky0HW7xrkuZ2D7BrIALCiNcEfvm8xHz2/lYDWgRcROWUKdjkn9I5mefTVXu57cifbe8dpr4vymSsX8bGLO0jGgjPdPBGRWUPBLueUUsnx2Ku93PvEDp7bNYgZnNdSy5qFKdYsqOedi+ppr4vOdDNFRM5ZCnY5Z23cO8wTr/WxfvcQL+4eYnyyAMDixhre09XIe7qauGxJA/FwYIZbKiJy7jiVYNdfTzmrVnfWsbqzDvBG1W87MMYzOwZ48vU+1j7fzfef2U3AZ6zurOOKJQ1cvriBixekiAT9M9xyEZHZQT12OWdMFoq8sHuIJ1/v5+k3Bni5e5iS85aXPb89ycq2BCvbEqxoTbJsXpxwQGEvInODLsVLVRjL5lm/a5Cntw+wsXuYLftHSeeKAAR8xjtaE6zuTHJhZ4oLO5Msaozj1/x5EalCCnapSqWSY89ghi09o7yyb4SNe4d5uXtk6j59KOBjcWMNS5vjdDXXsrAxRmsySmsyQnMirB6+iMxauscuVcnnMxY21rCwsYaPnN8KeGG/o3+cl/aO8NrBMbb3jrOxe5h/eaWHo7+zNtWGOb89yUWddVw0P8UFnUkSEU27E5HqomCXWc3nM5Y217K0ufaI7RO5It1DGQ6MZukZyXJgJMuewQwb9w7z2Ku9AJhBc22YeYkILYkI85IR2uqiLGqsYUlTDZ31MfXyRWTWUbBLVYqG/HS11NLVUnvMeyMTeTbuHealvcPsHfTCf9dAmv+3Y4DRbGFqP59BeypKXTRETdhPPBygJhygWHKkJwukJ4ukcwWCfh/v6WrkquXNrO6o031+EZlRuscucpiRiTy7+tPs7E+zo2+c3YMZRifypCeLjE8WSOcK+MyoCfupCQWIhwOMTOTZsHeYYslRXxPifcuaaE1GiAb9RIJ+IiE/zbVhzmuppbM+puAXkVOme+wipykZDR4x1/5kDWdyPP5aH+te7eU3r/czMpEjXzz2S3Mk6KOruZalzXEa4yHqa8I01IRI1YSojXhfFGIhPzXhAIlIkGhItwJE5NQo2EWmQV0sxPUXtnP9he1T2/LFEtl8kYl8kZ7hLNsOjLHt4BivHRzjuZ2DDKQnyeZLb/m5NSE/DfEwDfEQjfEw7XVROlJROlIxOlJRUjUhYkE/0ZCfcMCnVfRERMEuUilBv4+g30dtJEhzbeS4VwEyuQKD6RyD6Rzj2QLpXNG7f58rMJzJM5jOMTA+yUA6x56BDE9v75+ay380n0EiGmReeSDgoZ/NtRGaasNvPuJhQgGtsidSrRTsIjMoFgoQCwXoSMVOan/nHCMTebqHJugeyjAykSeTK5LJFZnIFRmeyHFgZJKDo1k27Rulf3zymM8wg5baCO2pKO11UdrqorTVRWhNvvkzHPDh9xlm4DcjVywxli0wOpFnNJsnX3QsbqqhKR7WVQKRc4yCXWQWMTPqYiHqYiFWtSffdv98scTAeI6+sUl6x7L0jk3SM5Jl//AE+4YmeGnvMP+6qee44wFORioWZFlLLctaalnc5NUYWNhQQ0cqStCvqwIiM0HBLlLFgn6fd1k+GQGO/0WgVHL0pyfZP5ylZ3iCA6NZcoUSJQcl5yiVHAG/j0TUG9CXiAYx4I2+cV47OMa2A2P8fMO+qQqAAH6f0ZGK0pmK0VkfpbM+RkcqRn0sRF0sSDIapC4WJOj3UXKOYslRchAO+LTgj8gZUrCLzHE+n9Fc692Lv/AUZgO8d1nT1HPnHP3jOXYPeFMFdw2k2TM4wZ7BDL/efJCBdO6kPtMMFtTH6Gqp5byWWrpa4jTFw6RqQqTKXwoU/CJvTcEuImfMzKYG561ZWH/M++OTBfYPTzCUzjE8kWdkIs9IJk+h5PAZ5fv5xuhEntd7x3jt4DiPvdpLsXTsLYJkNEh7XZT2lDdDoC0ZpTkRpjHu/fvNtWHqYqGz8WuLnJMU7CJScfFwgGXHqQL4ViYLRfYMZOgfzzGcyTGYyTE4nuPAaJZ9wxPs6k/z1PZ+MseZJdAYD7OyLcGq9gQr25LEQv6psQX7h7OkJwusaEtw8fwUqzuT1GrNAKkiCnYROSeFA4fKAp94H+ccoxMF+sYn6R+fpG/MmxGwtWeMzftHeHJ7/xG9fp9BSyJCJOjn4c0HAO/y/7LmWtrqIiSiQRKRILWRAPOSEVa2JVnRmlChIJlVFOwiMmuZGclYkGQsyNLm+DHvZ/NFXjs4xmShRFtdlJbaMIHyaP1DawZs2DPMxu5h+sYm2dmfZrQ8ra9Q/kLg9xlLm+KcN6+WgM8oOkeh5HDO0ZqMcn57klXtCRY1xlUuWM4JqhUvInIU5xw9I1k27Rth074RXtk3wva+cZzzgt5vBgb7hiaYLHjVA6NBP/PrYxya1m9mBHzG8nm1XLIgxSULUixpiuM7ifB3zjE2WWAkk2c4kycU8DG/PqYrB3OYasWLiJwBMysX7onyoZXzTrhfoVjijb40r5S/AOwfngDgUHcpmy/yyNaD/OSFbgASkQALGmqm1gWIRwIEfT6GMjmGMl4FwqGMN7jweAMH5yUizG+IsbixhpXtSVZ3JFk+L6FKgnIE9dhFRCrIOcfO/jQv7B7ixT3DHBiZYHyywFjWe+SLJVKxEPU13iNVEyQVC5GMBqce2UKJ3f1pdg1k2DOY5vXecYYzeQBCfh/nzaslGT1yAGAk6KetLjL1BaUjFWVFa0LTBWcp9dhFRM4RZsbipjiLm+LctKZzWj7TOUf30AQvd4/w8r5hNu8bZSJ/5OyAvrFJnt05wFj2zcJBoYCPizrruHRxA5ctqqetLkoo4CMU8NY1qAn5p8YgyOylHruISBUby+bpGcmysz/N+p2DPLtzkM37RzjOlX5Cfh9dLXFWtiVY0ZpgSXOcwXSO7qEJ9g5m2DuUIRr0s7S5lmUtcbqaa+msj2IYDsehOAkHfUQC/pMaTyAn51R67Ap2EZE5ZjSb58XdQwymc+QKJXLFErlCib6xSbb0jLJl/+gx1QIb42E6UlEyuQI7+9Mntb5AJOgjGvSTqgmVlxz2lhtuTUamqgmmYkHqYiESkcApLSjknGMiX2Qok/cKH2Xy+Aze0ZogVVN9BYp0KV5ERE4oEQny/vOaT/i+c47esUne6BufCvRY6M24yBdL7B5I8/rBcfaPZAEwmJoRMFkoMZErMpEvTi1N3D00wZb9B05YXrg2HPAWEWqsYVFDjFg4QO/oJAfHsvSOZhkYz5HNF5kslJgslMjmi1NTEo/Wloywoi3BirYkq9oSrGpP0pqMzJmVCNVjFxGRsyaTK9AzkmU4k2c4k5vqcXcPZdg5kGFXf5ruoQwlBzUhP82JCM21YRprw8SCfsJBH+GAn3DAR20kONXjT8WC5IoltvaMsnm/99jRNz51y6G+JsTKtgQLG2pojIdpiIdojIeJBH0cHM3SM5KlZzhL3/gkddHg1LLG7akoDTVhoiE/sZCfSND7t4EjFjCq9PgE9dhFROScFAsFWNJ0bDGhwx26PRAPn3pEvafrzcWJJnJFth4YZfO+ETbtG2XTfq8mwaEZBYcz8243NMXDvNozyoHR7HHHIZyImbeOQX0sRKomRFdznK997IJTbv90ULCLiMg55dBI/TMVDfm5eH6Ki+enjtieL5YYTOfoG5skmy/SkojQkogc8W/miyUOjHjrEgxnckzki0zkSmRyBSYLJXxmUwsYAYxlC1O1CAbTOcYOW8b4bFOwi4jInBL0+6bC/K326ayP0VkfO4stmx6asCgiIlJFFOwiIiJVRMEuIiJSRRTsIiIiVUTBLiIiUkUU7CIiIlWk4sFuZh82s21mtt3M7jzO+2Ez+3H5/WfNbGGl2yQiIlKtKhrsZuYH7gauAVYAHzezFUft9hlgyDm3FLgL+Hol2yQiIlLNKt1jfxew3Tm3wzmXA34EXH/UPtcD3y8/fxD4gM2VSv0iIiLTrNLB3g7sPex1d3nbcfdxzhWAEaChwu0SERGpSpUO9uP1vI8uq38y+2Bmf2Bmz5vZ8319fdPSOBERkWpT6WDvBjoPe90B7D/RPmYWAJLA4NEf5Jy71zm3xjm3pqmp6ei3RUREhMoH+3qgy8wWmVkIuBV46Kh9HgI+VX5+I/CYm42LxIuIiJwDKrq6m3OuYGZfAP4N8AP3O+c2m9lXgeedcw8B9wE/MLPteD31WyvZJhERkWpW8WVbnXO/An511LY/O+x5Frip0u0QERGZC1R5TkREpIoo2EVERKqIgl1ERKSK2GwcgG5mfcDuafzIRqB/Gj9PjqTjWzk6tpWjY1s5OranboFz7qTmes/KYJ9uZva8c27NTLejWun4Vo6ObeXo2FaOjm1l6VK8iIhIFVGwi4iIVBEFu+femW5AldPxrRwd28rRsa0cHdsK0j12ERGRKqIeu4iISBWZ88FuZh82s21mtt3M7pzp9sxmZtZpZuvMbKuZbTazL5a315vZI2b2evlnaqbbOluZmd/MNpjZL8uvF5nZs+Vj++PyYktyGsyszsweNLNXy+fw5Tp3p4eZ/efy34RNZvaAmUV07lbOnA52M/MDdwPXACuAj5vZiplt1axWAP6rc+4dwGXA58vH807gUedcF/Bo+bWcni8CWw97/XXgrvKxHQI+MyOtqg5/CzzsnFsOrMY7zjp3z5CZtQN3AGucc6vwFgS7FZ27FTOngx14F7DdObfDOZcDfgRcP8NtmrWccz3OuRfLz8fw/jC24x3T75d3+z5ww8y0cHYzsw7go8B3y68NuBp4sLyLju1pMrME8F681SZxzuWcc8Po3J0uASBqZgEgBvSgc7di5nqwtwN7D3vdXd4mZ8jMFgIXAc8CLc65HvDCH2ieuZbNav8T+DJQKr9uAIadc4Xya52/p28x0Af87/Ktju+aWQ06d8+Yc24f8DfAHrxAHwFeQOduxcz1YLfjbNM0gTNkZnHgp8CXnHOjM92eamBm1wK9zrkXDt98nF11/p6eAHAxcI9z7iIgjS67T4vyuITrgUVAG1CDd/vzaDp3p8lcD/ZuoPOw1x3A/hlqS1UwsyBeqP/QOfez8uaDZtZafr8V6J2p9s1i7wauM7NdeLeMrsbrwdeVL2+Czt8z0Q10O+eeLb9+EC/ode6euQ8CO51zfc65PPAz4Ap07lbMXA/29UBXeXRmCG9Ax0Mz3KZZq3zP9z5gq3PuG4e99RDwqfLzTwH/52y3bbZzzv1351yHc24h3nn6mHPuE8A64Mbybjq2p8k5dwDYa2bnlTd9ANiCzt3psAe4zMxi5b8Rh46tzt0KmfMFaszsI3g9Hz9wv3Pur2a4SbOWmV0J/AZ4hTfvA/8x3n32tcB8vP/kNznnBmekkVXAzN4P/JFz7lozW4zXg68HNgC/55ybnMn2zVZmdiHewMQQsAP4D3idH527Z8jM/hy4BW/mzAbgs3j31HXuVsCcD3YREZFqMtcvxYuIiFQVBbuIiEgVUbCLiIhUEQW7iIhIFVGwi4iIVBEFu8gcYWZFM3vpsMe0VVYzs4Vmtmm6Pk9ETl/g7XcRkSox4Zy7cKYbISKVpR67yBxnZrvM7Otm9lz5sbS8fYGZPWpmL5d/zi9vbzGzn5vZxvLjivJH+c3sO+V1t39tZtHy/neY2Zby5/xohn5NkTlDwS4yd0SPuhR/y2HvjTrn3gV8C68SI+Xn/+CcuwD4IfDN8vZvAo8751bj1VPfXN7eBdztnFsJDAMfK2+/E7io/Dm3VeqXExGPKs+JzBFmNu6cix9n+y7gaufcjvIiPgeccw1m1g+0Oufy5e09zrlGM+sDOg4v/1lepvcR51xX+fVXgKBz7i/N7GFgHPgF8Avn3HiFf1WROU09dhGBI5fMPNG3/bfrBRxe57vIm2N4PgrcDVwCvHDYil4iUgEKdhEBb4GOQz+fKT9/Gm8lOYBPAE+Wnz8KfA7AzPxmljjRh5qZD+h0zq0DvgzUAcdcNRCR6aNvziJzR9TMXjrs9cPOuUNT3sJm9izel/2Pl7fdAdxvZv8N6MNb7Qzgi8C9ZvYZvJ7554CeE/ybfuAfzSwJGHCXc2542n4jETmG7rGLzHHle+xrnHP9M90WETlzuhQvIiJSRdRjFxERqSLqsYuIiFQRBbuIiEgVUbCLiIhUEQW7iIhIFVGwi4iIVBEFu4iISBX5/6k9k4BB+UNEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Learning curve\")\n",
    "plt.plot(results.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(results.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot( np.argmin(results.history[\"val_loss\"]), np.min(results.history[\"val_loss\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"log_loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prediction Steps.. 43\n",
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "path_testing_input = \"/tank/data/navchetan/Lars_Annotated_Datasets/Testing_Input\"\n",
    "path_testing_output = \"/tank/data/navchetan/Lars_Annotated_Datasets/Testing_Output_With_Papillary\"\n",
    "path = \"/tank/data/navchetan/Lars_Annotated_Datasets/\"\n",
    "os.chdir(path)\n",
    "indlist_testing_input = list(pd.read_csv('Testing_indices.csv')['Indices'].values)\n",
    "# indlist_validation_output = list(pd.read_csv('Training_indices.csv')['Indices'].values)\n",
    "csvFiletesting_Input = 'Testing_Input.csv'\n",
    "df_testing_input = pd.read_csv(csvFiletesting_Input)\n",
    "csvFiletesting_Output = 'Testing_Output.csv'\n",
    "df_testing_output = pd.read_csv(csvFiletesting_Output)\n",
    "BS = 8\n",
    "\n",
    "N_PRED=len(indlist_testing_input)\n",
    "pred_steps= math.ceil(N_PRED/batch_size)\n",
    "    \n",
    "\n",
    "\n",
    "pred_X_generator = CustomImgGenSC(path_testing_input,path_testing_output,indlist_testing_input\\\n",
    "                                 ,df_testing_input,df_testing_output\\\n",
    "                                 ,H=256,W=256,onlyX=True,shuffle=False,BATCH_SIZE=BS)\n",
    "    \n",
    "# path = \"/data/navchetan/Data/Testing\"\n",
    "# os.chdir(path)\n",
    "\n",
    "# Create and save result matrix batch by batch\n",
    "a = 0\n",
    "print('Running prediction Steps.. ' + str(pred_steps))\n",
    "count = 1\n",
    "for step in range(pred_steps): \n",
    "    pred_Y_predict = model.predict_generator(pred_X_generator,steps=1,use_multiprocessing=False,workers=0)\n",
    "    print(step,end=\" \")\n",
    "    b = np.shape(pred_Y_predict)[0]\n",
    "    short_lst = indlist_testing_input[a:a+b]\n",
    "    a+=b\n",
    "    pos = 0\n",
    "    os.chdir(path_results_save)\n",
    "    for i in short_lst:\n",
    "        S = pred_Y_predict[pos,:,:,0]\n",
    "        fsave = 'Segment_Output'+str(count).zfill(5)+'.mat'\n",
    "        sio.savemat(fsave, {'S':S})\n",
    "        pos+=1\n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Testing Data for computation of Jaccard Index\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "0.9284455984517342\n",
      "0.9484429243518618\n",
      "0.8507016321928444\n",
      "0.9451308222542032\n",
      "0.7429508849750757\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "cols = ['sensitivity','specificity','accuracy','dice_score','Jaccard']\n",
    "df = pd.DataFrame(columns=cols)\n",
    "path_testing_ground_truth = \"/tank/data/navchetan/Lars_Annotated_Datasets/Testing_Output_With_Papillary\"\n",
    "N_TESTING_SAMPLES = 342\n",
    "print('Loading Testing Data for computation of Jaccard Index')\n",
    "sensitivity_t = 0\n",
    "specificity_t = 0\n",
    "accuracy_t = 0\n",
    "dice_score_t = 0\n",
    "Jaccard_t = 0\n",
    "for i in range(N_TESTING_SAMPLES):\n",
    "    if(i%10 == 0):\n",
    "        print(i)\n",
    "    os.chdir(path_results_save)    \n",
    "    pathr = 'Segment_Output'+str(i+1).zfill(5)+'.mat'\n",
    "    x = sio.loadmat(pathr)\n",
    "    Segment_model = x['S']\n",
    "    Segment_model1 = Segment_model*255;\n",
    "    retval1, Segment_model_threshold = cv2.threshold(Segment_model1,100,255,cv2.THRESH_BINARY)\n",
    "    Segment_model_threshold1 = Segment_model_threshold/255.0\n",
    "    \n",
    "    os.chdir(path_testing_ground_truth)\n",
    "    path_t = 'Segment'+str(i+1).zfill(5)+'.mat'\n",
    "    y = sio.loadmat(path_t)\n",
    "    ground_truth_testing  = y['S']\n",
    "    ground_truth1 = ground_truth_testing*255;\n",
    "    retval2, ground_truth_testing_threshold = cv2.threshold(ground_truth1,0,255,cv2.THRESH_BINARY)\n",
    "    ground_truth_testing_threshold1 = ground_truth_testing_threshold/255.0\n",
    "\n",
    "    mcm = confusion_matrix(np.ndarray.flatten(Segment_model_threshold1),np.ndarray.flatten(ground_truth_testing_threshold1))\n",
    "    tn = mcm[0, 0]\n",
    "    fp = mcm[0, 1]\n",
    "    fn = mcm[1, 0]\n",
    "    tp = mcm[1, 1]\n",
    "    \n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    accuracy = (tp + tn) / (tp + tn + fn + fp)\n",
    "    dice_score = 2*tp / (2*tp + fp + fn)\n",
    "    Jaccard = dice_score / (2-dice_score)\n",
    "    \n",
    "    sensitivity_t = sensitivity_t + sensitivity\n",
    "    specificity_t = specificity_t + specificity\n",
    "    accuracy_t = accuracy_t + accuracy\n",
    "    dice_score_t = dice_score_t + dice_score\n",
    "    Jaccard_t = Jaccard_t + Jaccard\n",
    "    \n",
    "    \n",
    "    df = df.append(pd.Series([sensitivity,specificity,accuracy,dice_score,Jaccard],index=df.columns), ignore_index=True)\n",
    "\n",
    "os.chdir(saveFolder)\n",
    "df.to_csv(saving_metrics,index=False)\n",
    "print(sensitivity_t/N_TESTING_SAMPLES)\n",
    "print(specificity_t/N_TESTING_SAMPLES)\n",
    "print(dice_score_t/N_TESTING_SAMPLES)\n",
    "print(accuracy_t/N_TESTING_SAMPLES)\n",
    "print(Jaccard_t/N_TESTING_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03334017958813631\n",
      "0.025128599745998348\n",
      "0.0463604875523117\n",
      "0.02237612104737828\n",
      "0.06847064067797413\n"
     ]
    }
   ],
   "source": [
    "print(np.std(df.sensitivity))\n",
    "print(np.std(df.specificity))\n",
    "print(np.std(df.dice_score))\n",
    "print(np.std(df.accuracy))\n",
    "print(np.std(df.Jaccard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "O1 = np.zeros((N_TESTING_SAMPLES,256,256))\n",
    "O2 = np.zeros((N_TESTING_SAMPLES,256,256))\n",
    "Ground_Truth = np.zeros((N_TESTING_SAMPLES,256,256))\n",
    "Segmentation_from_model = np.zeros((N_TESTING_SAMPLES,256,256))\n",
    "\n",
    "path = \"/tank/data/navchetan/Lars_Annotated_Datasets/Testing_Input\"\n",
    "os.chdir(path)\n",
    "for i in range(N_TESTING_SAMPLES):\n",
    "    if(i%10 == 0):\n",
    "        print(i)\n",
    "    pathr = 'Image'+str(i+1).zfill(5)+'.mat'\n",
    "    x = sio.loadmat(pathr)\n",
    "    O1[i,:,:] = x['U']/255\n",
    "    O2[i,:,:] = x['U']/255\n",
    "    \n",
    "path = \"/tank/data/navchetan/Lars_Annotated_Datasets/Testing_Output_With_Papillary\"\n",
    "os.chdir(path)\n",
    "for i in range(N_TESTING_SAMPLES):\n",
    "    if(i%10 == 0):\n",
    "        print(i)\n",
    "    pathr = 'Segment'+str(i+1).zfill(5)+'.mat'\n",
    "    x = sio.loadmat(pathr)\n",
    "    Ground_Truth[i,:,:] = x['S']\n",
    "    retval2, ground_truth_testing_threshold = cv2.threshold(Ground_Truth[i,:,:],0,255,cv2.THRESH_BINARY)\n",
    "    Ground_Truth[i,:,:] = ground_truth_testing_threshold/255.0\n",
    "    \n",
    "\n",
    "os.chdir(path_results_save)\n",
    "for i in range(N_TESTING_SAMPLES):\n",
    "    if(i%10 == 0):\n",
    "        print(i)\n",
    "    pathr = 'Segment_Output'+str(i+1).zfill(5)+'.mat'\n",
    "    x = sio.loadmat(pathr)\n",
    "    Segmentation_from_model[i,:,:] = x['S']\n",
    "    Segment_model1 = Segmentation_from_model[i,:,:]*255;\n",
    "    retval1, Segment_model_threshold = cv2.threshold(Segment_model1,100,255,cv2.THRESH_BINARY)\n",
    "    Segmentation_from_model[i,:,:] = Segment_model_threshold/255.0\n",
    " \n",
    "def concat_images(imga, imgb):\n",
    "    \"\"\"\n",
    "    Combines two color image ndarrays side-by-side.\n",
    "    \"\"\"\n",
    "    [ha,wa] = np.shape(imga)\n",
    "    [hb,wb] = np.shape(imgb)\n",
    "    max_height = np.max([ha, hb])\n",
    "    total_width = wa+wb\n",
    "    new_img = np.zeros(shape=(max_height, total_width))\n",
    "    new_img[:ha,:wa]=imga\n",
    "    new_img[:hb,wa:wa+wb]=imgb\n",
    "    return new_img\n",
    "\n",
    "\n",
    "h = 256\n",
    "w = 256\n",
    "img_array = []    \n",
    "for k in range(N_TESTING_SAMPLES):\n",
    "    print(k)\n",
    "    for i in range(h):\n",
    "        for j in range (w):\n",
    "            if Ground_Truth[k,i,j]>0:\n",
    "                O1[k,i,j]=Ground_Truth[k,i,j]\n",
    "            if Segmentation_from_model[k,i,j]>0:\n",
    "                O2[k,i,j]=Segmentation_from_model[k,i,j]\n",
    "    W1 = O1[k,:,:]\n",
    "    W2 = O2[k,:,:]\n",
    "    img = concat_images(W1,W2)\n",
    "    img_array.append(img)\n",
    "    \n",
    "    os.chdir(path_images_save) \n",
    "    pathr = 'Joint_Image'+str(k+1).zfill(5)+'.png'\n",
    "    cv2.imwrite(pathr, img*255) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
