{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, BatchNormalization, UpSampling2D, concatenate\n",
    "# from tensorflow.keras.utils import np_utils\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras import backend as Ks\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,CSVLogger\n",
    "import os\n",
    "import utilModels\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: '/tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Outer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a8d52324a545>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mparent_directory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/tank/data/navchetan/Lars_Annotated_Datasets/Results/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msaveFolder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_directory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname_save_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaveFolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mname_save_results_directory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname_save_directory\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Outer'"
     ]
    }
   ],
   "source": [
    "modelName = 'U_Net_Dice_Loss_Outer'\n",
    "name_save_directory = \"U_Net_Dice_Loss_Outer\"\n",
    "saving_metrics = 'Metrics_U_Net_Dice_Loss_Outer.csv'\n",
    "\n",
    "\n",
    "results = \"_Results\"\n",
    "images = \"_Joint_Images\"\n",
    "parent_directory = \"/tank/data/navchetan/Lars_Annotated_Datasets/Results/\"\n",
    "saveFolder = os.path.join(parent_directory,name_save_directory)\n",
    "os.mkdir(saveFolder)\n",
    "\n",
    "name_save_results_directory = name_save_directory+results\n",
    "path_results_save = os.path.join(saveFolder,name_save_results_directory)\n",
    "os.mkdir(path_results_save)\n",
    "\n",
    "name_save_images_directory = name_save_directory+images\n",
    "path_images_save = os.path.join(saveFolder,name_save_images_directory)\n",
    "os.mkdir(path_images_save)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customLoss(yTrue,yPred):\n",
    "    return (1e4)*(Ks.mean(Ks.square(yPred - yTrue), axis=-1))\n",
    "\n",
    "\n",
    "IMAGE_WIDTH = 256\n",
    "IMAGE_HEIGHT = 256\n",
    "N_TRAIN_SAMPLES = 1445\n",
    "CHANNELS = 1\n",
    "N_EVALUATE_SAMPLES = 475\n",
    "N_TESTING_SAMPLES = 342\n",
    "EVALUATE_FROM = 1\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.0001\n",
    "F = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomImgGenSC(path_train_input,path_train_output,indlst_train,\\\n",
    "                   df_train_input,df_train_output,\\\n",
    "                   H=256,W=256,onlyX=False,shuffle=False,BATCH_SIZE=16):\n",
    "    L_train = len(indlst_train)\n",
    "    while True:\n",
    "        if(shuffle):\n",
    "            random.shuffle(indlst_train)\n",
    "        ii = 0 # Current image index\n",
    "        left = L_train\n",
    "        while left>0:\n",
    "            BL = min(BATCH_SIZE,left)\n",
    "            X_BATCH = np.zeros((BL,H,W,1))\n",
    "            Y_BATCH = np.zeros((BL,H,W,1))\n",
    "            for bi in range(BL):\n",
    "                os.chdir(path_train_input)\n",
    "                imgIdx = indlst_train[ii] \n",
    "                pathr = 'Image'+str(imgIdx).zfill(5)+'.mat'\n",
    "                x = sio.loadmat(pathr)\n",
    "                X_BATCH[bi,:,:,0] = x['U']/255.0           \n",
    "                \n",
    "                if(not onlyX):\n",
    "                    os.chdir(path_train_output)\n",
    "                    pathr = 'Segment'+str(imgIdx).zfill(5)+'.mat'\n",
    "                    x = sio.loadmat(pathr)\n",
    "                    Y_BATCH[bi,:,:,0] = x['S']/255.0\n",
    "                ii+=1\n",
    "                \n",
    "            left = left - BL\n",
    "            if(not onlyX):\n",
    "                yield (X_BATCH,Y_BATCH)\n",
    "            else:\n",
    "                yield X_BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_input = \"/tank/data/navchetan/Lars_Annotated_Datasets/Training_Input\"\n",
    "path_train_output = \"/tank/data/navchetan/Lars_Annotated_Datasets/Training_Outer\"\n",
    "path = \"/tank/data/navchetan/Lars_Annotated_Datasets/\"\n",
    "os.chdir(path)\n",
    "indlist_train_input = list(pd.read_csv('Training_indices.csv')['Indices'].values)\n",
    "csvFileTrain_Input = 'Training_Input.csv'\n",
    "df_train_input = pd.read_csv(csvFileTrain_Input)\n",
    "csvFileTrain_Output = 'Training_Output.csv'\n",
    "df_train_output = pd.read_csv(csvFileTrain_Output)\n",
    "BS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = CustomImgGenSC(path_train_input,path_train_output,indlist_train_input\\\n",
    "                                 ,df_train_input,df_train_output\\\n",
    "                                 ,H=256,W=256,onlyX=False,shuffle=False,BATCH_SIZE=BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_validation_input = \"/tank/data/navchetan/Lars_Annotated_Datasets/Validation_Input\"\n",
    "path_validation_output = \"/tank/data/navchetan/Lars_Annotated_Datasets/Validation_Outer\"\n",
    "path = \"/tank/data/navchetan/Lars_Annotated_Datasets/\"\n",
    "os.chdir(path)\n",
    "indlist_validation_input = list(pd.read_csv('Validation_indices.csv')['Indices'].values)\n",
    "# indlist_validation_output = list(pd.read_csv('Training_indices.csv')['Indices'].values)\n",
    "csvFilevalidation_Input = 'Validation_Input.csv'\n",
    "df_validation_input = pd.read_csv(csvFilevalidation_Input)\n",
    "csvFilevalidation_Output = 'Validation_Output.csv'\n",
    "df_validation_output = pd.read_csv(csvFilevalidation_Output)\n",
    "BS = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator = CustomImgGenSC(path_validation_input,path_validation_output,indlist_validation_input\\\n",
    "                                 ,df_validation_input,df_validation_output\\\n",
    "                                 ,H=256,W=256,onlyX=False,shuffle=False,BATCH_SIZE=BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "batch_size = 8\n",
    "N_TRAIN=len(indlist_train_input)\n",
    "N_VALIDATE=len(indlist_validation_input)\n",
    "# N_TEST=len(indlist_testing_input)\n",
    "    \n",
    "t_steps = math.ceil(N_TRAIN/batch_size)\n",
    "v_steps = math.ceil(N_VALIDATE/batch_size)\n",
    "# tt_steps = math.ceil(N_TEST/batch_size)\n",
    "\n",
    "min_lr=0.0001\n",
    "epochs=300\n",
    "LR_patience=20\n",
    "LR_factor=0.1\n",
    "stop_patience=50\n",
    "retrainFlag=False\n",
    "\n",
    "modelSave = saveFolder + '/' + modelName + '.h5'\n",
    "trainGraphSave = saveFolder + '/' + modelName+ '_training_plot.png'\n",
    "\n",
    "callbacks = [EarlyStopping(patience=stop_patience, verbose=1),\n",
    "                 ReduceLROnPlateau(factor=LR_factor, patience=LR_patience, min_lr=min_lr, verbose=1),\n",
    "                 ModelCheckpoint(modelSave, verbose=1, save_best_only=True, save_weights_only=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/navchetan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "H = 256\n",
    "W = 256\n",
    "input_img = Input((H,W,1),name='img')\n",
    "model = utilModels.get_unet_large(input_img, n_filters = 32, dropout = 0.0, batchnorm = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 256, 256, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 256, 256, 32) 320         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 256, 256, 32) 9248        conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 256, 256, 32) 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 256, 256, 32) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 128, 128, 32) 0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 128, 128, 32) 0           max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 128, 128, 64) 18496       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 128, 128, 64) 36928       conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 64) 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 64) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 64)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64, 64, 64)   0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 64, 128)  73856       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 64, 128)  147584      conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 128)  512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 128)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 128)  0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32, 32, 128)  0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 256)  295168      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 256)  590080      conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 256)  1024        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 256)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 256)  0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 16, 16, 256)  0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 512)  1180160     dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 512)  2359808     conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 16, 16, 512)  2048        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 16, 16, 512)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 32, 32, 512)  0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 256)  524544      up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32, 32, 512)  0           conv2d_10[0][0]                  \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32, 32, 512)  0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 256)  1179904     dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 256)  590080      conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 256)  1024        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 256)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 256)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 64, 64, 128)  131200      up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64, 64, 256)  0           conv2d_13[0][0]                  \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 64, 64, 256)  0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 128)  295040      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 128)  147584      conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64, 64, 128)  512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 64, 64, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 128, 128, 128 0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 64) 32832       up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128, 128, 128 0           conv2d_16[0][0]                  \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 128, 128, 128 0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 64) 73792       dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 128, 64) 36928       conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128, 128, 64) 256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 128, 128, 64) 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 256, 256, 64) 0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 256, 256, 32) 8224        up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 256, 256, 64) 0           conv2d_19[0][0]                  \n",
      "                                                                 activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 256, 256, 64) 0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 256, 256, 32) 18464       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 256, 256, 32) 9248        conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 256, 256, 32) 128         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 256, 256, 32) 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 256, 256, 1)  33          activation_8[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 7,765,409\n",
      "Trainable params: 7,762,465\n",
      "Non-trainable params: 2,944\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dice_loss1(y_true, y_pred):\n",
    "#   y_true = tf.cast(y_true, tf.float64)\n",
    "#   y_pred = tf.math.sigmoid(y_pred)\n",
    "#   numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
    "#   denominator = tf.reduce_sum(y_true + y_pred)\n",
    "\n",
    "#   return 1 - numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "def generalized_dice_coefficient(y_true, y_pred):\n",
    "        smooth = 1.\n",
    "        y_true_f = K.flatten(y_true)\n",
    "        y_pred_f = K.flatten(y_pred)\n",
    "        intersection = K.sum(y_true_f * y_pred_f)\n",
    "        score = (2. * intersection + smooth) / (\n",
    "                    K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "        return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1 - generalized_dice_coefficient(y_true, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.2363 - acc: 0.9550\n",
      "Epoch 00001: val_loss improved from inf to 0.38761, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Outer/U_Net_Dice_Loss_Outer.h5\n",
      "181/181 [==============================] - 35s 191ms/step - loss: 0.2356 - acc: 0.9551 - val_loss: 0.3876 - val_acc: 0.8032\n",
      "Epoch 2/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.1707 - acc: 0.9823\n",
      "Epoch 00002: val_loss improved from 0.38761 to 0.20892, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Outer/U_Net_Dice_Loss_Outer.h5\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.1703 - acc: 0.9823 - val_loss: 0.2089 - val_acc: 0.9556\n",
      "Epoch 3/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.1411 - acc: 0.9844\n",
      "Epoch 00003: val_loss improved from 0.20892 to 0.16935, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Outer/U_Net_Dice_Loss_Outer.h5\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.1407 - acc: 0.9844 - val_loss: 0.1693 - val_acc: 0.9624\n",
      "Epoch 4/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.1196 - acc: 0.9857\n",
      "Epoch 00004: val_loss improved from 0.16935 to 0.13838, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Outer/U_Net_Dice_Loss_Outer.h5\n",
      "181/181 [==============================] - 25s 140ms/step - loss: 0.1193 - acc: 0.9857 - val_loss: 0.1384 - val_acc: 0.9713\n",
      "Epoch 5/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.1018 - acc: 0.9867\n",
      "Epoch 00005: val_loss improved from 0.13838 to 0.12097, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Outer/U_Net_Dice_Loss_Outer.h5\n",
      "181/181 [==============================] - 25s 140ms/step - loss: 0.1016 - acc: 0.9867 - val_loss: 0.1210 - val_acc: 0.9733\n",
      "Epoch 6/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0879 - acc: 0.9873\n",
      "Epoch 00006: val_loss improved from 0.12097 to 0.10439, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Outer/U_Net_Dice_Loss_Outer.h5\n",
      "181/181 [==============================] - 26s 141ms/step - loss: 0.0877 - acc: 0.9873 - val_loss: 0.1044 - val_acc: 0.9752\n",
      "Epoch 7/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9880\n",
      "Epoch 00007: val_loss improved from 0.10439 to 0.08546, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Outer/U_Net_Dice_Loss_Outer.h5\n",
      "181/181 [==============================] - 25s 140ms/step - loss: 0.0759 - acc: 0.9880 - val_loss: 0.0855 - val_acc: 0.9800\n",
      "Epoch 8/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9883\n",
      "Epoch 00008: val_loss improved from 0.08546 to 0.07151, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Outer/U_Net_Dice_Loss_Outer.h5\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0666 - acc: 0.9883 - val_loss: 0.0715 - val_acc: 0.9823\n",
      "Epoch 9/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9888\n",
      "Epoch 00009: val_loss did not improve from 0.07151\n",
      "181/181 [==============================] - 25s 140ms/step - loss: 0.0587 - acc: 0.9888 - val_loss: 0.0795 - val_acc: 0.9768\n",
      "Epoch 10/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9889\n",
      "Epoch 00010: val_loss improved from 0.07151 to 0.06229, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Outer/U_Net_Dice_Loss_Outer.h5\n",
      "181/181 [==============================] - 25s 141ms/step - loss: 0.0528 - acc: 0.9889 - val_loss: 0.0623 - val_acc: 0.9826\n",
      "Epoch 11/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9891\n",
      "Epoch 00011: val_loss improved from 0.06229 to 0.05768, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Outer/U_Net_Dice_Loss_Outer.h5\n",
      "181/181 [==============================] - 25s 141ms/step - loss: 0.0479 - acc: 0.9891 - val_loss: 0.0577 - val_acc: 0.9821\n",
      "Epoch 12/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9893\n",
      "Epoch 00012: val_loss improved from 0.05768 to 0.04790, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Outer/U_Net_Dice_Loss_Outer.h5\n",
      "181/181 [==============================] - 25s 140ms/step - loss: 0.0438 - acc: 0.9893 - val_loss: 0.0479 - val_acc: 0.9858\n",
      "Epoch 13/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9894\n",
      "Epoch 00013: val_loss improved from 0.04790 to 0.04707, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Outer/U_Net_Dice_Loss_Outer.h5\n",
      "181/181 [==============================] - 25s 140ms/step - loss: 0.0406 - acc: 0.9894 - val_loss: 0.0471 - val_acc: 0.9851\n",
      "Epoch 14/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9897\n",
      "Epoch 00014: val_loss did not improve from 0.04707\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0373 - acc: 0.9898 - val_loss: 0.0597 - val_acc: 0.9780\n",
      "Epoch 15/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9898\n",
      "Epoch 00015: val_loss did not improve from 0.04707\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0350 - acc: 0.9898 - val_loss: 0.0480 - val_acc: 0.9825\n",
      "Epoch 16/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9899\n",
      "Epoch 00016: val_loss did not improve from 0.04707\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0331 - acc: 0.9899 - val_loss: 0.0544 - val_acc: 0.9797\n",
      "Epoch 17/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9898\n",
      "Epoch 00017: val_loss improved from 0.04707 to 0.04660, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Outer/U_Net_Dice_Loss_Outer.h5\n",
      "181/181 [==============================] - 25s 140ms/step - loss: 0.0318 - acc: 0.9899 - val_loss: 0.0466 - val_acc: 0.9824\n",
      "Epoch 18/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9901\n",
      "Epoch 00018: val_loss did not improve from 0.04660\n",
      "181/181 [==============================] - 25s 140ms/step - loss: 0.0299 - acc: 0.9901 - val_loss: 0.0542 - val_acc: 0.9771\n",
      "Epoch 19/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9902\n",
      "Epoch 00019: val_loss did not improve from 0.04660\n",
      "181/181 [==============================] - 25s 138ms/step - loss: 0.0286 - acc: 0.9902 - val_loss: 0.0470 - val_acc: 0.9805\n",
      "Epoch 20/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9902\n",
      "Epoch 00020: val_loss improved from 0.04660 to 0.04193, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Outer/U_Net_Dice_Loss_Outer.h5\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0275 - acc: 0.9903 - val_loss: 0.0419 - val_acc: 0.9827\n",
      "Epoch 21/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9905\n",
      "Epoch 00021: val_loss did not improve from 0.04193\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0261 - acc: 0.9905 - val_loss: 0.0426 - val_acc: 0.9822\n",
      "Epoch 22/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9905\n",
      "Epoch 00022: val_loss did not improve from 0.04193\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0254 - acc: 0.9905 - val_loss: 0.0554 - val_acc: 0.9774\n",
      "Epoch 23/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9907\n",
      "Epoch 00023: val_loss improved from 0.04193 to 0.03973, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Outer/U_Net_Dice_Loss_Outer.h5\n",
      "181/181 [==============================] - 25s 141ms/step - loss: 0.0244 - acc: 0.9907 - val_loss: 0.0397 - val_acc: 0.9836\n",
      "Epoch 24/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9907\n",
      "Epoch 00024: val_loss did not improve from 0.03973\n",
      "181/181 [==============================] - 25s 140ms/step - loss: 0.0238 - acc: 0.9908 - val_loss: 0.1109 - val_acc: 0.9461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9908\n",
      "Epoch 00025: val_loss improved from 0.03973 to 0.03915, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Outer/U_Net_Dice_Loss_Outer.h5\n",
      "181/181 [==============================] - 25s 141ms/step - loss: 0.0231 - acc: 0.9908 - val_loss: 0.0392 - val_acc: 0.9829\n",
      "Epoch 26/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9911\n",
      "Epoch 00026: val_loss did not improve from 0.03915\n",
      "181/181 [==============================] - 25s 138ms/step - loss: 0.0221 - acc: 0.9912 - val_loss: 0.0397 - val_acc: 0.9831\n",
      "Epoch 27/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9911\n",
      "Epoch 00027: val_loss improved from 0.03915 to 0.03577, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Outer/U_Net_Dice_Loss_Outer.h5\n",
      "181/181 [==============================] - 25s 141ms/step - loss: 0.0218 - acc: 0.9911 - val_loss: 0.0358 - val_acc: 0.9846\n",
      "Epoch 28/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9911\n",
      "Epoch 00028: val_loss improved from 0.03577 to 0.03533, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Outer/U_Net_Dice_Loss_Outer.h5\n",
      "181/181 [==============================] - 25s 141ms/step - loss: 0.0214 - acc: 0.9911 - val_loss: 0.0353 - val_acc: 0.9849\n",
      "Epoch 29/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9913\n",
      "Epoch 00029: val_loss improved from 0.03533 to 0.03464, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Outer/U_Net_Dice_Loss_Outer.h5\n",
      "181/181 [==============================] - 25s 140ms/step - loss: 0.0208 - acc: 0.9913 - val_loss: 0.0346 - val_acc: 0.9848\n",
      "Epoch 30/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9915\n",
      "Epoch 00030: val_loss improved from 0.03464 to 0.03236, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Outer/U_Net_Dice_Loss_Outer.h5\n",
      "181/181 [==============================] - 25s 140ms/step - loss: 0.0202 - acc: 0.9915 - val_loss: 0.0324 - val_acc: 0.9858\n",
      "Epoch 31/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9915\n",
      "Epoch 00031: val_loss did not improve from 0.03236\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0199 - acc: 0.9915 - val_loss: 0.0356 - val_acc: 0.9839\n",
      "Epoch 32/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9915\n",
      "Epoch 00032: val_loss did not improve from 0.03236\n",
      "181/181 [==============================] - 25s 140ms/step - loss: 0.0196 - acc: 0.9916 - val_loss: 0.0339 - val_acc: 0.9852\n",
      "Epoch 33/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9915\n",
      "Epoch 00033: val_loss did not improve from 0.03236\n",
      "181/181 [==============================] - 25s 140ms/step - loss: 0.0196 - acc: 0.9915 - val_loss: 0.0348 - val_acc: 0.9843\n",
      "Epoch 34/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9917\n",
      "Epoch 00034: val_loss did not improve from 0.03236\n",
      "181/181 [==============================] - 25s 138ms/step - loss: 0.0189 - acc: 0.9917 - val_loss: 0.0405 - val_acc: 0.9814\n",
      "Epoch 35/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9916\n",
      "Epoch 00035: val_loss did not improve from 0.03236\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0191 - acc: 0.9916 - val_loss: 0.0338 - val_acc: 0.9849\n",
      "Epoch 36/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9917\n",
      "Epoch 00036: val_loss improved from 0.03236 to 0.03040, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Outer/U_Net_Dice_Loss_Outer.h5\n",
      "181/181 [==============================] - 25s 141ms/step - loss: 0.0186 - acc: 0.9917 - val_loss: 0.0304 - val_acc: 0.9862\n",
      "Epoch 37/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9917\n",
      "Epoch 00037: val_loss did not improve from 0.03040\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0185 - acc: 0.9917 - val_loss: 0.0357 - val_acc: 0.9841\n",
      "Epoch 38/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9918\n",
      "Epoch 00038: val_loss improved from 0.03040 to 0.03014, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Outer/U_Net_Dice_Loss_Outer.h5\n",
      "181/181 [==============================] - 25s 141ms/step - loss: 0.0183 - acc: 0.9918 - val_loss: 0.0301 - val_acc: 0.9864\n",
      "Epoch 39/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9920\n",
      "Epoch 00039: val_loss improved from 0.03014 to 0.02943, saving model to /tank/data/navchetan/Lars_Annotated_Datasets/Results/U_Net_Dice_Loss_Outer/U_Net_Dice_Loss_Outer.h5\n",
      "181/181 [==============================] - 26s 141ms/step - loss: 0.0177 - acc: 0.9920 - val_loss: 0.0294 - val_acc: 0.9865\n",
      "Epoch 40/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9921\n",
      "Epoch 00040: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 137ms/step - loss: 0.0174 - acc: 0.9921 - val_loss: 0.0299 - val_acc: 0.9862\n",
      "Epoch 41/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9919\n",
      "Epoch 00041: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0178 - acc: 0.9919 - val_loss: 0.0331 - val_acc: 0.9849\n",
      "Epoch 42/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9920\n",
      "Epoch 00042: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0176 - acc: 0.9920 - val_loss: 0.0499 - val_acc: 0.9789\n",
      "Epoch 43/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9922\n",
      "Epoch 00043: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0169 - acc: 0.9923 - val_loss: 0.0333 - val_acc: 0.9847\n",
      "Epoch 44/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9921\n",
      "Epoch 00044: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 140ms/step - loss: 0.0171 - acc: 0.9922 - val_loss: 0.0344 - val_acc: 0.9847\n",
      "Epoch 45/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9922\n",
      "Epoch 00045: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 140ms/step - loss: 0.0170 - acc: 0.9922 - val_loss: 0.0387 - val_acc: 0.9827\n",
      "Epoch 46/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9921\n",
      "Epoch 00046: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0171 - acc: 0.9921 - val_loss: 0.0298 - val_acc: 0.9865\n",
      "Epoch 47/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9922\n",
      "Epoch 00047: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 137ms/step - loss: 0.0168 - acc: 0.9923 - val_loss: 0.0299 - val_acc: 0.9863\n",
      "Epoch 48/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9924\n",
      "Epoch 00048: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0163 - acc: 0.9925 - val_loss: 0.0397 - val_acc: 0.9820\n",
      "Epoch 49/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9923\n",
      "Epoch 00049: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0166 - acc: 0.9923 - val_loss: 0.0330 - val_acc: 0.9846\n",
      "Epoch 50/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9923\n",
      "Epoch 00050: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0164 - acc: 0.9924 - val_loss: 0.0302 - val_acc: 0.9860\n",
      "Epoch 51/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9926\n",
      "Epoch 00051: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 141ms/step - loss: 0.0159 - acc: 0.9926 - val_loss: 0.0335 - val_acc: 0.9845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9928\n",
      "Epoch 00052: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 138ms/step - loss: 0.0153 - acc: 0.9928 - val_loss: 0.0469 - val_acc: 0.9796\n",
      "Epoch 53/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9930\n",
      "Epoch 00053: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 138ms/step - loss: 0.0150 - acc: 0.9930 - val_loss: 0.0308 - val_acc: 0.9857\n",
      "Epoch 54/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9933\n",
      "Epoch 00054: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0144 - acc: 0.9933 - val_loss: 0.0330 - val_acc: 0.9849\n",
      "Epoch 55/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9931\n",
      "Epoch 00055: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 140ms/step - loss: 0.0147 - acc: 0.9931 - val_loss: 0.0338 - val_acc: 0.9843\n",
      "Epoch 56/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9933\n",
      "Epoch 00056: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0142 - acc: 0.9933 - val_loss: 0.0311 - val_acc: 0.9856\n",
      "Epoch 57/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9933\n",
      "Epoch 00057: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0143 - acc: 0.9933 - val_loss: 0.0328 - val_acc: 0.9844\n",
      "Epoch 58/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9932\n",
      "Epoch 00058: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0144 - acc: 0.9932 - val_loss: 0.0327 - val_acc: 0.9850\n",
      "Epoch 59/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9935\n",
      "Epoch 00059: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0139 - acc: 0.9935 - val_loss: 0.0355 - val_acc: 0.9836\n",
      "Epoch 60/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9936\n",
      "Epoch 00060: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 137ms/step - loss: 0.0136 - acc: 0.9936 - val_loss: 0.0356 - val_acc: 0.9832\n",
      "Epoch 61/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9938\n",
      "Epoch 00061: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0133 - acc: 0.9938 - val_loss: 0.0328 - val_acc: 0.9846\n",
      "Epoch 62/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9938\n",
      "Epoch 00062: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 140ms/step - loss: 0.0131 - acc: 0.9938 - val_loss: 0.0370 - val_acc: 0.9827\n",
      "Epoch 63/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9938\n",
      "Epoch 00063: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0131 - acc: 0.9939 - val_loss: 0.0403 - val_acc: 0.9810\n",
      "Epoch 64/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9939\n",
      "Epoch 00064: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0130 - acc: 0.9939 - val_loss: 0.0387 - val_acc: 0.9827\n",
      "Epoch 65/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9938\n",
      "Epoch 00065: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 140ms/step - loss: 0.0131 - acc: 0.9938 - val_loss: 0.0387 - val_acc: 0.9825\n",
      "Epoch 66/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9941\n",
      "Epoch 00066: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0125 - acc: 0.9941 - val_loss: 0.0502 - val_acc: 0.9782\n",
      "Epoch 67/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9942\n",
      "Epoch 00067: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 140ms/step - loss: 0.0122 - acc: 0.9942 - val_loss: 0.0355 - val_acc: 0.9836\n",
      "Epoch 68/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9943\n",
      "Epoch 00068: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0120 - acc: 0.9943 - val_loss: 0.0374 - val_acc: 0.9828\n",
      "Epoch 69/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9945\n",
      "Epoch 00069: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0117 - acc: 0.9945 - val_loss: 0.0345 - val_acc: 0.9841\n",
      "Epoch 70/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9944\n",
      "Epoch 00070: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 140ms/step - loss: 0.0118 - acc: 0.9944 - val_loss: 0.0334 - val_acc: 0.9847\n",
      "Epoch 71/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9945\n",
      "Epoch 00071: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0115 - acc: 0.9945 - val_loss: 0.0345 - val_acc: 0.9841\n",
      "Epoch 72/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9947\n",
      "Epoch 00072: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 141ms/step - loss: 0.0112 - acc: 0.9947 - val_loss: 0.0407 - val_acc: 0.9819\n",
      "Epoch 73/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9949\n",
      "Epoch 00073: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 140ms/step - loss: 0.0108 - acc: 0.9949 - val_loss: 0.0440 - val_acc: 0.9802\n",
      "Epoch 74/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9949\n",
      "Epoch 00074: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 140ms/step - loss: 0.0108 - acc: 0.9949 - val_loss: 0.0440 - val_acc: 0.9797\n",
      "Epoch 75/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9951\n",
      "Epoch 00075: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 138ms/step - loss: 0.0105 - acc: 0.9951 - val_loss: 0.0409 - val_acc: 0.9817\n",
      "Epoch 76/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9950\n",
      "Epoch 00076: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0105 - acc: 0.9951 - val_loss: 0.0356 - val_acc: 0.9837\n",
      "Epoch 77/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9951\n",
      "Epoch 00077: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0103 - acc: 0.9951 - val_loss: 0.0887 - val_acc: 0.9569\n",
      "Epoch 78/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9951\n",
      "Epoch 00078: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 140ms/step - loss: 0.0104 - acc: 0.9951 - val_loss: 0.0399 - val_acc: 0.9816\n",
      "Epoch 79/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9951\n",
      "Epoch 00079: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0103 - acc: 0.9951 - val_loss: 0.0357 - val_acc: 0.9835\n",
      "Epoch 80/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9953\n",
      "Epoch 00080: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0100 - acc: 0.9953 - val_loss: 0.0380 - val_acc: 0.9829\n",
      "Epoch 81/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9956\n",
      "Epoch 00081: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0093 - acc: 0.9956 - val_loss: 0.0335 - val_acc: 0.9847\n",
      "Epoch 82/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9956\n",
      "Epoch 00082: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0092 - acc: 0.9956 - val_loss: 0.0325 - val_acc: 0.9852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9956\n",
      "Epoch 00083: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0092 - acc: 0.9956 - val_loss: 0.0337 - val_acc: 0.9846\n",
      "Epoch 84/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9956\n",
      "Epoch 00084: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 137ms/step - loss: 0.0092 - acc: 0.9956 - val_loss: 0.0470 - val_acc: 0.9784\n",
      "Epoch 85/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9959\n",
      "Epoch 00085: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0087 - acc: 0.9959 - val_loss: 0.0368 - val_acc: 0.9828\n",
      "Epoch 86/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9960\n",
      "Epoch 00086: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0084 - acc: 0.9960 - val_loss: 0.0376 - val_acc: 0.9824\n",
      "Epoch 87/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9961\n",
      "Epoch 00087: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0082 - acc: 0.9961 - val_loss: 0.0453 - val_acc: 0.9801\n",
      "Epoch 88/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9960\n",
      "Epoch 00088: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0085 - acc: 0.9960 - val_loss: 0.0334 - val_acc: 0.9847\n",
      "Epoch 89/300\n",
      "180/181 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9962\n",
      "Epoch 00089: val_loss did not improve from 0.02943\n",
      "181/181 [==============================] - 25s 139ms/step - loss: 0.0080 - acc: 0.9962 - val_loss: 0.0403 - val_acc: 0.9818\n",
      "Epoch 00089: early stopping\n"
     ]
    }
   ],
   "source": [
    "adam = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer=adam,loss= dice_loss, metrics=[\"accuracy\"]) \n",
    "results = model.fit_generator(train_generator, steps_per_epoch=t_steps,  epochs=epochs,use_multiprocessing=False, \n",
    "                                  workers=0,validation_data=validation_generator,validation_steps=v_steps,callbacks=callbacks, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Learning curve\")\n",
    "plt.plot(results.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(results.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot( np.argmin(results.history[\"val_loss\"]), np.min(results.history[\"val_loss\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"log_loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prediction Steps.. 43\n",
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "path_testing_input = \"/tank/data/navchetan/Lars_Annotated_Datasets/Testing_Input\"\n",
    "path_testing_output = \"/tank/data/navchetan/Lars_Annotated_Datasets/Testing_Outer\"\n",
    "path = \"/tank/data/navchetan/Lars_Annotated_Datasets/\"\n",
    "os.chdir(path)\n",
    "indlist_testing_input = list(pd.read_csv('Testing_indices.csv')['Indices'].values)\n",
    "# indlist_validation_output = list(pd.read_csv('Training_indices.csv')['Indices'].values)\n",
    "csvFiletesting_Input = 'Testing_Input.csv'\n",
    "df_testing_input = pd.read_csv(csvFiletesting_Input)\n",
    "csvFiletesting_Output = 'Testing_Output.csv'\n",
    "df_testing_output = pd.read_csv(csvFiletesting_Output)\n",
    "BS = 8\n",
    "\n",
    "N_PRED=len(indlist_testing_input)\n",
    "pred_steps= math.ceil(N_PRED/batch_size)\n",
    "    \n",
    "\n",
    "\n",
    "pred_X_generator = CustomImgGenSC(path_testing_input,path_testing_output,indlist_testing_input\\\n",
    "                                 ,df_testing_input,df_testing_output\\\n",
    "                                 ,H=256,W=256,onlyX=True,shuffle=False,BATCH_SIZE=BS)\n",
    "    \n",
    "# path = \"/data/navchetan/Data/Testing\"\n",
    "# os.chdir(path)\n",
    "\n",
    "# Create and save result matrix batch by batch\n",
    "a = 0\n",
    "print('Running prediction Steps.. ' + str(pred_steps))\n",
    "count = 1\n",
    "for step in range(pred_steps): \n",
    "    pred_Y_predict = model.predict_generator(pred_X_generator,steps=1,use_multiprocessing=False,workers=0)\n",
    "    print(step,end=\" \")\n",
    "    b = np.shape(pred_Y_predict)[0]\n",
    "    short_lst = indlist_testing_input[a:a+b]\n",
    "    a+=b\n",
    "    pos = 0\n",
    "    os.chdir(path_results_save)\n",
    "    for i in short_lst:\n",
    "        S = pred_Y_predict[pos,:,:,0]\n",
    "        fsave = 'Segment_Output'+str(count).zfill(5)+'.mat'\n",
    "        sio.savemat(fsave, {'S':S})\n",
    "        pos+=1\n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Testing Data for computation of Jaccard Index\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "0.9723848845371436\n",
      "0.9652491815585027\n",
      "0.9381915205572595\n",
      "0.9671974405210618\n",
      "0.8852270736916329\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "cols = ['sensitivity','specificity','accuracy','dice_score','Jaccard']\n",
    "df = pd.DataFrame(columns=cols)\n",
    "path_testing_ground_truth = \"/tank/data/navchetan/Lars_Annotated_Datasets/Testing_Outer\"\n",
    "N_TESTING_SAMPLES = 342\n",
    "print('Loading Testing Data for computation of Jaccard Index')\n",
    "sensitivity_t = 0\n",
    "specificity_t = 0\n",
    "accuracy_t = 0\n",
    "dice_score_t = 0\n",
    "Jaccard_t = 0\n",
    "for i in range(N_TESTING_SAMPLES):\n",
    "    if(i%10 == 0):\n",
    "        print(i)\n",
    "    os.chdir(path_results_save)    \n",
    "    pathr = 'Segment_Output'+str(i+1).zfill(5)+'.mat'\n",
    "    x = sio.loadmat(pathr)\n",
    "    Segment_model = x['S']\n",
    "    Segment_model1 = Segment_model*255;\n",
    "    retval1, Segment_model_threshold = cv2.threshold(Segment_model1,100,255,cv2.THRESH_BINARY)\n",
    "    Segment_model_threshold1 = Segment_model_threshold/255.0\n",
    "    \n",
    "    os.chdir(path_testing_ground_truth)\n",
    "    path_t = 'Segment'+str(i+1).zfill(5)+'.mat'\n",
    "    y = sio.loadmat(path_t)\n",
    "    ground_truth_testing  = y['S']\n",
    "    ground_truth1 = ground_truth_testing*255;\n",
    "    retval2, ground_truth_testing_threshold = cv2.threshold(ground_truth1,0,255,cv2.THRESH_BINARY)\n",
    "    ground_truth_testing_threshold1 = ground_truth_testing_threshold/255.0\n",
    "\n",
    "    mcm = confusion_matrix(np.ndarray.flatten(Segment_model_threshold1),np.ndarray.flatten(ground_truth_testing_threshold1))\n",
    "    tn = mcm[0, 0]\n",
    "    fp = mcm[0, 1]\n",
    "    fn = mcm[1, 0]\n",
    "    tp = mcm[1, 1]\n",
    "    \n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    accuracy = (tp + tn) / (tp + tn + fn + fp)\n",
    "    dice_score = 2*tp / (2*tp + fp + fn)\n",
    "    Jaccard = dice_score / (2-dice_score)\n",
    "    \n",
    "    sensitivity_t = sensitivity_t + sensitivity\n",
    "    specificity_t = specificity_t + specificity\n",
    "    accuracy_t = accuracy_t + accuracy\n",
    "    dice_score_t = dice_score_t + dice_score\n",
    "    Jaccard_t = Jaccard_t + Jaccard\n",
    "    \n",
    "    \n",
    "    df = df.append(pd.Series([sensitivity,specificity,accuracy,dice_score,Jaccard],index=df.columns), ignore_index=True)\n",
    "\n",
    "os.chdir(saveFolder)\n",
    "df.to_csv(saving_metrics,index=False)\n",
    "print(sensitivity_t/N_TESTING_SAMPLES)\n",
    "print(specificity_t/N_TESTING_SAMPLES)\n",
    "print(dice_score_t/N_TESTING_SAMPLES)\n",
    "print(accuracy_t/N_TESTING_SAMPLES)\n",
    "print(Jaccard_t/N_TESTING_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n"
     ]
    }
   ],
   "source": [
    "O1 = np.zeros((N_TESTING_SAMPLES,256,256))\n",
    "O2 = np.zeros((N_TESTING_SAMPLES,256,256))\n",
    "Ground_Truth = np.zeros((N_TESTING_SAMPLES,256,256))\n",
    "Segmentation_from_model = np.zeros((N_TESTING_SAMPLES,256,256))\n",
    "\n",
    "path = \"/tank/data/navchetan/Lars_Annotated_Datasets/Testing_Input\"\n",
    "os.chdir(path)\n",
    "for i in range(N_TESTING_SAMPLES):\n",
    "    if(i%10 == 0):\n",
    "        print(i)\n",
    "    pathr = 'Image'+str(i+1).zfill(5)+'.mat'\n",
    "    x = sio.loadmat(pathr)\n",
    "    O1[i,:,:] = x['U']/255\n",
    "    O2[i,:,:] = x['U']/255\n",
    "    \n",
    "path = \"/tank/data/navchetan/Lars_Annotated_Datasets/Testing_Outer\"\n",
    "os.chdir(path)\n",
    "for i in range(N_TESTING_SAMPLES):\n",
    "    if(i%10 == 0):\n",
    "        print(i)\n",
    "    pathr = 'Segment'+str(i+1).zfill(5)+'.mat'\n",
    "    x = sio.loadmat(pathr)\n",
    "    Ground_Truth[i,:,:] = x['S']\n",
    "    retval2, ground_truth_testing_threshold = cv2.threshold(Ground_Truth[i,:,:],0,255,cv2.THRESH_BINARY)\n",
    "    Ground_Truth[i,:,:] = ground_truth_testing_threshold/255.0\n",
    "    \n",
    "\n",
    "os.chdir(path_results_save)\n",
    "for i in range(N_TESTING_SAMPLES):\n",
    "    if(i%10 == 0):\n",
    "        print(i)\n",
    "    pathr = 'Segment_Output'+str(i+1).zfill(5)+'.mat'\n",
    "    x = sio.loadmat(pathr)\n",
    "    Segmentation_from_model[i,:,:] = x['S']\n",
    "    Segment_model1 = Segmentation_from_model[i,:,:]*255;\n",
    "    retval1, Segment_model_threshold = cv2.threshold(Segment_model1,100,255,cv2.THRESH_BINARY)\n",
    "    Segmentation_from_model[i,:,:] = Segment_model_threshold/255.0\n",
    " \n",
    "def concat_images(imga, imgb):\n",
    "    \"\"\"\n",
    "    Combines two color image ndarrays side-by-side.\n",
    "    \"\"\"\n",
    "    [ha,wa] = np.shape(imga)\n",
    "    [hb,wb] = np.shape(imgb)\n",
    "    max_height = np.max([ha, hb])\n",
    "    total_width = wa+wb\n",
    "    new_img = np.zeros(shape=(max_height, total_width))\n",
    "    new_img[:ha,:wa]=imga\n",
    "    new_img[:hb,wa:wa+wb]=imgb\n",
    "    return new_img\n",
    "\n",
    "\n",
    "h = 256\n",
    "w = 256\n",
    "img_array = []    \n",
    "for k in range(N_TESTING_SAMPLES):\n",
    "    print(k)\n",
    "    for i in range(h):\n",
    "        for j in range (w):\n",
    "            if Ground_Truth[k,i,j]>0:\n",
    "                O1[k,i,j]=Ground_Truth[k,i,j]\n",
    "            if Segmentation_from_model[k,i,j]>0:\n",
    "                O2[k,i,j]=Segmentation_from_model[k,i,j]\n",
    "    W1 = O1[k,:,:]\n",
    "    W2 = O2[k,:,:]\n",
    "    img = concat_images(W1,W2)\n",
    "    img_array.append(img)\n",
    "    \n",
    "    os.chdir(path_images_save) \n",
    "    pathr = 'Joint_Image'+str(k+1).zfill(5)+'.png'\n",
    "    cv2.imwrite(pathr, img*255) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
