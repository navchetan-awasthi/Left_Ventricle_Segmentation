{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, BatchNormalization, UpSampling2D, concatenate\n",
    "# from tensorflow.keras.utils import np_utils\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras import backend as Ks\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,CSVLogger\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#from tensorflow.contrib.layers.python.layers import initializers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import skimage\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,CSVLogger\n",
    "import MiniNetMiscnnV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import skimage\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,CSVLogger\n",
    "import modelFCdDNN\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 256, 256, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 13) 117         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 128, 128, 1)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 128, 128, 14) 0           conv2d[0][0]                     \n",
      "                                                                 max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 14) 56          concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 64, 48)   6048        batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 14)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64, 64, 62)   0           conv2d_1[0][0]                   \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64, 64, 62)   248         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d (SeparableConv (None, 64, 64, 64)   4526        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 64)   256         separable_conv2d[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 64, 64, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_1 (SeparableCo (None, 64, 64, 64)   4672        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 64)   256         separable_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64, 64, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 64, 64, 64)   0           dropout_1[0][0]                  \n",
      "                                                                 dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_2 (SeparableCo (None, 64, 64, 64)   4672        add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64, 64, 64)   256         separable_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64, 64, 64)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64, 64, 64)   0           dropout_2[0][0]                  \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCo (None, 64, 64, 64)   4672        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 64, 64, 64)   256         separable_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 64, 64, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 64, 64, 64)   0           dropout_3[0][0]                  \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_4 (SeparableCo (None, 64, 64, 64)   4672        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64, 64, 64)   256         separable_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 64, 64, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 64, 64, 64)   0           dropout_4[0][0]                  \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_5 (SeparableCo (None, 64, 64, 64)   4672        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 64, 64, 64)   256         separable_conv2d_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 64, 64, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 64, 64, 64)   0           dropout_5[0][0]                  \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_6 (SeparableCo (None, 64, 64, 64)   4672        add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 64, 64)   256         separable_conv2d_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 64, 64, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 64, 64, 64)   0           dropout_6[0][0]                  \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_7 (SeparableCo (None, 64, 64, 64)   4672        add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64, 64, 64)   256         separable_conv2d_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 64, 64, 64)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 64, 64, 64)   0           dropout_7[0][0]                  \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_8 (SeparableCo (None, 64, 64, 64)   4672        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 64, 64, 64)   256         separable_conv2d_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 64, 64, 64)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 64, 64, 64)   0           dropout_8[0][0]                  \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_9 (SeparableCo (None, 64, 64, 64)   4672        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 64, 64, 64)   256         separable_conv2d_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 64, 64, 64)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 64, 64, 64)   0           dropout_9[0][0]                  \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 64)   36864       add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 64)   0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 128)  0           conv2d_2[0][0]                   \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 128)  512         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d (DepthwiseConv (None, 32, 32, 128)  1152        batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_1 (DepthwiseCo (None, 32, 32, 128)  1152        batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 32, 32, 128)  0           batch_normalization_13[0][0]     \n",
      "                                                                 batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 128)  16384       add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 32, 32, 128)  512         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 32, 32, 128)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 32, 32, 128)  0           dropout_10[0][0]                 \n",
      "                                                                 batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_2 (DepthwiseCo (None, 32, 32, 128)  1152        add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_3 (DepthwiseCo (None, 32, 32, 128)  1152        add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 32, 32, 128)  0           batch_normalization_16[0][0]     \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 128)  16384       add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 32, 32, 128)  512         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 32, 32, 128)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 32, 32, 128)  0           dropout_11[0][0]                 \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_4 (DepthwiseCo (None, 32, 32, 128)  1152        add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_5 (DepthwiseCo (None, 32, 32, 128)  1152        add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 32, 32, 128)  0           batch_normalization_19[0][0]     \n",
      "                                                                 batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 128)  16384       add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 32, 32, 128)  512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 32, 32, 128)  0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 32, 32, 128)  0           dropout_12[0][0]                 \n",
      "                                                                 add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_6 (DepthwiseCo (None, 32, 32, 128)  1152        add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_7 (DepthwiseCo (None, 32, 32, 128)  1152        add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 32, 32, 128)  0           batch_normalization_22[0][0]     \n",
      "                                                                 batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 128)  16384       add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 32, 32, 128)  512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 32, 32, 128)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 32, 32, 128)  0           dropout_13[0][0]                 \n",
      "                                                                 add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_8 (DepthwiseCo (None, 32, 32, 128)  1152        add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_9 (DepthwiseCo (None, 32, 32, 128)  1152        add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 32, 32, 128)  0           batch_normalization_25[0][0]     \n",
      "                                                                 batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 128)  16384       add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 32, 32, 128)  512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 32, 32, 128)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 32, 32, 128)  0           dropout_14[0][0]                 \n",
      "                                                                 add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_10 (DepthwiseC (None, 32, 32, 128)  1152        add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_11 (DepthwiseC (None, 32, 32, 128)  1152        add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 32, 32, 128)  0           batch_normalization_28[0][0]     \n",
      "                                                                 batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 128)  16384       add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 32, 32, 128)  512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 32, 32, 128)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 32, 32, 128)  0           dropout_15[0][0]                 \n",
      "                                                                 add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_12 (DepthwiseC (None, 32, 32, 128)  1152        add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_13 (DepthwiseC (None, 32, 32, 128)  1152        add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 32, 32, 128)  0           batch_normalization_31[0][0]     \n",
      "                                                                 batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 128)  16384       add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 32, 32, 128)  512         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 32, 32, 128)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 32, 32, 128)  0           dropout_16[0][0]                 \n",
      "                                                                 add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_14 (DepthwiseC (None, 32, 32, 128)  1152        add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_15 (DepthwiseC (None, 32, 32, 128)  1152        add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 32, 32, 128)  0           batch_normalization_34[0][0]     \n",
      "                                                                 batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 128)  16384       add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 32, 32, 128)  512         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 32, 32, 128)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 32, 32, 128)  0           dropout_17[0][0]                 \n",
      "                                                                 add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_16 (DepthwiseC (None, 32, 32, 128)  1152        add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_17 (DepthwiseC (None, 32, 32, 128)  1152        add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d_16[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d_17[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 32, 32, 128)  0           batch_normalization_37[0][0]     \n",
      "                                                                 batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 128)  16384       add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 32, 32, 128)  512         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 32, 32, 128)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 32, 32, 128)  0           dropout_18[0][0]                 \n",
      "                                                                 add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_18 (DepthwiseC (None, 32, 32, 128)  1152        add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_19 (DepthwiseC (None, 32, 32, 128)  1152        add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d_19[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 32, 32, 128)  0           batch_normalization_40[0][0]     \n",
      "                                                                 batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 128)  16384       add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 32, 32, 128)  512         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 32, 32, 128)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 32, 32, 128)  0           dropout_19[0][0]                 \n",
      "                                                                 add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_20 (DepthwiseC (None, 32, 32, 128)  1152        add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_21 (DepthwiseC (None, 32, 32, 128)  1152        add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d_20[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d_21[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 32, 32, 128)  0           batch_normalization_43[0][0]     \n",
      "                                                                 batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 128)  16384       add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 32, 32, 128)  512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 32, 32, 128)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 32, 32, 128)  0           dropout_20[0][0]                 \n",
      "                                                                 add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_22 (DepthwiseC (None, 32, 32, 128)  1152        add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_23 (DepthwiseC (None, 32, 32, 128)  1152        add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d_22[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d_23[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 32, 32, 128)  0           batch_normalization_46[0][0]     \n",
      "                                                                 batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 32, 128)  16384       add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 32, 32, 128)  512         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 32, 32, 128)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 32, 32, 128)  0           dropout_21[0][0]                 \n",
      "                                                                 add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_24 (DepthwiseC (None, 32, 32, 128)  1152        add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_25 (DepthwiseC (None, 32, 32, 128)  1152        add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d_24[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d_25[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 32, 32, 128)  0           batch_normalization_49[0][0]     \n",
      "                                                                 batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 32, 32, 128)  16384       add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 32, 32, 128)  512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 32, 32, 128)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 32, 32, 128)  0           dropout_22[0][0]                 \n",
      "                                                                 add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_26 (DepthwiseC (None, 32, 32, 128)  1152        add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_27 (DepthwiseC (None, 32, 32, 128)  1152        add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d_26[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d_27[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 32, 32, 128)  0           batch_normalization_52[0][0]     \n",
      "                                                                 batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 128)  16384       add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 32, 32, 128)  512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 32, 32, 128)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, 32, 32, 128)  0           dropout_23[0][0]                 \n",
      "                                                                 add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_28 (DepthwiseC (None, 32, 32, 128)  1152        add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_29 (DepthwiseC (None, 32, 32, 128)  1152        add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d_28[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d_29[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, 32, 32, 128)  0           batch_normalization_55[0][0]     \n",
      "                                                                 batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 128)  16384       add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 32, 32, 128)  512         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 32, 32, 128)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_38 (Add)                    (None, 32, 32, 128)  0           dropout_24[0][0]                 \n",
      "                                                                 add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_30 (DepthwiseC (None, 32, 32, 128)  1152        add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_31 (DepthwiseC (None, 32, 32, 128)  1152        add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d_30[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 32, 32, 128)  512         depthwise_conv2d_31[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_39 (Add)                    (None, 32, 32, 128)  0           batch_normalization_58[0][0]     \n",
      "                                                                 batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 32, 32, 128)  16384       add_39[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 128, 128, 15) 135         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 128, 128, 1)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 32, 32, 128)  512         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 128, 16) 0           conv2d_19[0][0]                  \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 32, 32, 128)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 128, 128, 16) 64          concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_40 (Add)                    (None, 32, 32, 128)  0           dropout_25[0][0]                 \n",
      "                                                                 add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 64, 64, 48)   6912        batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 64, 64, 16)   0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 64, 64, 64)   73792       add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 64, 64, 64)   0           conv2d_20[0][0]                  \n",
      "                                                                 max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 64, 64, 64)   256         conv2d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 64, 64, 64)   256         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 64, 64, 64)   0           batch_normalization_61[0][0]     \n",
      "                                                                 batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_10 (SeparableC (None, 64, 64, 64)   4672        tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 64, 64, 64)   256         separable_conv2d_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 64, 64, 64)   0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_41 (Add)                    (None, 64, 64, 64)   0           dropout_26[0][0]                 \n",
      "                                                                 tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_11 (SeparableC (None, 64, 64, 64)   4672        add_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 64, 64, 64)   256         separable_conv2d_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 64, 64, 64)   0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_42 (Add)                    (None, 64, 64, 64)   0           dropout_27[0][0]                 \n",
      "                                                                 add_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_12 (SeparableC (None, 64, 64, 64)   4672        add_42[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 64, 64, 64)   256         separable_conv2d_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 64, 64, 64)   0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_43 (Add)                    (None, 64, 64, 64)   0           dropout_28[0][0]                 \n",
      "                                                                 add_42[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_13 (SeparableC (None, 64, 64, 64)   4672        add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 64, 64, 64)   256         separable_conv2d_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 64, 64, 64)   0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_44 (Add)                    (None, 64, 64, 64)   0           dropout_29[0][0]                 \n",
      "                                                                 add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 128, 128, 1)  577         add_44[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf.image.resize (TFOpLambda)    (None, 256, 256, 1)  0           conv2d_transpose_1[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 518,267\n",
      "Trainable params: 503,491\n",
      "Non-trainable params: 14,776\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=MiniNetMiscnnV2.Architecture().create_model_3D(input_shape=(256,256,1))\n",
    "model.summary()\n",
    "#line 138 --> changed n_filters to 1\n",
    "#added command upsamling=2\n",
    "#in upsampling layer n_filter=n_labels\n",
    "# resize.bilinear command changed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customLoss(yTrue,yPred):\n",
    "    return (1e4)*(Ks.mean(Ks.square(yPred - yTrue), axis=-1))\n",
    "\n",
    "\n",
    "IMAGE_WIDTH = 256\n",
    "IMAGE_HEIGHT = 256\n",
    "N_TRAIN_SAMPLES = 3120\n",
    "CHANNELS = 1\n",
    "N_EVALUATE_SAMPLES = 821\n",
    "N_TESTING_SAMPLES = 695\n",
    "EVALUATE_FROM = 1\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.0001\n",
    "F = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomImgGenSC(path_train_input,path_train_output,indlst_train,\\\n",
    "                   df_train_input,df_train_output,\\\n",
    "                   H=256,W=256,onlyX=False,shuffle=False,BATCH_SIZE=16):\n",
    "    L_train = len(indlst_train)\n",
    "    while True:\n",
    "        if(shuffle):\n",
    "            random.shuffle(indlst_train)\n",
    "        ii = 0 # Current image index\n",
    "        left = L_train\n",
    "        while left>0:\n",
    "            BL = min(BATCH_SIZE,left)\n",
    "            X_BATCH = np.zeros((BL,H,W,1))\n",
    "            Y_BATCH = np.zeros((BL,H,W,1))\n",
    "            for bi in range(BL):\n",
    "                os.chdir(path_train_input)\n",
    "                imgIdx = indlst_train[ii] \n",
    "                pathr = 'Image'+str(imgIdx).zfill(5)+'.mat'\n",
    "                x = sio.loadmat(pathr)\n",
    "                X_BATCH[bi,:,:,0] = x['U']/255.0          \n",
    "                \n",
    "                if(not onlyX):\n",
    "                    os.chdir(path_train_output)\n",
    "                    pathr = 'Segment'+str(imgIdx).zfill(5)+'.mat'\n",
    "                    x = sio.loadmat(pathr)\n",
    "                    Y_BATCH[bi,:,:,0] = x['S']/255.0\n",
    "                ii+=1\n",
    "                \n",
    "            left = left - BL\n",
    "            if(not onlyX):\n",
    "                yield (X_BATCH,Y_BATCH)\n",
    "            else:\n",
    "                yield X_BATCH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Papillary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_input = \"/media/beta/navchetan-beps/Lars_BEP/Training sets/Inputs\" \n",
    "path_train_output = \"/media/beta/navchetan-beps/Lars_BEP/Training sets/Inner\"\n",
    "path = \"/media/beta/navchetan-beps/Lars_BEP/Training sets\"\n",
    "os.chdir(path)\n",
    "indlist_train_input = list(pd.read_csv('Training_indices.csv')['Indices'])\n",
    "#print(indlist_train_input)\n",
    "csvFileTrain_Input = 'Training_Input.csv'\n",
    "df_train_input = pd.read_csv(csvFileTrain_Input)\n",
    "csvFileTrain_Output = 'Training_Output.csv' #training with papillary\n",
    "df_train_output = pd.read_csv(csvFileTrain_Output)\n",
    "BS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = CustomImgGenSC(path_train_input,path_train_output,indlist_train_input\\\n",
    "                                 ,df_train_input,df_train_output\\\n",
    "                                 ,H=256,W=256,onlyX=False,shuffle=False,BATCH_SIZE=BS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load papillary validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_validation_input = \"/media/beta/navchetan-beps/Lars_BEP/Validation sets/Inputs\"\n",
    "path_validation_output = \"/media/beta/navchetan-beps/Lars_BEP/Validation sets/Inner\"\n",
    "path = \"/media/beta/navchetan-beps/Lars_BEP/Validation sets\"\n",
    "os.chdir(path)\n",
    "indlist_validation_input = list(pd.read_csv('Validation_indices.csv')['Indices'].values)\n",
    "#print(indlist_validation_input)\n",
    "# indlist_validation_output = list(pd.read_csv('Training_indices.csv')['Indices'].values)\n",
    "csvFilevalidation_Input = 'Validation_Input.csv'\n",
    "df_validation_input = pd.read_csv(csvFilevalidation_Input)\n",
    "csvFilevalidation_Output = 'Validation_Output.csv'\n",
    "df_validation_output = pd.read_csv(csvFilevalidation_Output)\n",
    "BS = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator = CustomImgGenSC(path_validation_input,path_validation_output,indlist_validation_input\\\n",
    "                                 ,df_validation_input,df_validation_output\\\n",
    "                                 ,H=256,W=256,onlyX=False,shuffle=False,BATCH_SIZE=BS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load no papillary validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train1.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train1.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-89956cfc9f1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# # load the  data for experiment 1 in the manuscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train1.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0minp\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lab'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \"\"\"\n\u001b[1;32m    221\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reader needs file name or open file-like object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train1.mat'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # load the  data for experiment 1 in the manuscript\n",
    "\n",
    "data = scipy.io.loadmat('train1.mat')\n",
    "inp  = data['inp']\n",
    "lab=data['lab']\n",
    "lab.shape\n",
    "traininp =np.reshape(np.transpose(inp,(2,0,1)),(100,512,512,1))\n",
    "trainlab =np.reshape(np.transpose(lab,(2,0,1)),(100,512,512,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=traininp/2\n",
    "y=trainlab/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_n = x[10,:,:,0]\n",
    "Y_n = y[10,:,:,0]\n",
    "plt.imshow(Y_n,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.amax(Y_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train , X_valid , y_train , y_valid = train_test_split(x , y , test_size = 0.2 , random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.5\n",
    "alpha = 0.35\n",
    "gamma = 2\n",
    "epsilon = 1e-5\n",
    "smooth = 1\n",
    "def focal_loss_with_logits(logits, targets, alpha, gamma, y_pred):\n",
    "    weight_a = alpha * (1 - y_pred) ** gamma * targets\n",
    "    weight_b = (1 - alpha) * y_pred ** gamma * (1 - targets)\n",
    "\n",
    "    return (tf.math.log1p(tf.exp(-tf.abs(logits))) + tf.nn.relu(\n",
    "        -logits)) * (weight_a + weight_b) + logits * weight_b\n",
    "    \n",
    "def focal_loss(y_true, y_pred):\n",
    "    y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(),\n",
    "                                  1 - tf.keras.backend.epsilon())\n",
    "    logits = tf.math.log(y_pred / (1 - y_pred))\n",
    "\n",
    "    loss = focal_loss_with_logits(logits=logits, targets=y_true,\n",
    "                                    alpha=alpha, gamma=gamma, y_pred=y_pred)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/beta/navchetan-beps/Lars_BEP/Test_outputs/MiniNet/Trained'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/media/beta/navchetan-beps/Lars_BEP/Test_outputs/MiniNet/Trained\"           \n",
    "os.chdir(path)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "batch_size = 8\n",
    "N_TRAIN=len(indlist_train_input)\n",
    "N_VALIDATE=len(indlist_validation_input)\n",
    "# N_TEST=len(indlist_testing_input)\n",
    "    \n",
    "t_steps = math.ceil(N_TRAIN/batch_size)\n",
    "v_steps = math.ceil(N_VALIDATE/batch_size)\n",
    "# tt_steps = math.ceil(N_TEST/batch_size)\n",
    "\n",
    "min_lr=0.0001\n",
    "epochs=300\n",
    "LR_patience=20\n",
    "LR_factor=0.1\n",
    "stop_patience=100\n",
    "decay_rate=min_lr/epochs\n",
    "retrainFlag=False\n",
    "modelName = 'MiniNetIn300'\n",
    "model_filepath = modelName + '.json'\n",
    "weights_filepath = modelName + '_weights.hdf5'\n",
    "saveFolder='/media/beta/navchetan-beps/Lars_BEP/Test_outputs/MiniNet/Trained'\n",
    "#model_json = model.to_json() # serialize model to JSON\n",
    "#with open(model_filepath, 'w') as json_file:\n",
    "#    json_file.write(model_json)\n",
    "\n",
    "modelSave = saveFolder + '/' + modelName + '.h5'\n",
    "callbacks = [EarlyStopping(patience=stop_patience, verbose=1),\n",
    "                 ReduceLROnPlateau(factor=LR_factor, patience=LR_patience, min_lr=min_lr, verbose=1),\n",
    "                 ModelCheckpoint(modelSave, verbose=1, save_best_only=True, save_weights_only=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lars-bep/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "169/169 [==============================] - 28s 129ms/step - loss: 0.1431 - accuracy: 0.9889 - val_loss: 0.1661 - val_accuracy: 0.9828\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.16613, saving model to /media/beta/navchetan-beps/Lars_BEP/Test_outputs/MiniNet/Trained/MiniNetIn300.h5\n",
      "Epoch 2/300\n",
      "169/169 [==============================] - 20s 121ms/step - loss: 0.1423 - accuracy: 0.9893 - val_loss: 0.1680 - val_accuracy: 0.9815\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.16613\n",
      "Epoch 3/300\n",
      "169/169 [==============================] - 22s 129ms/step - loss: 0.1411 - accuracy: 0.9890 - val_loss: 0.1614 - val_accuracy: 0.9827\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.16613 to 0.16139, saving model to /media/beta/navchetan-beps/Lars_BEP/Test_outputs/MiniNet/Trained/MiniNetIn300.h5\n",
      "Epoch 4/300\n",
      "169/169 [==============================] - 21s 123ms/step - loss: 0.1387 - accuracy: 0.9892 - val_loss: 0.1615 - val_accuracy: 0.9822\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.16139\n",
      "Epoch 5/300\n",
      "169/169 [==============================] - 22s 127ms/step - loss: 0.1377 - accuracy: 0.9892 - val_loss: 0.1755 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.16139\n",
      "Epoch 6/300\n",
      "169/169 [==============================] - 22s 127ms/step - loss: 0.1359 - accuracy: 0.9898 - val_loss: 0.1648 - val_accuracy: 0.9810\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.16139\n",
      "Epoch 7/300\n",
      "169/169 [==============================] - 21s 127ms/step - loss: 0.1343 - accuracy: 0.9900 - val_loss: 0.1605 - val_accuracy: 0.9802\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.16139 to 0.16049, saving model to /media/beta/navchetan-beps/Lars_BEP/Test_outputs/MiniNet/Trained/MiniNetIn300.h5\n",
      "Epoch 8/300\n",
      "169/169 [==============================] - 21s 121ms/step - loss: 0.1390 - accuracy: 0.9880 - val_loss: 0.1762 - val_accuracy: 0.9791\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.16049\n",
      "Epoch 9/300\n",
      "169/169 [==============================] - 21s 126ms/step - loss: 0.1350 - accuracy: 0.9898 - val_loss: 0.1684 - val_accuracy: 0.9798\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.16049\n",
      "Epoch 10/300\n",
      "169/169 [==============================] - 21s 126ms/step - loss: 0.1322 - accuracy: 0.9901 - val_loss: 0.2023 - val_accuracy: 0.9749\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.16049\n",
      "Epoch 11/300\n",
      "169/169 [==============================] - 21s 126ms/step - loss: 0.1305 - accuracy: 0.9903 - val_loss: 0.1879 - val_accuracy: 0.9758\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.16049\n",
      "Epoch 12/300\n",
      "169/169 [==============================] - 21s 127ms/step - loss: 0.1309 - accuracy: 0.9893 - val_loss: 0.1832 - val_accuracy: 0.9731\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.16049\n",
      "Epoch 13/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.1321 - accuracy: 0.9888 - val_loss: 0.1657 - val_accuracy: 0.9785\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.16049\n",
      "Epoch 14/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.1306 - accuracy: 0.9898 - val_loss: 0.2001 - val_accuracy: 0.9746\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.16049\n",
      "Epoch 15/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.1299 - accuracy: 0.9898 - val_loss: 0.2287 - val_accuracy: 0.9716\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.16049\n",
      "Epoch 16/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.1282 - accuracy: 0.9901 - val_loss: 0.1954 - val_accuracy: 0.9763\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.16049\n",
      "Epoch 17/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.1263 - accuracy: 0.9899 - val_loss: 0.2135 - val_accuracy: 0.9741\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.16049\n",
      "Epoch 18/300\n",
      "169/169 [==============================] - 22s 127ms/step - loss: 0.1270 - accuracy: 0.9898 - val_loss: 0.2179 - val_accuracy: 0.9730\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.16049\n",
      "Epoch 19/300\n",
      "169/169 [==============================] - 22s 129ms/step - loss: 0.1305 - accuracy: 0.9886 - val_loss: 0.2429 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.16049\n",
      "Epoch 20/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.1272 - accuracy: 0.9896 - val_loss: 0.3231 - val_accuracy: 0.9641\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.16049\n",
      "Epoch 21/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.1267 - accuracy: 0.9897 - val_loss: 0.2766 - val_accuracy: 0.9692\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.16049\n",
      "Epoch 22/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.1283 - accuracy: 0.9890 - val_loss: 0.3093 - val_accuracy: 0.9652\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.16049\n",
      "Epoch 23/300\n",
      "169/169 [==============================] - 22s 129ms/step - loss: 0.1248 - accuracy: 0.9901 - val_loss: 0.2215 - val_accuracy: 0.9733\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.16049\n",
      "Epoch 24/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.1256 - accuracy: 0.9899 - val_loss: 0.2084 - val_accuracy: 0.9735\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.16049\n",
      "Epoch 25/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.1308 - accuracy: 0.9874 - val_loss: 0.1807 - val_accuracy: 0.9789\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.16049\n",
      "Epoch 26/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.1251 - accuracy: 0.9887 - val_loss: 0.1701 - val_accuracy: 0.9786\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.16049\n",
      "Epoch 27/300\n",
      "169/169 [==============================] - 22s 129ms/step - loss: 0.1230 - accuracy: 0.9890 - val_loss: 0.2186 - val_accuracy: 0.9753\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.16049\n",
      "Epoch 28/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.1210 - accuracy: 0.9899 - val_loss: 0.2104 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.16049\n",
      "Epoch 29/300\n",
      "169/169 [==============================] - 22s 129ms/step - loss: 0.1230 - accuracy: 0.9895 - val_loss: 0.1943 - val_accuracy: 0.9772\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.16049\n",
      "Epoch 30/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.1205 - accuracy: 0.9901 - val_loss: 0.2411 - val_accuracy: 0.9723\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.16049\n",
      "Epoch 31/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.1210 - accuracy: 0.9895 - val_loss: 0.1901 - val_accuracy: 0.9768\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.16049\n",
      "Epoch 32/300\n",
      "169/169 [==============================] - 22s 129ms/step - loss: 0.1187 - accuracy: 0.9900 - val_loss: 0.2158 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.16049\n",
      "Epoch 33/300\n",
      "169/169 [==============================] - 22s 127ms/step - loss: 0.1169 - accuracy: 0.9905 - val_loss: 0.2259 - val_accuracy: 0.9743\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.16049\n",
      "Epoch 34/300\n",
      "169/169 [==============================] - 21s 126ms/step - loss: 0.1150 - accuracy: 0.9907 - val_loss: 0.2020 - val_accuracy: 0.9765\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.16049\n",
      "Epoch 35/300\n",
      "169/169 [==============================] - 21s 127ms/step - loss: 0.1140 - accuracy: 0.9909 - val_loss: 0.1963 - val_accuracy: 0.9761\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.16049\n",
      "Epoch 36/300\n",
      "169/169 [==============================] - 21s 124ms/step - loss: 0.1132 - accuracy: 0.9906 - val_loss: 0.2173 - val_accuracy: 0.9753\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.16049\n",
      "Epoch 37/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.1123 - accuracy: 0.9907 - val_loss: 0.2319 - val_accuracy: 0.9744\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.16049\n",
      "Epoch 38/300\n",
      "169/169 [==============================] - 21s 123ms/step - loss: 0.1114 - accuracy: 0.9910 - val_loss: 0.2133 - val_accuracy: 0.9758\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.16049\n",
      "Epoch 39/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.1127 - accuracy: 0.9904 - val_loss: 0.1980 - val_accuracy: 0.9769\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.16049\n",
      "Epoch 40/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.1114 - accuracy: 0.9907 - val_loss: 0.2182 - val_accuracy: 0.9736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00040: val_loss did not improve from 0.16049\n",
      "Epoch 41/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.1101 - accuracy: 0.9906 - val_loss: 0.1822 - val_accuracy: 0.9785\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.16049\n",
      "Epoch 42/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.1082 - accuracy: 0.9910 - val_loss: 0.2178 - val_accuracy: 0.9723\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.16049\n",
      "Epoch 43/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.1139 - accuracy: 0.9893 - val_loss: 0.1626 - val_accuracy: 0.9783\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.16049\n",
      "Epoch 44/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.1101 - accuracy: 0.9902 - val_loss: 0.2110 - val_accuracy: 0.9757\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.16049\n",
      "Epoch 45/300\n",
      "169/169 [==============================] - 21s 123ms/step - loss: 0.1093 - accuracy: 0.9902 - val_loss: 0.2156 - val_accuracy: 0.9737\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.16049\n",
      "Epoch 46/300\n",
      "169/169 [==============================] - 21s 124ms/step - loss: 0.1127 - accuracy: 0.9889 - val_loss: 0.2057 - val_accuracy: 0.9739\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.16049\n",
      "Epoch 47/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.1079 - accuracy: 0.9907 - val_loss: 0.1861 - val_accuracy: 0.9768\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.16049\n",
      "Epoch 48/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.1051 - accuracy: 0.9910 - val_loss: 0.2555 - val_accuracy: 0.9636\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.16049\n",
      "Epoch 49/300\n",
      "169/169 [==============================] - 21s 123ms/step - loss: 0.1066 - accuracy: 0.9904 - val_loss: 0.1912 - val_accuracy: 0.9769\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.16049\n",
      "Epoch 50/300\n",
      "169/169 [==============================] - 21s 123ms/step - loss: 0.1136 - accuracy: 0.9884 - val_loss: 0.1891 - val_accuracy: 0.9771\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.16049\n",
      "Epoch 51/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.1068 - accuracy: 0.9903 - val_loss: 0.1970 - val_accuracy: 0.9762\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.16049\n",
      "Epoch 52/300\n",
      "169/169 [==============================] - 21s 123ms/step - loss: 0.1052 - accuracy: 0.9902 - val_loss: 0.1796 - val_accuracy: 0.9770\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.16049\n",
      "Epoch 53/300\n",
      "169/169 [==============================] - 21s 123ms/step - loss: 0.1055 - accuracy: 0.9899 - val_loss: 0.1858 - val_accuracy: 0.9749\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.16049\n",
      "Epoch 54/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.1046 - accuracy: 0.9896 - val_loss: 0.2208 - val_accuracy: 0.9722\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.16049\n",
      "Epoch 55/300\n",
      "169/169 [==============================] - 20s 121ms/step - loss: 0.1049 - accuracy: 0.9899 - val_loss: 0.1845 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.16049\n",
      "Epoch 56/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.1029 - accuracy: 0.9905 - val_loss: 0.1806 - val_accuracy: 0.9769\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.16049\n",
      "Epoch 57/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.1008 - accuracy: 0.9909 - val_loss: 0.1856 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.16049\n",
      "Epoch 58/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.1005 - accuracy: 0.9907 - val_loss: 0.1899 - val_accuracy: 0.9771\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.16049\n",
      "Epoch 59/300\n",
      "169/169 [==============================] - 20s 121ms/step - loss: 0.0991 - accuracy: 0.9911 - val_loss: 0.1790 - val_accuracy: 0.9784\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.16049\n",
      "Epoch 60/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.0990 - accuracy: 0.9908 - val_loss: 0.1697 - val_accuracy: 0.9780\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.16049\n",
      "Epoch 61/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.0986 - accuracy: 0.9908 - val_loss: 0.1797 - val_accuracy: 0.9772\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.16049\n",
      "Epoch 62/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.0969 - accuracy: 0.9913 - val_loss: 0.1654 - val_accuracy: 0.9787\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.16049\n",
      "Epoch 63/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.0955 - accuracy: 0.9914 - val_loss: 0.2023 - val_accuracy: 0.9739\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.16049\n",
      "Epoch 64/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.0962 - accuracy: 0.9907 - val_loss: 0.1926 - val_accuracy: 0.9762\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.16049\n",
      "Epoch 65/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.0984 - accuracy: 0.9899 - val_loss: 0.1378 - val_accuracy: 0.9799\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.16049 to 0.13777, saving model to /media/beta/navchetan-beps/Lars_BEP/Test_outputs/MiniNet/Trained/MiniNetIn300.h5\n",
      "Epoch 66/300\n",
      "169/169 [==============================] - 20s 121ms/step - loss: 0.1015 - accuracy: 0.9901 - val_loss: 0.1944 - val_accuracy: 0.9745\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.13777\n",
      "Epoch 67/300\n",
      "169/169 [==============================] - 21s 123ms/step - loss: 0.0960 - accuracy: 0.9909 - val_loss: 0.2296 - val_accuracy: 0.9710\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.13777\n",
      "Epoch 68/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.0948 - accuracy: 0.9910 - val_loss: 0.2080 - val_accuracy: 0.9727\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.13777\n",
      "Epoch 69/300\n",
      "169/169 [==============================] - 21s 124ms/step - loss: 0.1049 - accuracy: 0.9879 - val_loss: 0.2304 - val_accuracy: 0.9709\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.13777\n",
      "Epoch 70/300\n",
      "169/169 [==============================] - 21s 123ms/step - loss: 0.0937 - accuracy: 0.9913 - val_loss: 0.2258 - val_accuracy: 0.9717\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.13777\n",
      "Epoch 71/300\n",
      "169/169 [==============================] - 21s 125ms/step - loss: 0.1014 - accuracy: 0.9894 - val_loss: 0.1507 - val_accuracy: 0.9801\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.13777\n",
      "Epoch 72/300\n",
      "169/169 [==============================] - 21s 123ms/step - loss: 0.0951 - accuracy: 0.9910 - val_loss: 0.1913 - val_accuracy: 0.9748\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.13777\n",
      "Epoch 73/300\n",
      "169/169 [==============================] - 22s 130ms/step - loss: 0.0948 - accuracy: 0.9904 - val_loss: 0.1859 - val_accuracy: 0.9750\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.13777\n",
      "Epoch 74/300\n",
      "169/169 [==============================] - 22s 131ms/step - loss: 0.0943 - accuracy: 0.9903 - val_loss: 0.2270 - val_accuracy: 0.9666\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.13777\n",
      "Epoch 75/300\n",
      "169/169 [==============================] - 24s 143ms/step - loss: 0.1011 - accuracy: 0.9885 - val_loss: 0.2475 - val_accuracy: 0.9691\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.13777\n",
      "Epoch 76/300\n",
      "169/169 [==============================] - 22s 132ms/step - loss: 0.0986 - accuracy: 0.9892 - val_loss: 0.2580 - val_accuracy: 0.9683\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.13777\n",
      "Epoch 77/300\n",
      "169/169 [==============================] - 22s 130ms/step - loss: 0.0961 - accuracy: 0.9900 - val_loss: 0.2510 - val_accuracy: 0.9698\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.13777\n",
      "Epoch 78/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.1016 - accuracy: 0.9888 - val_loss: 0.2778 - val_accuracy: 0.9667\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.13777\n",
      "Epoch 79/300\n",
      "169/169 [==============================] - 21s 125ms/step - loss: 0.0971 - accuracy: 0.9900 - val_loss: 0.2405 - val_accuracy: 0.9699\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.13777\n",
      "Epoch 80/300\n",
      "169/169 [==============================] - 21s 125ms/step - loss: 0.0957 - accuracy: 0.9900 - val_loss: 0.2019 - val_accuracy: 0.9721\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.13777\n",
      "Epoch 81/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 21s 124ms/step - loss: 0.0952 - accuracy: 0.9900 - val_loss: 0.2276 - val_accuracy: 0.9700\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.13777\n",
      "Epoch 82/300\n",
      "169/169 [==============================] - 21s 123ms/step - loss: 0.0944 - accuracy: 0.9899 - val_loss: 0.2800 - val_accuracy: 0.9670\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.13777\n",
      "Epoch 83/300\n",
      "169/169 [==============================] - 21s 124ms/step - loss: 0.0938 - accuracy: 0.9901 - val_loss: 0.2467 - val_accuracy: 0.9707\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.13777\n",
      "Epoch 84/300\n",
      "169/169 [==============================] - 21s 123ms/step - loss: 0.0908 - accuracy: 0.9909 - val_loss: 0.2742 - val_accuracy: 0.9687\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.13777\n",
      "Epoch 85/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.0896 - accuracy: 0.9912 - val_loss: 0.2413 - val_accuracy: 0.9714\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.13777\n",
      "Epoch 86/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.0896 - accuracy: 0.9911 - val_loss: 0.2421 - val_accuracy: 0.9716\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.13777\n",
      "Epoch 87/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.0893 - accuracy: 0.9913 - val_loss: 0.2596 - val_accuracy: 0.9700\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.13777\n",
      "Epoch 88/300\n",
      "169/169 [==============================] - 21s 123ms/step - loss: 0.0909 - accuracy: 0.9906 - val_loss: 0.2409 - val_accuracy: 0.9714\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.13777\n",
      "Epoch 89/300\n",
      "169/169 [==============================] - 21s 124ms/step - loss: 0.0900 - accuracy: 0.9911 - val_loss: 0.2488 - val_accuracy: 0.9710\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.13777\n",
      "Epoch 90/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.0895 - accuracy: 0.9911 - val_loss: 0.2264 - val_accuracy: 0.9724\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.13777\n",
      "Epoch 91/300\n",
      "169/169 [==============================] - 21s 123ms/step - loss: 0.0890 - accuracy: 0.9912 - val_loss: 0.2497 - val_accuracy: 0.9708\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.13777\n",
      "Epoch 92/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.0871 - accuracy: 0.9916 - val_loss: 0.2534 - val_accuracy: 0.9704\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.13777\n",
      "Epoch 93/300\n",
      "169/169 [==============================] - 108s 642ms/step - loss: 0.0868 - accuracy: 0.9916 - val_loss: 0.2934 - val_accuracy: 0.9670\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.13777\n",
      "Epoch 94/300\n",
      "169/169 [==============================] - 26s 155ms/step - loss: 0.0861 - accuracy: 0.9917 - val_loss: 0.2819 - val_accuracy: 0.9686\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.13777\n",
      "Epoch 95/300\n",
      "169/169 [==============================] - 76s 454ms/step - loss: 0.0859 - accuracy: 0.9918 - val_loss: 0.2975 - val_accuracy: 0.9670\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.13777\n",
      "Epoch 96/300\n",
      "169/169 [==============================] - 99s 481ms/step - loss: 0.0853 - accuracy: 0.9918 - val_loss: 0.2763 - val_accuracy: 0.9691\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.13777\n",
      "Epoch 97/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.0886 - accuracy: 0.9903 - val_loss: 0.2738 - val_accuracy: 0.9693\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.13777\n",
      "Epoch 98/300\n",
      "169/169 [==============================] - 20s 121ms/step - loss: 0.0917 - accuracy: 0.9903 - val_loss: 0.2791 - val_accuracy: 0.9688\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.13777\n",
      "Epoch 99/300\n",
      "169/169 [==============================] - 20s 121ms/step - loss: 0.0883 - accuracy: 0.9911 - val_loss: 0.2797 - val_accuracy: 0.9686\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.13777\n",
      "Epoch 100/300\n",
      "169/169 [==============================] - 21s 123ms/step - loss: 0.0876 - accuracy: 0.9910 - val_loss: 0.2122 - val_accuracy: 0.9737\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.13777\n",
      "Epoch 101/300\n",
      "169/169 [==============================] - 21s 123ms/step - loss: 0.0861 - accuracy: 0.9912 - val_loss: 0.2342 - val_accuracy: 0.9719\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.13777\n",
      "Epoch 102/300\n",
      "169/169 [==============================] - 20s 121ms/step - loss: 0.0850 - accuracy: 0.9917 - val_loss: 0.2339 - val_accuracy: 0.9718\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.13777\n",
      "Epoch 103/300\n",
      "169/169 [==============================] - 21s 121ms/step - loss: 0.0842 - accuracy: 0.9917 - val_loss: 0.2328 - val_accuracy: 0.9717\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.13777\n",
      "Epoch 104/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.0837 - accuracy: 0.9917 - val_loss: 0.1835 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.13777\n",
      "Epoch 105/300\n",
      "169/169 [==============================] - 21s 123ms/step - loss: 0.0837 - accuracy: 0.9917 - val_loss: 0.2285 - val_accuracy: 0.9729\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.13777\n",
      "Epoch 106/300\n",
      "169/169 [==============================] - 21s 123ms/step - loss: 0.0816 - accuracy: 0.9922 - val_loss: 0.2386 - val_accuracy: 0.9717\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.13777\n",
      "Epoch 107/300\n",
      "169/169 [==============================] - 21s 123ms/step - loss: 0.0806 - accuracy: 0.9924 - val_loss: 0.2501 - val_accuracy: 0.9707\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.13777\n",
      "Epoch 108/300\n",
      "169/169 [==============================] - 20s 121ms/step - loss: 0.0806 - accuracy: 0.9923 - val_loss: 0.2272 - val_accuracy: 0.9731\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.13777\n",
      "Epoch 109/300\n",
      "169/169 [==============================] - 21s 121ms/step - loss: 0.0923 - accuracy: 0.9896 - val_loss: 0.1996 - val_accuracy: 0.9695\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.13777\n",
      "Epoch 110/300\n",
      "169/169 [==============================] - 21s 123ms/step - loss: 0.0923 - accuracy: 0.9886 - val_loss: 0.1332 - val_accuracy: 0.9805\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.13777 to 0.13322, saving model to /media/beta/navchetan-beps/Lars_BEP/Test_outputs/MiniNet/Trained/MiniNetIn300.h5\n",
      "Epoch 111/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.0888 - accuracy: 0.9898 - val_loss: 0.1450 - val_accuracy: 0.9797\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.13322\n",
      "Epoch 112/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.0855 - accuracy: 0.9907 - val_loss: 0.1339 - val_accuracy: 0.9806\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.13322\n",
      "Epoch 113/300\n",
      "169/169 [==============================] - 21s 127ms/step - loss: 0.0843 - accuracy: 0.9910 - val_loss: 0.1347 - val_accuracy: 0.9811\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.13322\n",
      "Epoch 114/300\n",
      "169/169 [==============================] - 21s 127ms/step - loss: 0.0836 - accuracy: 0.9912 - val_loss: 0.1510 - val_accuracy: 0.9792\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.13322\n",
      "Epoch 115/300\n",
      "169/169 [==============================] - 21s 126ms/step - loss: 0.0818 - accuracy: 0.9916 - val_loss: 0.1471 - val_accuracy: 0.9797\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.13322\n",
      "Epoch 116/300\n",
      "169/169 [==============================] - 21s 126ms/step - loss: 0.0809 - accuracy: 0.9917 - val_loss: 0.1489 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.13322\n",
      "Epoch 117/300\n",
      "169/169 [==============================] - 21s 126ms/step - loss: 0.0824 - accuracy: 0.9914 - val_loss: 0.1365 - val_accuracy: 0.9807\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.13322\n",
      "Epoch 118/300\n",
      "169/169 [==============================] - 21s 125ms/step - loss: 0.0805 - accuracy: 0.9917 - val_loss: 0.1579 - val_accuracy: 0.9783\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.13322\n",
      "Epoch 119/300\n",
      "169/169 [==============================] - 21s 126ms/step - loss: 0.0787 - accuracy: 0.9921 - val_loss: 0.1546 - val_accuracy: 0.9791\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.13322\n",
      "Epoch 120/300\n",
      "169/169 [==============================] - 21s 126ms/step - loss: 0.0787 - accuracy: 0.9920 - val_loss: 0.1689 - val_accuracy: 0.9754\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.13322\n",
      "Epoch 121/300\n",
      "169/169 [==============================] - 21s 127ms/step - loss: 0.0845 - accuracy: 0.9902 - val_loss: 0.1304 - val_accuracy: 0.9811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00121: val_loss improved from 0.13322 to 0.13042, saving model to /media/beta/navchetan-beps/Lars_BEP/Test_outputs/MiniNet/Trained/MiniNetIn300.h5\n",
      "Epoch 122/300\n",
      "169/169 [==============================] - 21s 125ms/step - loss: 0.0839 - accuracy: 0.9911 - val_loss: 0.1647 - val_accuracy: 0.9771\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.13042\n",
      "Epoch 123/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.0824 - accuracy: 0.9910 - val_loss: 0.1147 - val_accuracy: 0.9817\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.13042 to 0.11473, saving model to /media/beta/navchetan-beps/Lars_BEP/Test_outputs/MiniNet/Trained/MiniNetIn300.h5\n",
      "Epoch 124/300\n",
      "169/169 [==============================] - 21s 125ms/step - loss: 0.0894 - accuracy: 0.9891 - val_loss: 0.1525 - val_accuracy: 0.9771\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.11473\n",
      "Epoch 125/300\n",
      "169/169 [==============================] - 21s 126ms/step - loss: 0.0830 - accuracy: 0.9912 - val_loss: 0.1135 - val_accuracy: 0.9824\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.11473 to 0.11353, saving model to /media/beta/navchetan-beps/Lars_BEP/Test_outputs/MiniNet/Trained/MiniNetIn300.h5\n",
      "Epoch 126/300\n",
      "169/169 [==============================] - 21s 125ms/step - loss: 0.0856 - accuracy: 0.9897 - val_loss: 0.1997 - val_accuracy: 0.9723\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.11353\n",
      "Epoch 127/300\n",
      "169/169 [==============================] - 21s 127ms/step - loss: 0.0843 - accuracy: 0.9903 - val_loss: 0.1374 - val_accuracy: 0.9777\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.11353\n",
      "Epoch 128/300\n",
      "169/169 [==============================] - 21s 127ms/step - loss: 0.0960 - accuracy: 0.9872 - val_loss: 0.1503 - val_accuracy: 0.9753\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.11353\n",
      "Epoch 129/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.0905 - accuracy: 0.9892 - val_loss: 0.1365 - val_accuracy: 0.9772\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.11353\n",
      "Epoch 130/300\n",
      "169/169 [==============================] - 21s 127ms/step - loss: 0.0847 - accuracy: 0.9900 - val_loss: 0.1615 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.11353\n",
      "Epoch 131/300\n",
      "169/169 [==============================] - 22s 127ms/step - loss: 0.0820 - accuracy: 0.9905 - val_loss: 0.1764 - val_accuracy: 0.9753\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.11353\n",
      "Epoch 132/300\n",
      "169/169 [==============================] - 21s 127ms/step - loss: 0.0819 - accuracy: 0.9904 - val_loss: 0.1591 - val_accuracy: 0.9761\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.11353\n",
      "Epoch 133/300\n",
      "169/169 [==============================] - 21s 123ms/step - loss: 0.0816 - accuracy: 0.9905 - val_loss: 0.1374 - val_accuracy: 0.9789\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.11353\n",
      "Epoch 134/300\n",
      "169/169 [==============================] - 20s 121ms/step - loss: 0.0805 - accuracy: 0.9909 - val_loss: 0.1455 - val_accuracy: 0.9782\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.11353\n",
      "Epoch 135/300\n",
      "169/169 [==============================] - 20s 121ms/step - loss: 0.0797 - accuracy: 0.9910 - val_loss: 0.1483 - val_accuracy: 0.9775\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.11353\n",
      "Epoch 136/300\n",
      "169/169 [==============================] - 20s 120ms/step - loss: 0.0791 - accuracy: 0.9913 - val_loss: 0.1552 - val_accuracy: 0.9773\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.11353\n",
      "Epoch 137/300\n",
      "169/169 [==============================] - 20s 121ms/step - loss: 0.0781 - accuracy: 0.9914 - val_loss: 0.1559 - val_accuracy: 0.9772\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.11353\n",
      "Epoch 138/300\n",
      "169/169 [==============================] - 20s 121ms/step - loss: 0.0767 - accuracy: 0.9916 - val_loss: 0.1627 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.11353\n",
      "Epoch 139/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.0761 - accuracy: 0.9919 - val_loss: 0.1879 - val_accuracy: 0.9748\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.11353\n",
      "Epoch 140/300\n",
      "169/169 [==============================] - 20s 121ms/step - loss: 0.0754 - accuracy: 0.9921 - val_loss: 0.2068 - val_accuracy: 0.9734\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.11353\n",
      "Epoch 141/300\n",
      "169/169 [==============================] - 21s 123ms/step - loss: 0.0745 - accuracy: 0.9923 - val_loss: 0.1820 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.11353\n",
      "Epoch 142/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.0742 - accuracy: 0.9924 - val_loss: 0.2066 - val_accuracy: 0.9738\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.11353\n",
      "Epoch 143/300\n",
      "169/169 [==============================] - 20s 121ms/step - loss: 0.0761 - accuracy: 0.9914 - val_loss: 0.2069 - val_accuracy: 0.9726\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.11353\n",
      "Epoch 144/300\n",
      "169/169 [==============================] - 20s 121ms/step - loss: 0.0773 - accuracy: 0.9914 - val_loss: 0.1859 - val_accuracy: 0.9751\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.11353\n",
      "Epoch 145/300\n",
      "169/169 [==============================] - 20s 121ms/step - loss: 0.0734 - accuracy: 0.9925 - val_loss: 0.1736 - val_accuracy: 0.9762\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.11353\n",
      "Epoch 146/300\n",
      "169/169 [==============================] - 21s 124ms/step - loss: 0.0729 - accuracy: 0.9925 - val_loss: 0.2120 - val_accuracy: 0.9731\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.11353\n",
      "Epoch 147/300\n",
      "169/169 [==============================] - 21s 123ms/step - loss: 0.0724 - accuracy: 0.9927 - val_loss: 0.2069 - val_accuracy: 0.9740\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.11353\n",
      "Epoch 148/300\n",
      "169/169 [==============================] - 21s 124ms/step - loss: 0.0779 - accuracy: 0.9912 - val_loss: 0.1632 - val_accuracy: 0.9758\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.11353\n",
      "Epoch 149/300\n",
      "169/169 [==============================] - 21s 123ms/step - loss: 0.0800 - accuracy: 0.9904 - val_loss: 0.1594 - val_accuracy: 0.9767\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.11353\n",
      "Epoch 150/300\n",
      "169/169 [==============================] - 21s 125ms/step - loss: 0.0765 - accuracy: 0.9911 - val_loss: 0.1345 - val_accuracy: 0.9808\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.11353\n",
      "Epoch 151/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.0749 - accuracy: 0.9918 - val_loss: 0.1389 - val_accuracy: 0.9784\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.11353\n",
      "Epoch 152/300\n",
      "169/169 [==============================] - 21s 123ms/step - loss: 0.0784 - accuracy: 0.9908 - val_loss: 0.1606 - val_accuracy: 0.9776\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.11353\n",
      "Epoch 153/300\n",
      "169/169 [==============================] - 21s 123ms/step - loss: 0.0728 - accuracy: 0.9921 - val_loss: 0.1941 - val_accuracy: 0.9741\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.11353\n",
      "Epoch 154/300\n",
      "169/169 [==============================] - 20s 120ms/step - loss: 0.0831 - accuracy: 0.9893 - val_loss: 0.2099 - val_accuracy: 0.9692\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.11353\n",
      "Epoch 155/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.0837 - accuracy: 0.9893 - val_loss: 0.1662 - val_accuracy: 0.9742\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.11353\n",
      "Epoch 156/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.0775 - accuracy: 0.9905 - val_loss: 0.1625 - val_accuracy: 0.9753\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.11353\n",
      "Epoch 157/300\n",
      "169/169 [==============================] - 21s 123ms/step - loss: 0.0764 - accuracy: 0.9908 - val_loss: 0.1569 - val_accuracy: 0.9765\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.11353\n",
      "Epoch 158/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.0748 - accuracy: 0.9911 - val_loss: 0.1741 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.11353\n",
      "Epoch 159/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.0739 - accuracy: 0.9913 - val_loss: 0.1751 - val_accuracy: 0.9751\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.11353\n",
      "Epoch 160/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.0729 - accuracy: 0.9917 - val_loss: 0.1642 - val_accuracy: 0.9764\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.11353\n",
      "Epoch 161/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 21s 127ms/step - loss: 0.0726 - accuracy: 0.9918 - val_loss: 0.1647 - val_accuracy: 0.9759\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.11353\n",
      "Epoch 162/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.0714 - accuracy: 0.9922 - val_loss: 0.1677 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.11353\n",
      "Epoch 163/300\n",
      "169/169 [==============================] - 21s 126ms/step - loss: 0.0707 - accuracy: 0.9925 - val_loss: 0.1650 - val_accuracy: 0.9767\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.11353\n",
      "Epoch 164/300\n",
      "169/169 [==============================] - 21s 126ms/step - loss: 0.0700 - accuracy: 0.9927 - val_loss: 0.1808 - val_accuracy: 0.9753\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.11353\n",
      "Epoch 165/300\n",
      "169/169 [==============================] - 21s 127ms/step - loss: 0.0704 - accuracy: 0.9925 - val_loss: 0.1742 - val_accuracy: 0.9736\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.11353\n",
      "Epoch 166/300\n",
      "169/169 [==============================] - 21s 127ms/step - loss: 0.0795 - accuracy: 0.9897 - val_loss: 0.1898 - val_accuracy: 0.9741\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.11353\n",
      "Epoch 167/300\n",
      "169/169 [==============================] - 21s 126ms/step - loss: 0.0736 - accuracy: 0.9914 - val_loss: 0.1805 - val_accuracy: 0.9754\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.11353\n",
      "Epoch 168/300\n",
      "169/169 [==============================] - 21s 127ms/step - loss: 0.0720 - accuracy: 0.9918 - val_loss: 0.1692 - val_accuracy: 0.9764\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.11353\n",
      "Epoch 169/300\n",
      "169/169 [==============================] - 21s 126ms/step - loss: 0.0758 - accuracy: 0.9907 - val_loss: 0.1903 - val_accuracy: 0.9703\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.11353\n",
      "Epoch 170/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.0796 - accuracy: 0.9895 - val_loss: 0.1305 - val_accuracy: 0.9790\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.11353\n",
      "Epoch 171/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.0761 - accuracy: 0.9905 - val_loss: 0.1328 - val_accuracy: 0.9790\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.11353\n",
      "Epoch 172/300\n",
      "169/169 [==============================] - 21s 127ms/step - loss: 0.0729 - accuracy: 0.9913 - val_loss: 0.1219 - val_accuracy: 0.9802\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.11353\n",
      "Epoch 173/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.0722 - accuracy: 0.9915 - val_loss: 0.1389 - val_accuracy: 0.9786\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.11353\n",
      "Epoch 174/300\n",
      "169/169 [==============================] - 21s 127ms/step - loss: 0.0714 - accuracy: 0.9918 - val_loss: 0.1353 - val_accuracy: 0.9789\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.11353\n",
      "Epoch 175/300\n",
      "169/169 [==============================] - 21s 127ms/step - loss: 0.0710 - accuracy: 0.9919 - val_loss: 0.1387 - val_accuracy: 0.9783\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.11353\n",
      "Epoch 176/300\n",
      "169/169 [==============================] - 21s 127ms/step - loss: 0.0692 - accuracy: 0.9925 - val_loss: 0.1569 - val_accuracy: 0.9764\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.11353\n",
      "Epoch 177/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.0690 - accuracy: 0.9926 - val_loss: 0.1700 - val_accuracy: 0.9753\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.11353\n",
      "Epoch 178/300\n",
      "169/169 [==============================] - 22s 127ms/step - loss: 0.0688 - accuracy: 0.9926 - val_loss: 0.2445 - val_accuracy: 0.9695\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.11353\n",
      "Epoch 179/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.0683 - accuracy: 0.9926 - val_loss: 0.2014 - val_accuracy: 0.9729\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.11353\n",
      "Epoch 180/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.0671 - accuracy: 0.9931 - val_loss: 0.2093 - val_accuracy: 0.9726\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.11353\n",
      "Epoch 181/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.0664 - accuracy: 0.9934 - val_loss: 0.1783 - val_accuracy: 0.9740\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.11353\n",
      "Epoch 182/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.0687 - accuracy: 0.9927 - val_loss: 0.1908 - val_accuracy: 0.9741\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.11353\n",
      "Epoch 183/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.0943 - accuracy: 0.9875 - val_loss: 0.2209 - val_accuracy: 0.9718\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.11353\n",
      "Epoch 184/300\n",
      "169/169 [==============================] - 21s 127ms/step - loss: 0.0820 - accuracy: 0.9893 - val_loss: 0.2233 - val_accuracy: 0.9723\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.11353\n",
      "Epoch 185/300\n",
      "169/169 [==============================] - 21s 127ms/step - loss: 0.0760 - accuracy: 0.9902 - val_loss: 0.2386 - val_accuracy: 0.9715\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.11353\n",
      "Epoch 186/300\n",
      "169/169 [==============================] - 21s 126ms/step - loss: 0.0723 - accuracy: 0.9913 - val_loss: 0.2496 - val_accuracy: 0.9715\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.11353\n",
      "Epoch 187/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.0709 - accuracy: 0.9916 - val_loss: 0.2626 - val_accuracy: 0.9706\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.11353\n",
      "Epoch 188/300\n",
      "169/169 [==============================] - 21s 127ms/step - loss: 0.0766 - accuracy: 0.9901 - val_loss: 0.1984 - val_accuracy: 0.9747\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.11353\n",
      "Epoch 189/300\n",
      "169/169 [==============================] - 21s 127ms/step - loss: 0.0773 - accuracy: 0.9901 - val_loss: 0.2008 - val_accuracy: 0.9746\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.11353\n",
      "Epoch 190/300\n",
      "169/169 [==============================] - 21s 126ms/step - loss: 0.0750 - accuracy: 0.9906 - val_loss: 0.2102 - val_accuracy: 0.9745\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.11353\n",
      "Epoch 191/300\n",
      "169/169 [==============================] - 21s 126ms/step - loss: 0.0724 - accuracy: 0.9911 - val_loss: 0.2067 - val_accuracy: 0.9745\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.11353\n",
      "Epoch 192/300\n",
      "169/169 [==============================] - 21s 126ms/step - loss: 0.0710 - accuracy: 0.9914 - val_loss: 0.2140 - val_accuracy: 0.9742\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.11353\n",
      "Epoch 193/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.0700 - accuracy: 0.9917 - val_loss: 0.2078 - val_accuracy: 0.9743\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.11353\n",
      "Epoch 194/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.0695 - accuracy: 0.9918 - val_loss: 0.2131 - val_accuracy: 0.9742\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.11353\n",
      "Epoch 195/300\n",
      "169/169 [==============================] - 21s 126ms/step - loss: 0.0685 - accuracy: 0.9921 - val_loss: 0.2140 - val_accuracy: 0.9742\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.11353\n",
      "Epoch 196/300\n",
      "169/169 [==============================] - 21s 126ms/step - loss: 0.0670 - accuracy: 0.9926 - val_loss: 0.2249 - val_accuracy: 0.9737\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.11353\n",
      "Epoch 197/300\n",
      "169/169 [==============================] - 21s 127ms/step - loss: 0.0674 - accuracy: 0.9924 - val_loss: 0.1881 - val_accuracy: 0.9754\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.11353\n",
      "Epoch 198/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.0670 - accuracy: 0.9925 - val_loss: 0.2258 - val_accuracy: 0.9732\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.11353\n",
      "Epoch 199/300\n",
      "169/169 [==============================] - 21s 126ms/step - loss: 0.0670 - accuracy: 0.9925 - val_loss: 0.2304 - val_accuracy: 0.9730\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.11353\n",
      "Epoch 200/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.0656 - accuracy: 0.9929 - val_loss: 0.2260 - val_accuracy: 0.9735\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.11353\n",
      "Epoch 201/300\n",
      "169/169 [==============================] - 21s 126ms/step - loss: 0.0651 - accuracy: 0.9932 - val_loss: 0.2024 - val_accuracy: 0.9750\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.11353\n",
      "Epoch 202/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 21s 126ms/step - loss: 0.0646 - accuracy: 0.9933 - val_loss: 0.2224 - val_accuracy: 0.9740\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.11353\n",
      "Epoch 203/300\n",
      "169/169 [==============================] - 21s 124ms/step - loss: 0.0637 - accuracy: 0.9936 - val_loss: 0.1826 - val_accuracy: 0.9764\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.11353\n",
      "Epoch 204/300\n",
      "169/169 [==============================] - 21s 125ms/step - loss: 0.0637 - accuracy: 0.9935 - val_loss: 0.2149 - val_accuracy: 0.9744\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.11353\n",
      "Epoch 205/300\n",
      "169/169 [==============================] - 21s 125ms/step - loss: 0.0628 - accuracy: 0.9938 - val_loss: 0.2139 - val_accuracy: 0.9747\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.11353\n",
      "Epoch 206/300\n",
      "169/169 [==============================] - 21s 126ms/step - loss: 0.0862 - accuracy: 0.9897 - val_loss: 0.2346 - val_accuracy: 0.9703\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.11353\n",
      "Epoch 207/300\n",
      "169/169 [==============================] - 21s 126ms/step - loss: 0.0848 - accuracy: 0.9887 - val_loss: 0.2021 - val_accuracy: 0.9743\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.11353\n",
      "Epoch 208/300\n",
      "169/169 [==============================] - 21s 126ms/step - loss: 0.0721 - accuracy: 0.9910 - val_loss: 0.1964 - val_accuracy: 0.9741\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.11353\n",
      "Epoch 209/300\n",
      "169/169 [==============================] - 21s 125ms/step - loss: 0.0696 - accuracy: 0.9912 - val_loss: 0.2016 - val_accuracy: 0.9745\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.11353\n",
      "Epoch 210/300\n",
      "169/169 [==============================] - 21s 126ms/step - loss: 0.0671 - accuracy: 0.9922 - val_loss: 0.2106 - val_accuracy: 0.9740\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.11353\n",
      "Epoch 211/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.0661 - accuracy: 0.9925 - val_loss: 0.2086 - val_accuracy: 0.9741\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.11353\n",
      "Epoch 212/300\n",
      "169/169 [==============================] - 21s 127ms/step - loss: 0.0654 - accuracy: 0.9926 - val_loss: 0.2067 - val_accuracy: 0.9741\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.11353\n",
      "Epoch 213/300\n",
      "169/169 [==============================] - 21s 127ms/step - loss: 0.0642 - accuracy: 0.9930 - val_loss: 0.2116 - val_accuracy: 0.9742\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.11353\n",
      "Epoch 214/300\n",
      "169/169 [==============================] - 21s 127ms/step - loss: 0.0635 - accuracy: 0.9932 - val_loss: 0.2345 - val_accuracy: 0.9724\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.11353\n",
      "Epoch 215/300\n",
      "169/169 [==============================] - 21s 127ms/step - loss: 0.0630 - accuracy: 0.9934 - val_loss: 0.2365 - val_accuracy: 0.9718\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.11353\n",
      "Epoch 216/300\n",
      "169/169 [==============================] - 21s 127ms/step - loss: 0.0636 - accuracy: 0.9931 - val_loss: 0.1969 - val_accuracy: 0.9722\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.11353\n",
      "Epoch 217/300\n",
      "169/169 [==============================] - 21s 127ms/step - loss: 0.0685 - accuracy: 0.9916 - val_loss: 0.2866 - val_accuracy: 0.9675\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.11353\n",
      "Epoch 218/300\n",
      "169/169 [==============================] - 21s 127ms/step - loss: 0.0694 - accuracy: 0.9917 - val_loss: 0.1446 - val_accuracy: 0.9788\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.11353\n",
      "Epoch 219/300\n",
      "169/169 [==============================] - 22s 128ms/step - loss: 0.0650 - accuracy: 0.9929 - val_loss: 0.1598 - val_accuracy: 0.9775\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.11353\n",
      "Epoch 220/300\n",
      "169/169 [==============================] - 21s 127ms/step - loss: 0.0633 - accuracy: 0.9933 - val_loss: 0.2021 - val_accuracy: 0.9747\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.11353\n",
      "Epoch 221/300\n",
      "169/169 [==============================] - 21s 127ms/step - loss: 0.0629 - accuracy: 0.9932 - val_loss: 0.1984 - val_accuracy: 0.9747\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.11353\n",
      "Epoch 222/300\n",
      "169/169 [==============================] - 21s 124ms/step - loss: 0.0619 - accuracy: 0.9936 - val_loss: 0.1435 - val_accuracy: 0.9790\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.11353\n",
      "Epoch 223/300\n",
      "169/169 [==============================] - 20s 121ms/step - loss: 0.0632 - accuracy: 0.9929 - val_loss: 0.1893 - val_accuracy: 0.9757\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.11353\n",
      "Epoch 224/300\n",
      "169/169 [==============================] - 20s 121ms/step - loss: 0.0614 - accuracy: 0.9936 - val_loss: 0.1987 - val_accuracy: 0.9745\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.11353\n",
      "Epoch 225/300\n",
      "169/169 [==============================] - 21s 122ms/step - loss: 0.0612 - accuracy: 0.9936 - val_loss: 0.1911 - val_accuracy: 0.9757\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.11353\n",
      "Epoch 00225: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = tf.keras.optimizers.Adam(lr=0.0001, decay=decay_rate),\n",
    "             loss = \"binary_crossentropy\",\n",
    "             metrics = ['accuracy'])\n",
    "results = model.fit_generator(train_generator, steps_per_epoch=t_steps, epochs=epochs,use_multiprocessing=False, workers=0,validation_data=validation_generator,validation_steps=v_steps,callbacks=callbacks, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfgAAAHwCAYAAABKe30SAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAACzBklEQVR4nOydd5gbxfnHv6Nyul58rufeOy64YIqN6RB66L0HQkkgJCEhpBCSEPgFEgglJJBC6N2EXgzGFOOCC+7dPtfrXX1+f8yOdrS3klY66VTu/TzPPSut2ki3u+983zaMcw6CIAiCIHILW7oHQBAEQRBE8iEDTxAEQRA5CBl4giAIgshByMATBEEQRA5CBp4gCIIgchAy8ARBEASRg5CBJwgiIoyxoxhjG9M9DoIg4odRHTxBZCaMsR0AruGcf5jusRAEkX2QgieIHgxjzJ7uMXSVXPgOBJEKyMATRJbBGLMxxu5gjG1ljNUxxl5kjPVSHn+JMbafMdbEGFvEGJuoPPYvxthjjLG3GWNtAOYzxnYwxm5njK3WXvMCYyxfe/7RjLFq5fURn6s9/hPG2D7G2F7G2DWMMc4YGxXhe/RijP1Te24DY+x1bf8VjLHFhueG3sfkO9yufV+78vyzGGOrrfxeBJGrkIEniOzjZgBnApgHoApAA4BHlMffATAaQF8AKwA8Y3j9RQB+B6AEgDSk5wE4CcBwAIcAuCLK55s+lzF2EoDbABwHYBSAo2N8j6cBFAKYqI31wRjPj/Qd/gKgDcAxhsef1W7H+r0IIichA08Q2cf1AO7knFdzzj0Afg3gHMaYAwA4509xzluUx6YwxsqU17/BOf+ccx7knLu1fQ9xzvdyzusBvAlgapTPj/Tc8wD8k3O+lnPern22KYyxAQBOBnA957yBc+7jnH8ax29g/A7PAbhQe+8SAKdo+4AYvxdB5Cpk4Aki+xgK4DXGWCNjrBHAegABAP0YY3bG2L2aO7oZwA7tNb2V1+82ec/9yu12AMVRPj/Sc6sM7232OZLBAOo55w1RnhMN43s/C+BsxpgLwNkAVnDOd2qPRfy9EvxsgsgKyMATRPaxG8DJnPNy5S+fc74HwjV9BoSbvAzAMO01THl9qkpn9gEYpNwfHOW5uwH0YoyVmzzWBuG6BwAwxvqbPCfsO3DO1wHYCeEVUN3z8rMi/V4EkbOQgSeIzMbJGMtX/hwAHgfwO8bYUABgjPVhjJ2hPb8EgAdAHYSR/H03jvVFAFcyxsYzxgoB3BXpiZzzfRC5Ao8yxioYY07G2Fzt4VUAJjLGpmoJfL+2+PnPAvgBgLkAXlL2R/u9CCJnIQNPEJnN2wA6lL9fQySVLQDwPmOsBcBXAGZrz/8PhJLdA2Cd9li3wDl/B8BDABYC2KJ8tifCSy4F4AOwAcBBAD/U3mcTgLsBfAhgM/REwFg8B5FI9zHnvFbZH+33IoichRrdEASREhhj4wF8C8DFOfenezwE0dMgBU8QRNLQ6s9djLEKAH8E8CYZd4JID2TgCYJIJt+DcLdvhchUvyG9wyGIngu56AmCIAgiByEFTxAEQRA5CBl4giAIgshBcqpVY+/evfmwYcPSPQyCIAiC6DaWL19eyznvY9yfUwZ+2LBhWLZsWbqHQRAEQRDdBmNsp9l+ctETBEEQRA5CBp4gCIIgchAy8ARBEASRg+RUDJ4gCILoXnw+H6qrq+F2u9M9lJwnPz8fgwYNgtPptPR8MvAEQRBEwlRXV6OkpATDhg0DYyz2C4iE4Jyjrq4O1dXVGD58uKXXkIueIAiCSBi3243Kykoy7imGMYbKysq4PCVk4AmCIIguQca9e4j3dyYDTxAEQWQ1xcXF6R5CRkIGniAIgiByEDLwBEEQRE7AOcePf/xjTJo0CZMnT8YLL7wAANi3bx/mzp2LqVOnYtKkSfjss88QCARwxRVXhJ774IMPpnn0yYey6AmCIIik8Js312Ld3uakvueEqlL86rSJlp776quvYuXKlVi1ahVqa2sxc+ZMzJ07F88++yxOPPFE3HnnnQgEAmhvb8fKlSuxZ88efPvttwCAxsbGpI47EyAFTxAEQeQEixcvxoUXXgi73Y5+/fph3rx5WLp0KWbOnIl//vOf+PWvf401a9agpKQEI0aMwLZt23DzzTfj3XffRWlpabqHn3RIwRMEQRBJwarS7m7mzp2LRYsW4a233sIVV1yB2267DZdddhlWrVqF9957D48//jhefPFFPPXUU+kealIhBU8QBEHkBEcddRReeOEFBAIB1NTUYNGiRZg1axZ27tyJfv364dprr8U111yDFStWoLa2FsFgEN/97ndxzz33YMWKFekeftIhBU8QBEHkBGeddRa+/PJLTJkyBYwx3Hfffejfvz/+/e9/4/7774fT6URxcTH+85//YM+ePbjyyisRDAYBAH/4wx/SPPrkwzjn6R5D0pgxYwan9eAJgiC6j/Xr12P8+PHpHkaPwez3Zowt55zPMD6XXPQEQRAEkYOQgc8G3r8LeOGSdI+CIAiCyCIoBp8N1G4G6remexQEQRBEFkEKPhsI+gG/J92jIAiCILIIMvDZQNAPBLzpHgVBEASRRZCBzwZ4gAw8QRAEERdk4LOBYADwk4EnCIIgrEMGPhsI+oEAxeAJgiC6SrS143fs2IFJkyZ142hSCxn4bEDG4HOoKRFBEASRWqhMLhsI+sU24AMceekdC0EQRCTeuQPYvya579l/MnDyvREfvuOOOzB48GDceOONAIBf//rXcDgcWLhwIRoaGuDz+XDPPffgjDPOiOtj3W43brjhBixbtgwOhwMPPPAA5s+fj7Vr1+LKK6+E1+tFMBjEK6+8gqqqKpx33nmorq5GIBDAXXfdhfPPP79LXzsZkIHPBoIBsQ14yMATBEEonH/++fjhD38YMvAvvvgi3nvvPdxyyy0oLS1FbW0tDjvsMJx++ulgjFl+30ceeQSMMaxZswYbNmzACSecgE2bNuHxxx/HD37wA1x88cXwer0IBAJ4++23UVVVhbfeegsA0NTUlJLvGi9k4LMBqeD9XsCV3qEQBEFEJIrSThXTpk3DwYMHsXfvXtTU1KCiogL9+/fHrbfeikWLFsFms2HPnj04cOAA+vfvb/l9Fy9ejJtvvhkAMG7cOAwdOhSbNm3CnDlz8Lvf/Q7V1dU4++yzMXr0aEyePBk/+tGP8NOf/hSnnnoqjjrqqFR93bigGHw2EHLRUyY9QRCEkXPPPRcvv/wyXnjhBZx//vl45plnUFNTg+XLl2PlypXo168f3G53Uj7roosuwoIFC1BQUIBTTjkFH3/8McaMGYMVK1Zg8uTJ+MUvfoG77747KZ/VVUjBZwOqi54gCIII4/zzz8e1116L2tpafPrpp3jxxRfRt29fOJ1OLFy4EDt37oz7PY866ig888wzOOaYY7Bp0ybs2rULY8eOxbZt2zBixAjccsst2LVrF1avXo1x48ahV69euOSSS1BeXo5//OMfKfiW8UMGPhuQBp5q4QmCIDoxceJEtLS0YODAgRgwYAAuvvhinHbaaZg8eTJmzJiBcePGxf2e3//+93HDDTdg8uTJcDgc+Ne//gWXy4UXX3wRTz/9NJxOJ/r374+f//znWLp0KX784x/DZrPB6XTiscceS8G3jB9aDz4b+NN4oGUvcP3nQP/cqdEkCCL7ofXguxdaDz7XCMXgyUVPEARBWINc9NmAmkVPEARBdIk1a9bg0ksvDdvncrmwZMmSNI0oNZCBzwZCSXZk4AmCILrK5MmTsXLlynQPI+WQiz4boDI5giAIIk7IwGcDIRc9xeAJgiAIa5CBzwa4wUW/43PA05K+8RAEQRAZDxn4TIfzcBe9uxn496nAqufTOy6CIIgMIFlLvH7yySf44osvkjCi2J9z6qmndvk5ViADn+nwoH7b7wG8rWKfrz19YyIIgkiE++4DFi4M37dwodifZrrLwHcnZOAzHaneAaHgfR3a/kB6xkMQBJEoM2cC552nG/mFC8X9mTO79LZ+vx8XX3wxxo8fj3POOQft7UIALV++HPPmzcOhhx6KE088Efv27QMAPPTQQ5gwYQIOOeQQXHDBBdixYwcef/xxPPjgg5g6dSo+++yzsPf/9a9/jcsvvxxHHXUUhg4dildffRU/+clPMHnyZJx00knw+XwAgI8++gjTpk3D5MmTcdVVV8HjEXlT7777LsaNG4fp06fj1VdfDb1vW1sbrrrqKsyaNQvTpk3DG2+80aXfwQiVyWU6ZOAJgsgWfvhDIFb5WVUVcOKJwIABwL59wPjxwG9+I/7MmDoV+POfo77lxo0b8eSTT+KII47AVVddhUcffRQ/+MEPcPPNN+ONN95Anz598MILL+DOO+/EU089hXvvvRfbt2+Hy+VCY2MjysvLcf3116O4uBi333676Wds3boVCxcuxLp16zBnzhy88soruO+++3DWWWfhrbfewkknnYQrrrgCH330EcaMGYPLLrsMjz32GK6//npce+21+PjjjzFq1KiwdeJ/97vf4ZhjjsFTTz2FxsZGzJo1C8cdd1z03y8OSMFnOqqB93sUA+83fz5BEEQmU1EhjPuuXWJbUdHltxw8eDCOOOIIAMAll1yCxYsXY+PGjfj2229x/PHHY+rUqbjnnntQXV0NADjkkENw8cUX47///S8cDms69+STT4bT6cTkyZMRCARw0kknARA19Tt27MDGjRsxfPhwjBkzBgBw+eWXY9GiRdiwYQOGDx+O0aNHgzGGSy65JPSe77//Pu69915MnToVRx99NNxuN3bt2tXl30NCCj7TUZV6wAf4NQPPScETBJFhxFDaAHS3/F13AY89BvzqV8D8+V36WMZYp/ucc0ycOBFffvllp+e/9dZbWLRoEd5880387ne/w5o1a2J+hsvlAoDQgjLyM202G/z+xAQX5xyvvPIKxo4dG7b/wIEDCb2fEVLwmU6Yi54UPEEQWYw07i++CNx9t9iqMfkE2bVrV8iQP/vsszjyyCMxduxY1NTUhPb7fD6sXbsWwWAQu3fvxvz58/HHP/4RTU1NaG1tRUlJCVpaEi8/Hjt2LHbs2IEtW7YAAJ5++mnMmzcP48aNw44dO7B161YAwHPPPRd6zYknnoiHH34YctG3b775JuHPN4MMfKajKvgwFz0peIIgsoylS4VRl4p9/nxxf+nSLr3t2LFj8cgjj2D8+PFoaGjADTfcgLy8PLz88sv46U9/iilTpmDq1Kn44osvEAgEcMkll2Dy5MmYNm0abrnlFpSXl+O0007Da6+9ZppkZ4X8/Hz885//xLnnnovJkyfDZrPh+uuvR35+Pp544gl85zvfwfTp09G3b9/Qa+666y74fD4ccsghmDhxIu66664u/Q5GaLnYTKdxN/BnrcZz1veAqmnA69cDh90InPT79I6NIIgeDy0X273QcrG5hNFFTzF4giAIwgIpN/CMsZMYYxsZY1sYY3eYPH49Y2wNY2wlY2wxY2yC8tjPtNdtZIydmOqxZiRhLnovxeAJgiAIS6TUwDPG7AAeAXAygAkALlQNuMaznPPJnPOpAO4D8ID22gkALgAwEcBJAB7V3q9nQXXwBEEQRAKkWsHPArCFc76Nc+4F8DyAM9QncM6blbtFAGRSwBkAnuecezjn2wFs0d6vZ0FZ9ARBZDi5lMuVycT7O6fawA8EsFu5X63tC4MxdiNjbCuEgr8lntfmPGGNbryA3y1uqz3qCYIg0kR+fj7q6urIyKcYzjnq6uqQn59v+TUZ0eiGc/4IgEcYYxcB+AWAy62+ljF2HYDrAGDIkCGpGWA6CWt049UXmSEFTxBEBjBo0CBUV1ejpqYm3UPJefLz8zFo0CDLz0+1gd8DYLByf5C2LxLPA3gsntdyzp8A8AQgyuS6MtiMhBsNvKbgycATBJEBOJ1ODB8+PN3DIExItYt+KYDRjLHhjLE8iKS5BeoTGGOjlbvfAbBZu70AwAWMMRdjbDiA0QC+TvF4Mw9pyJlda3QjFTwl2REEQRCRSamC55z7GWM3AXgPgB3AU5zztYyxuwEs45wvAHATY+w4AD4ADdDc89rzXgSwDoAfwI2c98Dib2ng84q0OnhS8ISBd38GFPcFjrw13SMhCCKDSHkMnnP+NoC3Dft+qdz+QZTX/g7A71I3uixAGnJnoVhsRip4SrIjJBvfAcqHkIEnCCIM6mSX6UhXvLNAc9GTgicMtNcDnubYzyMIokdBBj7TCXPRU6MbwkDAB3iaADcZeIIgwiEDn+mEuei9ei96UvAEINQ7AHgSX+aSIIjchAx8phMy8AXhvegpBk8AQHud2JKLniAIA2TgM51QDL6QWtUSgvrtwMf3AJzrBt7vFhNAgiAIDTLwmY408HmFFIMnBBvfBhbdDzTt1g08QG56giDCIAOf6agueh6kGDyh90Jo3mcw8E3pGQ9BEBkJGfhMJ2Tgi8z3Ez0Pv0dsW/bqSXYAKXiCIMIgA5/phMrkCsP3U5JdzyWk4PeGK3gqlSMIQoEMfKYTSrIjBU9oyGQ6o4GnTHqCIBTIwGc6agw+bD8l2fVYpIJv2Qd01AP55eI+uegJglAgA5/pmLnoHQWk4HsyMgYvFXwvbalOctETBKFABj7TUTvZSVzF4evEEz0LYwy+Ypi4Ty56giAUUr6aHNFFuLLYjCSvWNTEEz0T+b9v2QfYnEBJFWB3kYEnCCIMUvCZjlmSnauYYvA9GangA17A1wYU9gLyS8lFTxDdxcZ3ga0L0z2KmJCCz3SCfoDZAIdL35dXAgT3pW9MRHqRMXhJYSXgKqEkO4LoLhbdLybVI+eneyRRIQWf6QT9gM0B2PP0fRSD79n43YCrVL9fWCnuk4ueILqHoC8rvKhk4DMdaeAdioHPIxd9j8bv1hPrAFLwBNHdBINZ0WyMDHymEwxoCl5x0buKqUyuJ+P3AuVDADBxv7ASyC+jGDxBdBdBv1jNMcMhA5/pRIzBk4LvsfjdwotT3FfcJxc9QXQvPEAKnkgCoRi8U9xndsCZTwq+J+P3iAlfaZW4X1ChuejJwBNEtxAkA08kA6OL3lkojDwPZIWLiEgBfjfgyBf17/nlgN0hMno9LXRMEER3EPRnRaIzlcllOtLAyyQ7Z764D4gZJLOnb2xEevB7xPEw8Sy9Ta2rRBwP3lZxmyCI1MGzI8mODHymE/QDNrteJucsEPcBzfiTge9RcA4EPELBH3Ku+AP0sjlPCxl4gkg1QX9WGHhy0Wc6oRi85qJ3qAae4vBZQVM1sOBmfZnXriAvLGrSJSBc9EDnTPr6bcC+VV3/XIIgdCgGTySFkIF3iGx6Z4Hios/8GBABYPsiYMV/hLHtKrJNrSM/fL+q4AHA2wa8fiPw8Azgv9/t+ucSBKFDCp5ICtLAA8JN7yzQ4+6k4LMDuTiMv6Pr7yXb1EY08E1iu+I/wMr/AuWDAU9r1z+XIAidLElyJgOf6QQDgE37N9ld4Qo+mPkzyB5Dw05g11fmjwV8Yutzd/1zpIJXWxcDnV30u74CyoYAE88WbTUJgkgewUBW9CIhA5/pqArekafF4G36Y0RmsPgB4OWrzB8LGfj2rn9ORAWvJdbJUrndS4DBs0T/hCzpukUQWQPF4ImkwAOdXfQhBU8GPmNwN0V2hYdc9MlQ8NLAG5LsQi76ZqBpt1grfshhYr14ICvUBkFkDRSDJ5KCquBnXi1qn2UMnpLsMgdvW2QDHlLwyYjBR0iyyysWIZzaTcDur8W+wbOUigty0xNE0qBWtURSCCoK/qgfAeNPJQWfiXjbRX26mStcKvikGPgICt5mA6ZcAKx6AVj3BuAsAvpO1Fsc07FCEMmB86xpdEMGPtORjW5UQqos8w+wHoNXc89LA6ySTBd9IIKBB4AjbxXHy/oFwKBDRWmlnAwGSMETRFKQ4a4s8KCSgc90VBe9hBrdZB7eNrE1M+JGF/2Cm4FXr0vscyIpeEC0rZ18jrg9eLbYhrw9mX8xIoisQBr2LEhcJQOf6ZgaeGp0k3HIDHlTA29w0R/cANRsTOxzIsXgJUfdDhT1AcacJO6HXPSk4AkiKUhhlQUueupFn+moMXgJNbrJPEIu+igGXja68bUn/r+LVCYn6TMG+PEW/T656AkiuYRc9GTgia4S9IsWtSrkds08Qi56sxi8odGNty1x70ukRjeRsFGSHUEkFVLwRNIwU/DqanJE+vF79ZM+qotec+PLyUBCnxVDwRuhfA2CSC7SsGfB9ZcMfKYTLcmOYvCZgVdpcGMli97XDoAl9lnRkuzMoDI5gkgupOCJpEEx+MxHbUFrVuuuZtFzLhS8sfTRKrGS7IxIFz3F4AkiOWRRDJ6y6DMd0zp4anSTUagu92gK3tehTQC4+N8lYnT9HgBMV+axoGOFIJJLSMFTmRzRVaLWwZOLPiMIc9FHy6J3x1b7sfC7hXueWXTx28nAE0RS4aTgiWQRrQ6eDHxm4FWMdtQs+o5wtZ+IgQ94rcffASqTI4hkQy56ImmYxuC1fxsl2WUGYS56EwUfjGDg/YkqeIvxd0ApkyMDTxBJgVrVEkmDYvCZj2UXfUcSXPSe+BS8nZaLJYikkkVZ9GTgM52oBp4u2hmBarSj9qJ3d91F73eLZWGtIo8dctETRHKgGDyRNHi0Rjek4DOCWC56tdFNlw28J0EXPR0rBJEUSMETSYHzGIvNZP4B1iOIWSanqWdjFn1CMfg4XfQhbw8peIJICnKZ7iy4/lKjm0xGHkCRkuxIlWUG3jbxP7I5Y5fJeVr0/d2h4GUMPkDHCkEkBTW5LhgEbJmrkzN3ZIRuwCkGn9l424C8IsCZry8ooyINPAB01Ou3zZ4bC1kHbxVKyCSI5KKeSxmu4snAZzIhA08x+IzG1wY4iwBHQeQkO/k/bFcMPLnoiVzG7wVevxGo357ukSQXVViRgScSJqKBJ1WWUUgF73BFblXrKhW322r1/Qk1ukm0TI6OFaKbqd0IrPwvsPPzdI8kuZCCJ5KCnCkyg4te3s/wg6vH4G0D8gpFbNyo4DkXBj5fM/Dttfpa7gm3qo0ni152siMDT3QzrQfENtdKNNXrboZfg8nAZzIRY/Dkos8ovO1AXrG5gpf/I6ng2+uAgl7idnc0uiEXPZEuWg+Kba5dp8IUfGbnQZGBz2SkgqfFZjIbb6vmos/vHFeXCXb5ZWLbVge4SkSzGvW5nAMH18f+rHgVPLnoiXSRqwqeYvBEUqAYfHbgawecheYK3mjg22uFO99ZEK7gty8CHj0MqNkY/bP8Ht3FbwVy0RPpIqTgc83AUwyeSAaxDHyGu4d6DN424aJ3mmTRS/UiXfS+dpFxbzTwTdVi27wn8udwnkAnO5oMEmkiVxW8et3N8DXhycBnMpFc9Ixc9GmncTfw58lAzSbFRW9BwQPmCr6jQds2Rv7MgA8Ajy8Gz5g4fnJNRRGZT87G4MlFTySDiEl2NgCMDHw62fsN0LgL2LFIS7KLkEUfMvCl+r48k5p5aeDdTZE/Uz4/HgUPCAOfayqKyHykgicDnzbIwGcykVz0gDD6uXbiZBPSpb53pVDHERW8IYseUFz0Sl96d2P41gz53vEoeEC00KXJINHd5KqLXr3uZvh5RQY+k4mk4AFh9LMxBl+/Pbwfe7YiY+XVy8Q2r1goa2PpW1QXvYmCj+aiDyn4eA28nVz0RPfic+veqFwTIpwUPJEMIsXgARGHz/DZoylPnQh8+Ui6R9F1mnaLbc0GsXVKF32kGLzRRZ8fruCtuOjle8Xrorc7c09FEZlN20H9dq4de5RFTyQFOVOMpOCzcWbcXq8bs2ymSWa7a1m0oTp4d3hmrby45RUBYOK2dNGbxuAbI39mwgremZ3HCpG9tBzQb+ea9yhIneyIZBA1Bm/LTgUf9OfGjL6pGqgYrt+XMXjw8O8nVbfdJVQ+oLjo41TwXUmyIwNPdCetioHPhfNdhVz0RFKIauCz8KIdDALg2T+j93vFBWz0Cfo+qeCBcGUeMvB5YjlZ+dxOMfjG8K0ZchLgLIhvvPYsPFaI7EYaeHte7h175KInkkI0A8/s2ZdkJw17tndVa9kLgAP9JwMlA8S+kIKHwcBr39nuFKVxgL60rEzICwZ05R7NRb/pfTGJqJoe33ipTI7obloPAmBASf8cNPCk4EMwxk5ijG1kjG1hjN1h8vhtjLF1jLHVjLGPGGNDlccCjLGV2t+CVI8144iWZGdzZJ+LXp7o2X7Cy/h72UCg73hxW8bVgSgKXntcuuhlL3p3E0Qsn0V20QeDwPo3gVHHAa7i+MZLMXiiu2k9ABRWiolsrk0uScELGGN2AI8AOBnABAAXMsYmGJ72DYAZnPNDALwM4D7lsQ7O+VTt7/RUjjUjiVoml4VZ9PJEz3YXvayBLxsM9NUO5zAXvZJJH81FL/MRpOu9dKBw0Zu1v9yzXHgOxidwGpCLnuhuWg8Cxf2E5yrXjj1aLjbELABbOOfbOOdeAM8DOEN9Aud8IedcZht9BWBQiseUPcgTw7gePJCdjW7khCTbZ/TNmoEvHQiMPQUYOAMo7hvDRe8Id9FLNe/r0N3yFcNE2MXb2vkz178hlPiYE+Mfb6Iu+taD2TeJJDKD1gPinLDZs/98N0IKPsRAALuV+9XavkhcDeAd5X4+Y2wZY+wrxtiZKRhfZhMryS5bY/DZNjEx0lQt1nTPKwSGHQFc+5Ew7jEVvOKiVxPypIKvGCa2Rjc958C6BcCIeUBBefzjTcRF72kB/jIFWP1i/J9HEFLB25zZ77Ezok56M3wCbGI50gNj7BIAMwDMU3YP5ZzvYYyNAPAxY2wN53yr4XXXAbgOAIYMGdJt4+0WYja6yTJDKceb7TP6pj0i/m5EKni1m52pgS/SS+Z87XrmfK9hYtvRqPeqLxsoJgCNO4FZ1yY23kQqLhp3i7HJcARBWCUYBFr3CwXfuDP7z3cjpOBD7AEwWLk/SNsXBmPsOAB3Ajidcx6SP5zzPdp2G4BPAEwzvpZz/gTnfAbnfEafPn2SO/p0E4zV6CazD65O5FIMvmxw5/3SBR+m4NUsek21O4v0eLxPVfBaXb27EXj7R8CLl4r7srVvQUVi47Un4KJv2at9dnNin0n0XNrrxMS2bFB2JgPHgurgQywFMJoxNpwxlgfgAgBh2fCMsWkA/gZh3A8q+ysYYy7tdm8ARwBYl+LxZhYxG91km4IPhG+zleZqEX83YhqDVxW82uhGVfAmLvr9a/RuYDImnxdn9rwkERd9877wzyYIq8h1GkqrtCS7LJ/QGwkrk8vs9eBT6qLnnPsZYzcBeA+AHcBTnPO1jLG7ASzjnC8AcD+AYgAvMcYAYJeWMT8ewN8YY0GIici9nHMy8JKsbHQj6+Cz+IT3tgsDXDqg82OmjW6kgley6J3GGHyjMN6FlWJfWw3QsEMofQDwaEY23vI4SSLrwbdoBj4XFgYiupdmzftTOlBMLrP5fDcji+rgUx6D55y/DeBtw75fKrePi/C6LwBMTu3oMpyca3Qj6+Cz+ISXGe9m7vKQgjdJsrM5gPxysaqcza5k0WsKvqBCT6Dbt0r8Vt4WoRCkkVWXnI0HewJuUnmRJgNPxEtIwQ/MzTK5LIrBZ0ySHWFCrjW6kTP5bO5k59Zi0mbGNqTKlSS7oE+od8aAOTcC404V+0MGXovBF5QDrjIATF+ClgfFBMCrGdmEXfSJxOClgicXPREnzXvEMVfUJzfL5MJi8Jl9DSYDn8mEFLxJqkQ2NroJxeCz+ISXSWfq+u4Sp1mZnGbgAaCot/gD9IQ8X4cw8Pnl4v/sKgUOrFU+ryUJLvoE4qDNlGRHJEjzXqCkShzPOVkmlz0KnnrRZzJRY/DZWCaXAzF4SwrekGRnd3Z+bqitbYfuogeAgrJwVeBpUVz0JYmN2e6MfzLYQkl2PZ59q4BXv5dYeEeWkdqd2e2xM4OWiyWSAo/hos9w91AncqEXvUdrQmOm4KVSN8bg5X4VYyc7aeDzyw2f16xk0Sdo4ON1k/q9ItEPoBh8T2b7Z8Dq5/UqD6s0VYsMeiA7k4FjQWVyRFKIlWSXbSdOqA4+y8atIhV8vomCZ0yoeGMWfVQD3x6u4OXEoXKU2EoF78gXyXKJEK+btHW/NpZyisH3ZOR5qk5YY8G5UPDSwOdkmRy56IlkIA2iaS/6LGx0kwu96GUb2UgZ7Q5X+DrvkVz0MgYvm4LIDHq5lUvCSgOfqHseiF9FyRr4PmNFCCGb/19E4oQMvDv681Ta64GAR+8Tketlchl+DSYDn8l420QttGmSXTY2usmBTnaeZjHhyisyf7yTgo/gorfZALsLWPW8uF+qrbEkXfQDFQPvbU08gx6IPw4qu9j1Hq2Pgeh5hFpLe62/Rl2ICchNFz0peCIpeFoiZ04nEoPf/TXwxcNdH1eihC4YWXzCu5uFmhZNmTrjyDfJojdR8IBw83c0AHN/Akw8S9unueirtK7Mnlbxl2gGPZC4gu89Vmwp0a5nkoiLXm1yAyTWJjnTyaLlYqlMLpPxtkVWionE4Fe/CKx6Djj85q6PLRFyoRe9p9k8wU5iVcEDwPnPiPfqO07f13eCKDHqN0n/PE9L4k1ugPg72bXsFd6Fcq3fPin4nklCCl5pUwvkbpkcswnjTgaeSJhortlEXF8BT3wna7IJ1cFnuYI3S7CTOFzWsugBYMjszvumXQxMvUh4COx5mou+BSjun/iY7U5xIQoGzcM9Rpr3iVa8clJBiXY9k0QVPLOLleSA+I+9bCAY0HILPBlv4HPkF89RvG1RDLw9/gSPgE8YnHQtkKCuB5/hizRExNOsdZyLgFkWvVkVRDSk+99Voje66ZKLXkvStDqxatknvAgysY8UfM9ETsjjMfBNe4CSAfoxJ4/9bJ7UGwn69Ul7hpcqk4HPZKLG4BNw0csTNV0nm/q52XrCW1LwFl30sQgZ+K5m0Ws5AFZdpc17NQWvfaaXDHyPJOSij0fB79Hd84Cef5JLbnoe1EtWScETCRMrBh/v7FG659PlpleNerYm3ribosfDnQXJN/DJyKIHrE2qgkFxkS4bpH8mKfieiTxH41Hw7XW6ex7QJ5fZer6bEabgycATiZL0GHyaDbyaPZ+tM3pPU5wx+ChZ9LHIKxFd7nztXa+DB6xVL7QdFMdH2WBy0fd0Ekmya68DCnvp93PSRR9QDHxmhxrJwGcyyY7BS8OTrtl0mILPwhNeLt2arCz6WLhK9J7wyTDwViZVjbvFNszAU5JdjyTeGDznmoGv1PdJV3a2K/jNHwJPniB+i6Bfn7RnuIKnLPpMhXOh4KPVwWebglcNTDbO6L2t4oSO5qI3VfBdMPCyJr0rLvp4VFTTLrEtH6ytW19IK8r1VOLNonc3idcU9tb3xZv/kYl424E3fyCa+LQeFKFRW3YYeFLwmYqvQxw8kWLwtiyPwWfjCR+tD73EkS/+d5JIrWqt4CrRE5y6ouDtccRBVQUvP5ca3fRM4k2ya68T2zAFH0f+R6by2Z/0Dn1+T7iLPsOX7CYDn6mEVhCLoNwSaXTjlwY+TcY1kOVJdlLJRlXwZp3suqDgzW7HS0hFBYBtnwDrFuieASNN1aIMUE5i8oopBt9TiVfBt9eLrWrg48n/SCVv/xh46cr4X9e8D/jiIaC4n7jvd2sGPjuy6MlFn6nEMvA2R/yzRzkTzwgFn4UzernQTCwFb2U1OSuoE4kuuehlHbwPePEy/XsceRtw7C/D2+427dY72AFaJj8p+B5JvEl2URV8mif0+9fox3081G0R33/SOcBXj4jJDg9kTRY9GfhMRV5Uo9XBg8fXIUqeqP4MiMFno4IPuejLIz/HkS8uAAG/1oe7Ky565X+fDBe93yMuclMuFB6gxQ+Idd9Pf1g38o27gfIh4Z9LCr5nEu9qcu21YlukKvgMKZNzN4eHzqwihVZxH7ENyCS77DDw5KLPVLxtYhstBg/Ep4T96Y7Bq8ssZqGBt+Sid4mt3y0SJbuaRR+6nYQkO3ej2PafDJzxV+Cw7wPfPA3UbNCf27Rb1MCrYyAD3zMJGfguKPhMKZPzJGjg5bFfpBn4kIuekuyIrhBy0UdQbnKN+HgS7RJx0T91MvDJvdafH/XzszyL3qqLHtAvBODJMfCRjgMrSBUlY6RyNbyRx4j7cjLpbhIXQqOLnjrZ9UwSSbKz54WHkzKlTC5RBS8NvKwMkEl2WZJFTy76TCVk4CMp+ARmxvIki+dk27cSKOod82mWyPY6eCsK3qkYeDmR6koWfeh2VzrZacdKh2LgAX3iIcdpzKAHKMmuJxOqg49DwRf2Ds/psGVAFn0wKM7dRM7DkIKXBt6txeCzw8CTgs9ULMXgEV+inT9OBe9tE13UpGu3q4TVwWeYi/79u0R2eTTczWJi5SyI/JyQgldW7utqkp09T3f9J4KcDLY3aO9rMPDyuGjSDHynGDwl2fVIQoLAooJvMzS5AVKTZLdvFfDs+aI+3QreVgBauCzexGRPi1gatqBC3M+yRjdk4DOVUAw+ShY9YP2A5Tx+F32bljTT0Wjt+bFQx5pul92eFcDiB/X7y/8FfPVo9Ne4m0QXO1WhGFFj8PI7dlXBdyWDHtBVVEjBKxMHQB9nSMGrMfhicdykKzGTSB9xJ9kZ2tQCSpJdEhX8N88Am94Ftn9q7flqo6Z43fTeVhEeM4beKMmOsERrjeiOZETGPSPWwWv/Oqsx+EQWepFZsYmUl5iRSTH4b18BPvyNcN8Fg2KmXr00ulr1NEd3zwPhF4IuK/iS8G2iyMlgh0HBOwwu+qbdYqxFymIh8vtSs5ueRyJJdp0UfBxtkq2y7ROx3fqxtee7u2Dg5UqOqmeOkuwIyyy4GXj9hs77vW1i9uuIYBzijcGHNV+xquC1rNikuegzqNGNrwMAB3xt4g9cjG/Xl5FfE2upWEBX8L4kGHg5ueuqgZcX2fZIMXjpoq8GSgeGl12G+tFTu9oeh/S4WU6yq+2cr2NLcpJd816gdqMQOFs+svYa9dj1J2rg1eqYQHjzqAyGDHy6aT0gapGNeKL0oQfid9GrRt2ygdfG5W6Of2EbMzKpF710O3pawlW7VAdmxK3gu+iilwY+ZS56Q42y2UI6tGRszyUeBR/wCU+fUcEnO8lum+aWn3IRUL8VaNgR+zVdVvDFynntpTp4Ig587ULtGYm2khwQfx18IgZeuujBk6Pgwno4Z4KChzj5Q8aL6RcQM9zN0VeSA8JdeXISkWiCnM0m4n/JdtHL48qujSu0RoGn81hpydieSzxlcvLYiuSiT5aC3/aJ+IwjbhH3rbjpw2LwFhPzQq/VFLzdIUqTqQ6eiAtfu7nbyNsSw8DH2Qs5IRd9rX47GW76gE/PQE93mZz8PTwtuvEaPAs4sCb8e6v42iKXLUpUBR+rWZEVXCVdK5EDwl30eSW6C96YRe83acojx241Y5nIHeQk3EoverMmN0ByFTznIrFu+Dyg9xhRzmnFTa/mEJmJqWh4W/XrsMOlGXhVwdN68EQ0vO3mbiNvDGMik+y6quDb64F7hwDbF3V+jWrokpFJH/QDDs3Ap1vBy0mVp1mf4Y87VWx3LI7wGhOFayQUq/MoBr4LCnz+z4AZVyX+eiC8k53qDTC66M0UvDwGfW1dGwORfcSzHry8VqSyTK5+G9CyDxgxT1SyDJ8H7F4S+3VdVvBaSMvh0nN3SMETlojkorccg0/EwCsnW80GMcOt2dj5Ne1JVvBBv94IJu1JdjIGr7jo+08S2/YICt7v1hV6JEIKviN2syIrTL8MGD438dcD4V23VAPvMLjo/V7dbS9xFoqtlwx8jyMeF30sBZ+ox271i7obXk4iZCOm4j7WhIcag7da8ifxtCpVJ/n6BCFLOtmRgU8nwWAUF73VGLzFJLtILnpZ+2xWBtVWC5QMELeTUSoXpuDT7aKXCl5x0ctmFpF+UzMXtpEwBZ8EA58M1CS/MAVvKJMLeDtXbYRc9GTgexzxJNlFMvBdLZP76G7g639o45A5LdokOq9YvG8sD0OidfBcyz1yKS56eR7Y7ABYfK3C0wAZ+HQijUzQ31nRWo3Bd9VF37RLbM3qv9tqgcpR4nYyXPRqDD7dBl4qeDXJThr4SN4FKwpefr+wGHwXY+hdRU4GgXADb7OLUI+aZGdU8CEXPcXgexzxNLoxWwse6FqZXDAgXPLyOikNuTwHQwmgMXo0yA6UQHzHsVcrnzVV8A5x7pCCJyKiJi4ZZ5YxY/BysRmLB1gkF300Bd9eC1SOFLeT5qKXSXaZEoNv0ZsKyWVgzSYfAb+YrceKwdszUMHbFAVvrOO354W76I0KXnpcKMmu5yHd6laScmvWi/PHePzYuhCDbz0ozsVQEqh2zspzUE6cYy2G5GkGivuJ2/Ek2YXO3wgK3mYnA09EQU1cMs6SY8bg4yyT80dS8Lv1z1ORfejLh4rJRFJc9AFFwafbwBuy6B35erzZ1MBL9RDDwNtswmhKBc/sXesjnwykegE6l9zZXcoywiYK3mYTvwt1sut5hBR8DBd47WZg7WvAtEs6P2bvQgy+ea/Y+gwKXl5D5PVRXrveuh3Y/GHn93GrBj6Oiar07IWS7PJ1A8/spOCJGIQpeOV2wCcutsmMwQcixOCbqrWxGGbBMqGlqA9QUJ6kLHqf7l5Ld5mcvGh4mvRa12jNg+TFxWgAzXDk61n0ecXRe9d3B2ExeKOCdxoUvMn3cxaSi74nIg08D0S/znz6R3HMH/HDzo8xJoxhIiG55j1iG1Lwhr4Sao+GYBBY9iSwxcTAe5qB4r7h72GFkIFXOj+GXPRk4IlYqBdN1XVkdA2ZIV2nVi+8Zi56znUXvVHBy0zyot6iuUuyXPQOFwBmTcGveVmP7SWbsE520sDbxElrdjHyW1Tw8jk+LYs+3e55QE8IAkwUfJ4hBm+SRJhXRC76ngbnWkhKadxkRs0mcZ7Ouk5ktZthdybmsQsZ+AgxeFl+6m0VfzxobsDdTSJ8YHclqOCliz5fPw+kgU9Gh88UQgY+nagHm5pJb6VBijyZzBaqMUO6YZldP1Ha6/TPNbpgZR/6oj7i5EiGiz7gEzE5myN2DL61BnjlarEoTLLh3NzAA2JspgbekMEbDUeBruC72qQmWUjvhNHAOzQDHwwqEzADeUXkou9pSMUur0GRlO+uLwBw4NArIr+XzZmgi96g4H2GGHzIRd+iX5/MJiJyDQlnQXwxeKOCD4vBU5IdEYtISXYeC8lZcsWvNosGXrroXcXK8qBaBr3N2VnByz70hZVJdNEHxIlhd8Z22cn8hFSUZ6kXK5lFL13XkQy8VLmRFv9RkR2vPBmi4AHdTR9Jwcvjw0zBk4u+5yHPAXn8Rkq0k9etgvLI72V3JKbgm/aEf0ZIwWvey1CSXateCmeciASD+hoS8R7HnZLs8vXrEsXgiZioSXY+EwUfrQe5q1gceJEUfEdj+KxZnqB5JeHLgwKi7aMxBp8SF71PnOw2KwZeO1GtdNGKF/Ui4GkRFwB5EidFwRti8JlAJAVvzxMTvmghiLxCctH3NKRBdkoFH+E8lNcqmaBqhhWPnRkyyU6NwTO7XluvJtlFUvDeVgBcU/D58dXBmyXZyWsDKXgiJupFUzU6obXgY6i/4r5iNTojnAMPTwe+fkJ5f2ngi3QDL+PvfcebK3hHvjBQRhd93VZg0f3x92EO+jUFb+GED8Xd4lz9yQo+o4FXulXZIiQEyd/PUpKdpuAzJQYPKAbesFiOPU9cFKMtbZtXTK1qexohBa8Z7mgKntmiN4CyJRqDlwa+Qw+rqRNsNQYvu9UZu+5JZS8VfFeS7NTJLyXZETHxRXDRS1Uu67IjUdzPXMEHvCK+vneFss/ERd+0W1y8ywaZx+ALe4ssWOmilwZ97WvAx/foqv6FS4APfhV9rIDwKNic1k74lCp46VbspbeqtRyDt5pF747dy6A7ieWi98dw0VMnu56FjMFLZR7JMPo6hMqPVilid8S/bnowALTs1Y1o0K8ZeOX8szvEueZpjqzgpeHPLw1vVGMFT4u4HsjPVCcXIQNPneyISHgjuOi3fyrc4n3GRX99JAUvZ9u1m5V9mkHNK1Zc9NWir7NL26fWyrfXAYW9xO38MmGQ5ckRindpJ9OBtUDtpuhjBTQFb9dKs2K46KURjnf9ZivIyUNxX2323xTbwFutgweEKzDTDHxEF71TK8uUOQbkoiegKHjNDR6pXa2vTa9Lj4TNGb+Lvq1GjKF8iPY5HeKcMn6Wq0R44CLF4EMKviz+JDuv5tmTkxf13KAYPBETn4mLnnNg60KxUpKMNUWiKIKBlydj3VZddfs94oB0Figu+l1A+eBwV5fEo6x9Lj0JcpYst9L4+tzWul0FfcKg2OxxKPg4F4ewgpw8FPXRxxVm4KPUwVtW8FkSg3e4Yiv4vGJKsutpdHLRR/Ck+TpiG/hEyuRkgl0vrZOm32O+mmNesTZJb9SeZ7hehBS8NPBxKnh1JcgwF72DOtkRMfC2I1SfLA+82k2iPGTUsbFfX9xPGFvjrFQaW2+L7sKXXcrsymy6eQ9QOjC83ETibtaTS2SGrMykdxtmy/6Ozga+fjvw7s/ClbqMwVtJsgvF4FNg4FUFL4mVRS/HYTUGn0l18EAMF70nuodCdrLL8LWviSQS8vjFSLLztcc+xhMpk5MlcrJVtr/DfC0IV7GWZGfwKko8ioveWRB/DN5s9UVAcdFn9jlBBj6d+Nr1BU6k0ZFLI46YH/v10kDJkjaJamzrtmj7fOJiLmOunIvJQWGv8HITiadZ71sulbycJRvdYX5P5xN4+T+Brx7VXffBoJjt2pzhk4xIyN8jHpeaVeS4i1QDHysGH8WFbcSRr/1WPPMVfMhFr/0/zCYweYVaE5EU5EMQmUkoBh+jTM7bbsFFb8FjZ6TZRMH73CYKvkQPswH6ud2yH3j7J2I5bCCxMrlOBt4Yg2ek4Iko+Nq1DmpOXbFu/Vis4FYxNPbrZX9lY6KdmYH3e0QNtyyLCniFIcsr6tzTGdBrR4EoLnq3mCj4TBT8tk/EVpbihcpL7JGNqIo/lS56MwWvGHizyUdcZXIuvQNfpih4m1NcrNWV5YDOSXZmdf7yIk9u+p6DsQ4+apJdlBI5IPaEfvfSzrk2zXvEuVY6QP98v1uvgZe4ig2NbrTr0I7FwNd/E9U+gJJkF2eZnNqoimLwRFzIJCyZ/OH3iANz5DHWXi8NlDEOb6rgvZqLXnPJqkuZhmLwmouec3Fw51tw0Qe8AHj4Z7bVAftWi9uymY6cwctGNzHL5FJo4OVJHsnAm8XgoyWhGXEUANBcdxmj4O2dV5ID9MVmQo1uInSyAyiTvidhjMFHTbKLYeCjheRaDgBPHg8s/Ye+b9ULwLJ/ivJd+d7y+mgWg5e9LAD9eiEno8X9xATVWagp+ASS7CRmWfTxVgd0MzGyuIiU4mvXDjwt+aNxt9hWTbf2+pCCNxh41ZVat1VsA15hWKViU5cyNSp42dfZqOA7GsRWdYdJY6mewDsWIWTgGneGP263WiaXwhi8qYteNrqJVAcfT5mc8pxMUfB2p3njJLnYTCgEYZZkp11kycD3HOQ5EHLRdyXJLkrfi4NrAXBgz3Jxf/VLwGvXAUMOB85+AmjYLvZLBW9cb95VbHDRy7a22vl69ftagjHTGt10xUVvSLJjmZ9kRwY+nXjbxcVT1k3LGLcsT4uFzALv5KJXSuLqtFI5OfuVLnq1370xBi8Vujy488vFbLVDczvL2bKvQz+hVAW/7RPhFSiq1JvpyBi9zaG5wS266FMRgw8peGVxjJhJdnGuJifJFANvi2Tgja1qzZLspIueDDwAcUzWbgQGTEn3SFJHqBe9VPDRDLwFBR+pzLJmo9juWyW2m94BSgYAl78pJgYt+7XPly56w/HpKjUk2cmwofZ5RX317+AsEHXrAV/4CouR6JRFr5zX5KInYuJr091Hvg7dBR6rwY3EkSeS9Dq56LWTsc84kc0e8CtJdppiC/W7L1aWXdT2yWx66dK12URTmLZa8T7y5PF79NwBdYa+7RNg+FFArxGKi14x8HYLMfhuV/Cxkuw84W0yoxGm4DPERT/pbOCQ8zvvl4vNREsiDCl4isEDANa+CvxtHtCwI90jSR3SwxarF723TT8+IhGtTO7gerGt3yZU+K4lwODZ+nkmj0dp4I3eAtllUXoXwbXWyyY5M/GswBkMdnbR240Kngw8EQ2ZgSp7JEsFH23hBiPF/SLH4PtNECdW0y59KdCQi15phxtS8No+tTmEpLBSNL+RM2VAGHepsNUFbBp2ACOOFk0qzGLwVlz03ZFkV9QbnZZRtTsj1MGbqIdIZKKCn3UtMPt7nfdbWWyGYvDhtNcD4MCOz9M9ktTRqdFNtCS7WFn0UTx2NRv1Y27jO0BzNTDkMP1x+d6RYvAytNZeKybggDiWfe3iPLTZTN7LkGjnaQGqlxnGpWXfy0Y7gMFFbyMDT8TAJ130Wn2mnIVaVfCASBTrVCanGc++E8S2bptQaNJFD+gxq7wiPbveY3DRq0lZRb3Fhc2j9KT3uRUFr00qWrTJRq+Roktee62YyKgxeCud7FLZ6Mbn1vtnu0q0BkCaCokUgw944zDwGRiDj4Q9T2sDGq0Onlz0YchjctcX8b+2vT41YadkE4rBR0myCwbF+R/TRW9YTe7bV4ElTwhXes16YMxJYr9cO2PwbP25qoL3mdTBqx4yGbL0e4QRNz43lLBnMPBL/wH849jwCduOxWI77EhlLGqSHSl4Ihbeds1FX5BcBS8v1mWDxbbtoJZkl6fHnuRkQp4gsiMUoBtx1T1V2EsY6zAF79Y/S57AoUVLnEC5VurXtLtzDD6mgle65CUbWW7DmIjhqe0oI43NrMlGJFRFE21FwExATvhkWMZUwZOLPgx5zO9MwMD/4zjgs/9L7nhSgTTwDpdQxgGPyKdR833kORpvmdySvwEf3CUqfNxNwLCjRMOtPcvFe/WfrD9XutVDMXhjoxvl/JI5NTL51zgup/Zao4GXLb3fuk2fyOxcDJQNCS9XNpbJ2cjAE9GQCl4a+I5GcVBaVYqAvuCM2lFJnkwlWg1pe324ix7obOBlRyig8zKJgOKiVxS8mkUvP1N19Ur3VuMuQww+jkY3ciWpZOLr0E92V0n494zW6Cbailkq2abgAf1/HlXBk4EHoCv4+m16EpgVgkGRFd6yLzXjSiYyTGWza6sjeoDnLxLdKSU+iwbeZgh7Ne4Uv+HiB8X9PmP1hMWBh4YnwMnj0dchri3RFLysKpIK3hg6CHkjDAa+YYdo5lWzAfjyr+J6s2NxuHoHSMETcRDwCaXoLNKy6DUDH497HhBuKV97eBc6aWSLKsWB2F6nuejNFLx28ZYdoQBzF32h5qJX14WXC0AAunKXhtuRJ/rcA+KETjQGr753slCVgKskXAVE7EUfh4KXz2M2669JF9LAy/991Bh8a+fHeiJqRnk8Kt7dqHUETPLxnArUCbnDJa4xB9cLL55ETvislMnJ893n1ic4q54T277jgQFTxW3VPa++txQWkWLwgGLg3eYG3hFBwddvB8aeAow7Ffj0PmDz++Ka2cnAm7WqJQNPmBEqU5MKXiuTi8c9D+hGWO1CF3KTu0SWfUe94qLXLuDtmoGXs1rZEQrQkuxY+Oy4sFKUmMiyN0CfKQPigsB5+KIlxf2FMW/cbaiDt7B8pGrgk72inBqf6z1G73cNRKmD95jXiJshLwR5xdGX0cwE5ITP06q5He2dn+NwiYsZuegFfreYWDuL4jPwsrthpJryTCKgTMjtLuHGDvrCz0V5PMTKoldXk5OdLUsHCeNYUCF+y4GHiv1DDze8VlPKUlhEc9EXKS56fxQF7zNcW1r2AhXDgZP/KD7r5avFY8OOCH+96XKxZOAJM0KzX6XRTSIKXrpP1QznUF/xPFHe1i4NvEs3Ph0NWutS7RBQY/ByoRnVOMkGE/XbxFZ6HVQ1oy47aneJ9y4bpLnopcvPEb3xhUS9kCS7B7rfo5/8pz8EnPtv/bFoy8XGq+Az3T0P6MeDpzlyaIgxcazksove2w58er+urttqgS8fBZ45F1j1fPhz/R7xvx0yO04DX6e9PpsUvFNMbPevEffVY0C9hkVDLZNr0BpfHX6z2PYZL46vUccCV7zVuYsnY+J8kiXEzmgueq3s1e+N4KKXCl75DrLKp2KYuFbN/5moJiobrOcQSahVLWGZ0OxXuugTVPBy9qxmOKt9xQsrhTEP9aJXXPSqATLG4I1tTYs0Ay87SxX1Da+DB7RyKzm50D5HlsoFuuCiN8bMuopfUfA2g2qN1FbT77HW5AbILgMfctG3Rc8xyCvKbRf9tk+AhfcA1UvF/fd/Abz3M2DzB8C6N8KfK8M1FcOB1jhi8NLAZ4OCV2PwdpeuoNWJt1UDr5bJNe4Q2wmnA4MPA0Zqi2oxJlziZh4vR77ioo+m4KWBd2tlcpEUvPId6rXrWa/hYjv7emDQLGDimZ3HYtbJjlrVEqZIg+wsEAee3y2Udv9D4nsfedCq7tOQis4T2e/12zq76I0GXo3BqwvNSFQFn1csXuvrCHd3BX2dlx0tHywukp2S7Cw0umF2ERZItoL3mTTMkERbDz6/rPN+M7LKwGsTMW9r9OTOvMLcdtFLAyY9Ye11Ii7sLAyvHAH0emyHKz41LjtBZsOqfGExeMWohhl4q0l2jnAFb3eJ8N3V71kbS2h1Rpj3opeEFLzH/ByX91XBIJsVVWgG3u4U7W3NJhphjW5ouVgiGl7VRa+dQG0H9eVjrSIPcFXBqwa+oEJx0RsNvHJyqAre3dRZwUsD37hbGDpnvl66EvpcX3h4ABDJeR0N+gkeisFbUPChpXRTqOCN2OwRVpMzabIRCTUGn+nIi5anJbqHItdd9FIhhpo9teoJmB6jgdeOH4crPjUectFnm4FXPDumLvpYSXZKDL5xp5j02+IwPU7FRd+ptr1AGFpm069R8STZNWwXx3ZRb31fpLwZm02/rjFaLpaIhjTIeUW6KynoT9xF7zUYeJtTHICFvYRyCPWily76eoOCLxYXN841BW+o3y7UTgAeEOreYWbglWVH5YmQXxbeGtdymVyH/lukQsFHMtZRY/DxuuizwcArSXbRkgjzCnO7k500IPI7elvE/y+/1MTAy3PJJY5tqyouq1z0hiQ7SaJJduBameDOzrHtWIQpeIOBZ0x4H12likL3xEiyM7joew23ngyrhvYoBk9ERHVvqQdi3El2Zi56n26MCiu1i1AgXMEHvJ1j8DwoxuVp6eyilx33AGG0HfnCUKonS1iSnWLgAb28Rja64YHoF0ZVwaciBh/VRR9hNblcTLILxeBjKPi8otw28FLBh/JQWsU54So1cdFrx4KcEFmdgMos+qxIslOSYuX3zCsR3z2oGTXLSXZaJDjoEwq+IhEDHyEGD4j/U36poeudSaMbhwsAMyj4HSLBzvJYtM8gA09ERU2yU41N3ArepI2o36MrswJlZTrVwAPhNaTqinLuZvO1w6ULLF8q+A4TF30kA6+pF5lkB0RecIZz8b5yspOKLPqILvooMXirjW6cWWTgHYqLPpqCdxbmuIu+UWxDCr4tXMGrk1FVwQPWFXk2lcmFylqVGPwALT8o1GVSipRYvei18729XoTrElHw8rpiauBLxHVG/j/8Si96FcbEWOX4g8EEDLxU8LLRTWYn2SVk4BljNsaYiQUgwvjfrcDHvzN/LJRkVxh+ICZFwStd19SlZ1UXPWCIwcsV5VrMk+wAPZPeVarF4D2dXfQBr2bEtUMrZOC1i5vdqc/oI7nppUHvagz+3Z8DH9/TeX+0BTKi1sFbVPDyQpNNLnoeJAUP6DF4b6v4zq5ScTyElW1KBS8NikVFnpVlcg79WiITgOVvEU+ZHADUbxXbeBW8WhpnFiYrqBBCRj7mbRHHs9k57sjXr5Ut+8RkS2bQW0F+huwZoSr41oPA+3dl1FoDlg08Y+xZxlgpY6wIwLcA1jHGfmzhdScxxjYyxrYwxu4wefw2xtg6xthqxthHjLGhymOXM8Y2a3+XWx1rxrDzS2D7IvPH1PiVeoIkrOCNBl5x0UuMCl5VmKEkup3i9WY91EMKvky4631ukyx6Q0vXqAo+koHXLiChGHyCJ8yOz4CN75q8fxR3uz1KmZzVRjd2B1BSFZ8ySBfq/yqWgs9lA6/G4IMBcT65SpRGUi36c0MKXoa7LCryjixU8LKTnc0J9B0n9slrja9d7I+1trpNm9DXbRHbRBS8xMxon3I/cNK9+vNkl06z55b012vfZU+PeM5TeV01a1W7dSHwxUNiTfsMIR4FP4Fz3gzgTADvABgO4NJoL2CM2QE8AuBkABMAXMgYm2B42jcAZnDODwHwMoD7tNf2AvArALMBzALwK8ZYnCnmacbfEd7aUSWs0Y1yAMebRW+za7NSQye7iC76CH3SZS/obZ+IrVlJWJiL3qW56A0xeGPPdvk+bUoMXo4tUqmcnDSEXPQJGni/Rz+ZJdL9H08MnvP4Gt0AwM3LxBKtmY56PMRS8Dntoldi8KEuk8W6J0tNtOuk4K266LNJwSsx+KGHA5PP1Sf9IQVvEuc2Qxp4uahLvBNfRwwF33+yWBrb7gTAohv4gdPFojaciy0A9J8Sx1ikgTdZLlZO3Ix9E9JIPAbeyRhzQhj4BZxzH4BY6aOzAGzhnG/jnHsBPA/gDPUJnPOFnHN55fgKwCDt9okAPuCc13POGwB8AOCkOMabfnzuzku5Srxt+uxXbcgQr4se0NSVcvFVS7oKjQZeddErBr64r1g9aetCcd/MRS8z6fPLtFiWJ1zBSxd9TAWvJN2YEVLw0kWfqIHvECvjSXUG6JOFaFn04OFx+KA/tgvbSF6RedvXTMNsYQ8zpIEPZnZSUcKEYvCt+mRZuugBwyqK2mRPTViNRTCoG55ULIGcbGT4jNmAGVcBZz2mZKFr1xpvW+wMekA/xla/CFSOjl/EhBn4KJPsTl3vTMY2cIbwpDRsB3YvASpH6avQWR2LvH4Z6+DlRG/T+xnTMyIeA/83ADsAFAFYpLnSm6O+AhgIQGlejmptXySuhvAOWH4tY+w6xtgyxtiympoIxjRd+DuEMjCbsauz364k2QGd1VXAp59U6oTBYXTRG2LEgw4F9q/WXhclyc6lKfjQYjNM/1zjuulGAy/r4IHISXbSoHfVRS9POFXFhwx8lBi8cWzR1krPdtTjIVoSYaSVuLqDRf8HvHBJaj8jFINv1TPpw1z0hlUU41XwcqEZV5mY2Gb6RCno10ttJfI6FabgYyTYAXpIru0gcPQd8a/PECsGr+JwRc+4HzRDbKuXAbu+Et304kEunwuI76EKAXkc+NqArR/F974pwrKB55w/xDkfyDk/hQt2ApifrIEwxi4BMAPA/fG8jnP+BOd8Bud8Rp8+cczEugN5IkjjpqKWaskD2FkUO55lhjEBSo3B2x26kbe7EDEGD+gLPgARFLzmDZAx+KBPj1UCuoEPU4X54gRXFXzIRR9DwXfVRS8nCqqBl/uMPa0lZpOPkIGPw0WfLYTF4KNcPON1RyeTA2uBvatS9/4Bn67avW2Kgldd9FoMPuAXx4Zq4K0oeJlkWjrA+mvSSdCvnwsSo4L3tVtz0cuk2r4TgYlnxz+WMAUfY0LhcEVX8H3Gi/2rnhNKfki8Bl5V8HZzF31+WWc3PefAkieA1u4VofEk2f1AS7JjjLEnGWMrABwT42V7AAxW7g/S9hnf+zgAdwI4nXPuiee1GUswoJ/EZm56n1s3MvKgTUS9A50ToIxucmmYo7noAeG+kpgl2RWpLnqZ0NKoPzfo61xOxph4vrwoWCmTk0Y4r1C8V8IK3sTAy8lDRAVvZuCl6reYZJdNqN8p2uQyltcllfBA7M6HXUF1v3sMLvp8g4teDfHY45j0yAluiTTwGZ5oFwyYGHijgrdo4KUX75g74+tgJwktv2zXJwsRn+uKHoO3O0QL4q0fi/tD5sQ5Fpfu5TPG4OVxMO5UseSsSv024J0fAyv/G9/ndZF4fu2rtCS7EwBUQCTY3RvjNUsBjGaMDWeM5QG4AMAC9QmMsWkQ7v/TOecHlYfeA3ACY6xCS647QduXHahGyczA+zt0IyMPxETi74CJi96gomWiXSwX/YApuvvJzEXffzJQOhDoMzY8Y1WqHLnYjNHVqybsydXkAF3Bcx7uslSNsGyoEy8yMQ4QlQESywpecb0FeoiCj5ZjYI8xKUslwUDszoddQcbfbQ4xUQ656E2S7FRvTjxeDWngS6u012SDgjfkkBg7wVl10Y+YD1z3KTDuO4mNRZ53Vs4/R350Aw+IUCQgcorUpaKtjiWagbfnif+xWnUB6It01WyK7/O6SDwGXgZOTgHwNOd8rbLPFM65H8BNEIZ5PYAXOedrGWN3M8ZO1552P4BiAC8xxlYyxhZor60H8FuIScJSAHdr+7ID1SiZuehVBe9MsoI39k0PKfgodfCAUMz9tCIHMxd9rxHAbevECnHyZHM36pOBkIs+hoE3lsl9/XfgL1P0hBV5wXTm6y1x40V9TZcVvKH9bi5hU8MpUQy8zTAp606CqVbwjWJbUiVqqENZ9CW6d8pUwcdRJidL5LJGwfuiKHiZZGdRwdvsQNXUxMcij0srOTAOV/hCXmbIUOSQwxLLBwhLslNd9Fpo1OYU+1WRIJfJrd0Y3+d1kXhWk1vOGHsfojzuZ4yxEgAxM0U4528DeNuw75fK7eOivPYpAE/FMcbMQU1GMlXwbt3I2PMAsPizSyXRkuwAPTnOnqe1WNRWaTPrtDZwhlj72czAq8iThwc7x+CNJ2InBW8ok9v5OdC0S3wHuUodIH4fZ5INvE+5QJvR02LwNpu+fG+0CUyssEoqCfpjrz7YFWRSVtlAUTolm93ISoi8YkXBy+MnQQVf0t/6a9KJaQzexEVvJYu+qzgNns5oWMm4HzRLGOdhR8Y/lpnXAiOOFrc7KXg3wpbkDvh0tS+9iDWbhJCJd2KRIPEY+KsBTAWwjXPezhirBHBlSkaVC6gK3jQG36ErX8bETDhhF71ZDF4xYKqLHtDi2h3mndYOv1nMcK3EuiQhA6+VyRlPRGngmU0zKAYjKhtgdDSKi6qqkhz5iXWy8ykX4sZd+kkl3ztaHTwQrlRDBj4HFTwgjoegL7pCitV9MJUE/alV8DIpq3QgsOtLPSFOtnJ2lZq46FUFbyXJrk5bvrnS+mvSSTDQOSejU5KdRRd9V4lLwatNcSJMPsoGAt9bBPQeG/9Y+k8Sf0DnTnZ+b+fySemllcvSeltEBz0Zqkkx8WTRByES3X7BGPs/AIdzzlenbGTZjqqoZZMXFVXBA8DYk4ER8xL7LGdR5Fa1AFCoeQZCmfVy8QgTBV85EpgetX+RQB27S3HR+z2dY7lyIiNVoFoHHwwCdVoLSxk7U3tcO/ITUzvSkFeOEhdn6YZVFZgZZgo+l2PwgH4hj6rg05xkl9IYvKLgAaD1gNg6tfMjv9TERR+vgq8Pb6eaFQreEIO354lJekjBt1lz0XcVea2xcv6px3C0yUf/yV2fsBuXiw14whOZ1WO2YacuqGq6z00fTxb9vQB+AGCd9ncLY+z3qRpY1hOWZGdi4H0d4Yle5zwJHHJeYp+VV9h5PXj14JUNakIz4SgG3ipmCj7o6xweAHQFL42EegI0V+vhDGMjEIeMwSeg4OUFtPcYsZVu+pD7P5KBl3XwJvWt8TS6ySasKKRY7YVTSTCATs2Hkomc/JVqPbZa9gvjLjO+Iyr4OMvkCivjy7xPJ2YueulpjLeTXVdJWMGn2Ltg6qJX8pzUc6Vxp+7a70YDH4+L/hQAUzUlD8bYvyHazP48FQPLeuRJYHfFjsF3lbwicUL6NcNuLFWbdLY4OcuHaGNKgoF3mil4rz6LVZEGXrp51XiubF8JKApecaM7E1Xw2u+vGvgBU/RZdaTZe9QyuRw18Pa88K3pc2K0F04l0rCrMc1k4m4Sx6QsA209EL7SYn6pstSrMkGMZ7lYT3P4kqYZn2RnYuABcU762sVxEPB2j4GX1xor18vQYjC21CfFdjLwXvPQjbtZXNsGzRTrY3Rjol28RYnlym2TZuVECGkUygaa96M3KviuIF2JUsUbS9Xyy4BDr9ATO2R73K5cLNWZcswku3KxDSl4JZ4r4++AouA7tJpXZ+Ix+JCCHy22UsHLWbXN2fk16hh7SpIdoBvvqArepMNfdyE/M1Xeg45GcY7I47hlf/jk11ViruDl8WDFWMt4dbwr0KWLQDQD36GspZGhMXhnYeoT2cx60cssekAXEzLBrmKoiPt3Y6lcPAb+DwC+YYz9S1PvywFEWAuVCBml8iFRYvBJMhgyk1Um2pmVqqnY87q+VnmYgS8GwDQDH81F7wzfBn1CwRtXgfK5lVl7ggpe/v4lA8RJ17Jf3DeuV2/ErN4755PsXOFbM9Lqotf+F6mKw7ubRImqjJG27DcspVyq1zWr4SN5DFkx1j6t70W8K9ClC7MYPCDEhK9dP7+6I4s+nhh8aDLQDZNxZu8cylNd9PJ4lQl25UOBPmMyU8Fzzp8DcBiAVwG8AmAO5/yFVA0s65EnQNlg0RlLVaGcJzcDVSp4r7YYSKySp2QY+LD+0AXioA54IyTZRYjBB/1A3Wag7wRhQEKJcB3KTLygazF4p5YMJU826WKO1LVNjcE37wO+faXnJNlFm8DEai+cSrh2EU2V98DdKI5ReU4EPOGdHMOS7JTjijHrnRb9RgWfDQbe5BwJKXhZa56hMfjuGJdZoxszF72sga8YJhR8W40e8kkxMQ08Y2y6/AMwAGLRl2oAVdo+wgy/YuCBcBUf8ALgSVTwios+GCPGDIiLtVkr2ngIS2bR1ExIwRs+W8bopWte3t+7EqjdItzoBRWKi96jKHhX17Lo5eRD/i4hBR/LRe8DVj0LvHwVUK91ocrFRjeAEoO30OgmVYlu0VBj8KnA3aS56BXVHuaiLxPnc8DXuQrD7rKWZCcbW8VTWpdOIsbgtSQ7tdIl1cRVB6/9vskKf0bDuJqcLE82ToYbd4prXkEF0Gec2FfbPW56K0l2f4ryGEfsfvQ9E5koVi4NfI1+O9knR8hF326t65o9r+vGythQQhrRgKfz5MKo4EsHAIdcACx5XJwUlaOFsVfL5EKz9gLzGPyeFeJ9I7WaVBPjbE79ghpPDF6WHu74rPN3ziXksRBtUhhrid9U0h0x+PKh4W75PEOSHSBUvDEfQya1xsLXLo7lVCn4fauBfpPCe72/+UPx253x1/jfz6wXPSCuWe11euOeRJtzxUNCCr4bJh5Ws+gbdorjizFgyGzgmo/1jqEpJqaB55xbWjGOMXY85/yDrg8pRwgpeK30RlXwsWqx4yXkom/TZ43R1NjUi7uegKKeQHLFOL8neqta1age/xtgw1vi+b1HGRS8UmFgpuCDQeDZ80SryfMjLN6gNrSR3gVA+X0sGHjpmt+3Sh9LLuKwoODT6aIPKfhUueg1BR8WdzfE4AGRaGesqLC7rMXT/VLBx1FaZ5XaLcDfjgIueDa83/u+VYkn0gb95se7dNG3asuGFPdP7P3jIZEYfLe46Jm1LPrGnaIfByCOM9kLvxtIYGmfiPwxie+V/fjcAJjojgWEZ9KnSsH72mK7oAHg0MuB6Zd17TNtDjGDBXQjGioNjFEHD4iWnUffIW73mxxu4NUKAxmDV11h+1YKj4jagtaI2snO7gw38MwW+cKnuqLlxIIHtR76KSjRygRCCt5Kkl2OZdFzLmLwBeXiWJPHdJix18JZHkXBS0PtyIudZBcMaga+UCmtS6D9ciSaq8XWWF/taU58QmbWix7QXPTtetJqSb/E3j8eElHw3eFts9n1/BBALxEOZdErq4kWd8PvZEIyDXz3NNfNFmRSTZG2Rr1aC59sBS/jhd52XU2kvAaUhc+s7U49i9+oBPOKzJd6nHMjcPMKRcE3iv1qhYGZS3PLR2LbvDfy+MIWBVFc9LEqDNRyMPUinKvueUCJwUdbLjaNZXJSJaXCe+BrF98pv0wc09KwR3TRu8NXRLSi4I2Nm4Dkuuilu1xma0vcXTHwMcrkWveL755oe+14iGs1ue5U8GYu+vzOPSNkC9s0kEwDz2M/pQfh0zLBXSXiJFCTKpKt4EN18O1KI5ducCc7lRPP7tTX0TYaULkmvFlnLBlDLyg3KHhDcwvV2G75UGzbaiJfKP0RFHyk7GCJqlRVZZarCXaAtSS7tLroU6jgZZta6WWShj2Si95nKG+1ouCN4SIguS56mZGtLosMiNK+RH8zs170gJ5k13JAqPfuWDTFmQ8c92tg4tmxn5vWGLzWaMz4PzbLS+omkmngCRVZy82YWLVo+2f6Y0lX8EodfMiFGMWIJYvQyaRlB0sFb3Yw55dFN6wFFWKCIDOV1fcG9N+soxGoXqq3FY2k4v1u8Xk2u76YCqAp+GgGXonB+9364iA9QcFnuos+FTF4dWlYQPeGqVn0RgWv/k6O/NgKXp3Qh0rrkqngNQPfoBj4gE/P/E+EiHXwWie71v3d63Y+8lag77jYz5OT1G7LotcMPOd6oxu74qLnPLbXMIUk08DvSOJ7ZT9qLffwuWJ2LU/AZCt4qXK9agy+GxR8yHVWIAxjyEVvZuBLo8ewZTZuR6NBwRsM/PZPRdxr2iXifkQDr5TaqVn0Zo14VEKryfnFe5RWARXDc7fJDZD5i83IJLtUKHjpdZIGXSr3PKWM1KWpe0+LVuusGA+7y7qCDy0PbbG0zirSRd+0W/+tZN1+ov+vaC56HgCaqtMWV45Kulz0Qb+4rbrog/7YjbVSjOVe9IwxM/9IE4A1nPODnHML/pMehNqNbfhcsd3xmWhXGGvBk3ix2fQOU1aS7JKFUcFLJWE2uZhzk/kFQxIy8PUi6UkmNsnPkElzWz4ULtMJZwCf3gs07zF/P7XUzphkZzUGH9Au5iOP0Ve8y0WsJDGldbnYFNbBewwG3sxFLxV8R72Jgs/TjWkkQm1d4yyts4o08EG/OB/KhwAeLfSQ6G8WiJJkB4gE15EZWCHdnUl2TLtWcB7e7VJ10cvrcZoqcOJdD34OgIXa/aMh2tUOZ4zdzTl/Osljy25UBd9nnEi2275IKM9Ya5InglwTvjsPqLAYfJ4SgzeZXMRaKa+gXGz3rRZx0X4T9fcG9N9sx+fA0CP0hXMiGXi/R1FMSoZ/pOxgSZiLXjPwx98dfezZjpXFZjKhVW0qvAfS6xRS7nKruOjtTrEiY+uBzi2mrSTZhRZPKrT+mnhor4PIceZazfUQvbVuV2LwkRQ8IP4X3VEiFy/dreAB8VupnlM1i97fjR5VE+Jx0TsAjOecf5dz/l0AEyAS62YD+GkqBpfVqG5mxoBhR4k4vGxTCyR3linLV7rTJeQoUOLcjshJdlaQCn7bJ2I7YIq2v1xsm/eI0pz6rcDQw8UFOb8MaIpk4I0K3moWvSEGn8vJdRJLi82keT14IDUKPuSiNxh21UUPiLLOlv16O1KJpSQ7w/lu5TXx0F6nLKqkhQGlVyHpWfSK4eyOErl46dYkOy3BkAc7V+0A2op76V3HIh4DP5hzfkC5f1DbVw8gDdP6DMfYa374XKBlr3D1pkTBF2lJdt1p4F36d7DnKd6DLhp4Zgf6agp+4Azhkt/0LrDzC7Fv2BFiWzowegw+1E5UddH740iyS195S7fi0Oq/LWXR51gdfKQYvOqiB0S8uWV/52WeLSl4Q86N3ZXcOvj2ejEhZjY9z8eTKgOvfPeMjsF3UxY9oBl4ZZVB1UVvpbNoConHRf8JY+x/AF7S7p+j7SsC0JjsgWU9RlfeoJliu39VahS8NPDdqeCdBYoRVT4vkc+W9bTN1aLlphqvHHUcsPEdccFxFgH9NXVfOlBv8mFEdg6T47GcRa8sNmOMt+Yq0y8TrTNtUeb7NjsAliYXfSoVvMyij+KiB4SCr9kgjjn1MStq3GjgHSlIsivuJypLZC18l130FhQ8GXix5cHw0KiaRZ8tSXYAbgRwNoAjtfv/BvAK55wDsNTOtkdhVPCy4U1HQ2oUfDpc9M5CvURPvRgkEm/KL0Mojjhgavhj474DrH0VWPWC6OUsE75Kq0RXOzPUemVjL/po5XpqH2mjOzZXKRso/mJhc3S/iz4YRKjFRipj8MYkuzwTBd96ACjopZdOAvE3ugHM2y8nirddhAAKK0UCr9FFH/SLsGC89eqxYvCAmPRkGqVVQK+Reg5PKpFiQHXRqzF4NYs+05PsOOecMbYYgLYUGr7WjDthhlHBy1hyR4Mw/sye3Ez3vCLhquuKmzxejrxV1MMCBgWfwPey2YWRdzfq8XfJ6OPFSeNrE/F3SdkgvdmN8QRSa9jjyqI39KLvCQbeKurv2F2oRj0lWfQt4bXLw+cCB9fpzW0kJQP0LHV1gSOHhTK5UBZ9CsrkOrTKlcJKsaCJbAIls+gBMe54z8mgL3IdPCDUqxQtmUR+GXDLiu75rJCCD+jHgMMlPGE2R3Yl2THGzgPwNYRr/jwASxhj56RqYFmPUcE7XELxdjTqfamTibNQaxTTjQq+3wS9VEb9vESNoozDGw18fploFgSIDHpJaZXYmsXhVfe6cbEZY8tcFWOSXU+IwVvF5ux+Ba/2+k5JDL4tPN4+7Ajg/Kc7hytkQllHvaGTXRxZ9PGuQBeNzx4Alj6pl8gVVor1xlv3i2uPWrqXyGQilou+qE/urs1glTAXvRKDB3SvYah1eDeULZsQj4v+TgAzOecHAYAx1gfAhwBeTsXAsh6jgQf0fut2Z/I7LRX1FivWdWeSnYpqNBP97IIKEUPsP6nzY4deIVawqpqu75ML+TTvBXoNF8a7dpNwz6nGOUzBewF7eeQxMDUG7+kZWfRWsdnT4KJXFXyKXPTGeLsZakmYOoGVajyaG1xm0SezTG7lM+J9eg0X9wsr9dLRxt16DB6I3/PBeWTVL69pmRh/725CBp6Hu+gBTVT4w5Pv0kA8WfQ2adw16uJ8fc8hGBBqw2Fm4Bs6Z+Img9KBgLdFX7Wu2w18F130AFDcF+g73vyCO/FM4PtfhE+MQgZeK5Vb/QLw+FFAq+a2d6oGXsbgY/Wit4kTNyBj8KTgQ6TFRZ9qBd/aOd5uhloSZuxFD0RX5HJlyVBDoSSUybXXAbUbgeZ94n5hLz2PomWvnkUPxD8pk93Zoil4MvCGLHpDaFRec7qzs6gJ8Sj4dxlj7wF4Trt/PoC3kz+kHCCUNWswDtLAp0LBy5O7fpvYdveMUTWaiR7MJ/0hvguf/M5NWiZ97Sbh0m3Zqy/2A8SXRQ+IC5vfDYBTDF4lHS561cCnqg7eioGPpuABocgjndO+dnEsSoXfVQUfDGgrL3Jgl1Y6WlipG+TmvQYXfZy/m/wfR4vBZ2INfHdj6qJXREXQ1705USbEk2T3Y8bYdwHIIOgTnPPXUjOsLMfYe1qSXybq4PNLk68M5eIr0sBH69aWClSjmaj3oNeI+J6fVyQubDJzuHG32IYS75Qs+qBfZGTH6kUPaH31tfpoMvA69nRk0Sufl6oYvBUD78zXkkCbOsfggegTU7/bkI/TRQUvjTsAbFsEgIkyU+n5ajYq+Dh/NzkhiJRFb3cBZUPie89cRO1kZ6x3l96u0P7MV/DgnL8C4JUUjSV3iKbg3Y2Ar2/y6zTLpIHfLg6y7ljGUSUsya4bZ6sVw/TmHk3SwNeGtwpWS99iZdEDYkIQWhmPXPQhbI7ud9GrSXapiMF7Wq27m0sGaAZeVfCyqUkMF32Ygc/vWqMbmVgHAE27xHXFrq1RX1ChGfguxOBDCt7EPNjswFXvhlcS9FQiNboB9MZf3bk2iAkxY+iMsRbGWLPJXwtjLMYqCz2UUCObaDH4JBuOkv4iQczdmJ7ZYjIUfCKUD9Wbe0gF37JPnHRqoxtAXOhi9aIHxEUs2sp4PRWbs/sb3VhV8J7WyG2Lo+Ft0xc2ioWcCJgq+GgGvt2kf31XFLxWGgdtEq/W5cvuju5m/foTt4HXJlWRzpOB07W+FT0c00Y3au8NX+Yn2XHOSzjnpSZ/JZzz0liv71FsfAe4f7Se6GaWRe93CyOfbAVvswuFAaRntpguA18xTCh3b7teky9j8kYFL2fUMRW86qInBR/C7giPiXcHVuvgP70X+OdJ8b+/t9VaFj2gN3ZRPXNqW9JIGMtiu1omJxX8gEPENszAV+lJdnJ/vJOyaAqe0AlT8DKL3uCiT3OSHWXBJ8qr1wGfPxS+b/8aoO0gsHeluN/JRV8uti37U2M4ZNJZOmaL8sC2Obs3PFAxVFyQqr/W90Uy8EF/7F70gGbgteYkFIPXSYeLPhhUbkdx0TfsEP939flWiMfAJ6zgOwyTApcwupHGunUhULs58vtJAy+XoVYNfMkA4cnwtIjMeiC5LnpCJ6yTXQQXvbqMbBogA58omz8Atn4Uvk+eePtXi62Zix4Qs+tU9EqWcfh0KHiZRd/dLu2KYWK7Y7G+T7pqjX3yQwreioGXMXgy8CHS7aKPZqja68WF1tsS+TlG/NrxYFw5LhJSwRvd7UB0Ba9WdAD6xd74Gs6BT/4IPH0m8PE9kd8vZOCPFltpyAHhom+vBcAVBR9n7gIZeGuYuehVBR8kBZ+dBPwiDmaM+bVrsbF9moE3S7KTpELBy7rwdMbgu3umWj5UbLd/Jra9RojEIyA8HgaIky1WL3ogPAZPBl7H7uz+1eSsdrJr08JiHQ3W39tn6EMfi5CBV5eLtaDg/R3hLnq1tE7lsz8Bn/xeHJ/SC2VGu9ZNb7C2gFVhb/2x0gH6bWng4433k4G3htFFb3cppZBGF32GJtkRJsgZdPMeMes27q/ZILaRFDyQYgWfBndQaObazZ9dNkgkF+5ZLk64/oeITGcgvNENIC7CPGgtBi8v/hSD18nkTnbtCRh4j2Gp2FiUDRZbNSnPkoveHT7Zj1Rat+tLsZLi5HP15k1mtNeLRW/yy4Bz/wXMvFp/TLZvBhQDn8Q6eEInbD14b+fqCumiT0dVkwYZ+ERo0xr6+drDLyjSwEvVYVTwcklUIMUKPo1Jdt3tPbA7Re5B0Cfij+oFzuiil3H1aL3o5XvKi3+aXGsZSbpd9JE+OxjQvWfxGHjppTGu/R6JQTOBC58Hhh2l77NUJtdhWEM+wms6GkScv2yQyNOJZJjb63TjPfEsvUUtoF8DAHLRpxpjoxvVwNsUBZ/GawgZ+ERoq9Fvq640eZGRGBeU6S4Fn5YkO2f4tjuRcfiywaInv8SYZCdVeUwFb++8eASRpla1SiJapM/uaECo8UsiBt5KoxtAqLCxJ4crW8suelXB55u/pqNBxNNLqwBwYeTN6KgPj7urlKgu+i4m2aXJrZw1GOvgVUOuNrpJU4IdQAY+MWS8Dwh3pXVorjOJUaW7SvTFTFKSRZ9GF72Ma6fDIMo4fNmg8Hik0cBLBR8zBq8oF3LR69jSXCYXScGr52NcBl5LyLPqojfDSpmcz6RMDjA38AUV+nlstkoiEK7gjeSXAU7t+0gDT2VyqYEZsujNXPSk4LMQMwXvc4uSmyGH6Y8ZVTpjuopPhYIvrBQGKa0x+DQq+PLB4WtUqyUrgF7bbiWLPvQe1OgmhM2R5iz6CK7m9kQNfJwK3gyrCt408155jewvX1ChLIMcIQ7fXhdZwTOmJ9olGoMPUAzeEmqrWqOLXs2iT6MnhAx8IrTVCBVoc+onoewuNXi29iRmbmhlLXwqlCFjIgaXVhd9Gj47zEWvGHg5iZKK3Sdj8KTgEyIdLnorWfRq69aORuvvnQwDH6tMLuATkxRjL3ogPMnO3QSAawbesEqiipwIRFLwgD5BkN4sqoNPDZ1c9IYVNUMu+vQpePoPJkJrjTAkdodeKicvMr2Gi0QZT4t55mQqFTwAnHJfeDJfdxEy8GlQvP0mAWBiW6Rc+Dop+Pbw+5EIM/AUgw9hcybPRR/wC4+KnPBGwkodvHTR2/PiVPCaR8dqkp0ZsZaLDa1LoSbZmSh4Oe6CCt3NbuailwvNRDPwJVXC+ORrjUbj9brIhWqs9gfoqYStB29YWjpDXPRk4BOhrUYkc+UV6y56mWBXWAn0HgMcWGv+WmngU6UMRx2XmveNRTpd9H3HAbdvEuvJq4tsyMxlY5KdlV70Esqi17HZk+ei//oJYPGDwI+jdGwD9CQ7R0HkbHA5ua4YntoyOTMi1bRLQitLxiiTk56HggohDMoGmtfCy+8azcCPPl54q9Q1GOKhVasSKu4b3+t6GsZGN2o4z6b1jKAkuwynfhvwyjV6bTUgDHxxX3ESNksDr5x4w+cC/Saav19IweeY69eexiQ7QL8Y5RXrF9OQgjck2VlZTU5CCl4nmS762k2i3DRW4xxp1B2u6AreVSqOgURi8M4uGPhYy8XKsJAzRpmcquAB4WZXFXzABxxcr19n1IocI5PPAc5/Wj+O4y2TIwNvDZtJoxuJ3Wl97YsUQgY+GsEA8NoNwJqX9E5pgLigFPUR2a7N+4TKCJ14vYB5PwGu+J/5e0r3ubEJTrYTalWb5tIaxvTYo7yoyhMs3hi82pmK0Fz0SWp0IxNVYy2bKj/PWRAlBl8rJtZytUareFuFcbd14TLItFybSN/Dp+0Pi8GbJOZ1MvCDwmPwix8EHp0D7NRaMkdT8JLQIkvxKvgD4jpFk9vohBR8IHKjGzLwGczXTwC7vxK3D3wrtpwL5VHUWyTDBH3ifshFHyG7VZKzCl666DPgolDUW5x8IUMtFbzVLHpZypgB3yWTsDmSaOC1uHmsVdVkkp0jP7Lab6sV//NEDHxX3POSaMu/+k2WjrZk4Kv0ZjcBP7DsKQBcX+AqLgMfZ6va1gP6wjpEZKI1urE7NcPvTut1hAx8JOq3Ax/+Bhh9AlA5SqwUB4iLgt+tK3hAJNp11AOustjGIxSDzzEFL7vDZcL66UV9hEGQ6ttmcNFbrYMnAx+OPYmryYUUfEf05wUVAx9RwdcLr4008Gr76Gh425Jj4KMt/xpKslNj8Nq5L5PZAL0KR3r4ygYi1Oxm0ztAyz6g7wT9NVYMfKIuehmCJKITq9ENIPI8SMFnICX9gcNuAE79s8jOlgpeXpiK+irlLNXRa1NVKkeKf7hazpULyIM4E+rGpYGXdHLRW8yipxK5cJLpom+3qOCDStvniK1ba0X1REGFUKvy/xwLb1vXMugldlfkJDufiYIv6Q+UDQG2fKjv62jQBIJ27IWuLXuFei8dCJz/X2FUHPlAnqFLphnSE5WIi54MfGyMBt7oogfEMUYKPgNxFgDH/UrMpPtPFutNu5t116Kq4Bt2RO8upTLqOOBHG4HiHDXwmaDgZ14DHH+3fj/kom8Lvx8JWwZ5IzIJm0O4Ha0q5Ej4vXrSqoxdf/ArYO3rnZ8bSrKLoOA5F+ekVPCAdTe9p6VrNfASR17kJDu/SQyeMWDC6WLdd5k939EQXjIoDfyi+4CtHwPTLxfiYNI5et+HWDCW2PoBrQfJRW8FtZOdWS96QFxzSMFnOP0ni+2BtXqGaVFvodgrR4kEPKsKnjFrz8s2MskoDjoUmH6pfj9UJhdnkh0p+HCkuuyqm17tPCcN4MpngM3vd36uauDNYvCeZmHAihIw8Mly0VtR8Ma+FxPPEuPe+I64L9vUSsoGieNw68fAmJOBWdeK/Wf8Fbjq3TjGlhff/8vTKsKQpOBjE6vRDSB+yzReE6kO3gr9JontgW/1i788AUYdByz/lyjT6TshLcPLCDJJwRsxNrqJFYO3UwzelFBM1wegC/9ntdWzdNH7Oszd9aGVGSNk0UuPWiIK3tsWvhJbokRT8CEXvWGyOPBQkSm/7g1g6oWdDXx+KXDNR0JJq2u8O1zxHZfx5k3IlTJJwccm1Ko2aN7oBgDAyUWf8ZRWiZNv/5rwCwoAjDxWqJC2g9Zc9LlKJht4m12cjKEseqsxeDLwYcjfpatx+DYTBe/rMM/2DiXZRaiDl+WpCSn4JGXRlw4Eatabhy5CLnpDzJwxYMIZwNaPRLjCaOABoGpquHFPhHhd9K1KjhERHZnE63dDGHITBQ+Qiz7jYVob1P1rtBrRMv2fOewIPXsyF13vVnHkAUf/TMQWMxGbU3HRx+pkRwbelFDZVTINvEcYbh4wN+AhF32ETnahCXdlfAbe2y4y02UeTVcYe7LIwzHrXmmWRa++LuAFdn5pbuCTQbzNiVoPiC256GMjFbxZt0LVwJOCzwL6HwLsXQEs/TtQ3F/fn1cEDD1c3C7owQYeAI6+I3IHv3Rjz4u/Fz3F4MMJKfguxuDDXPRu3QhGU/CRsujVDpLxGPiaDSJ2KsNvXWHsKQAYsP7Nzo+ZZdFLBh4qErWqv06tgY/H4xIy8OSij4msUpD/Y7tJFr3xdjdDMXirzLpGnCwF5cCwueGPjToW2LawZ7voMx27U7/wx6yD107cTAw3pJNkuejbDQpeKiBTAx8ji16uPZBfKuL0dpc1Ay/VdjImpMV9gSFzgA3/A+b/LPwxf4f43cy8RnmFQP9JwJaPxGQjFR5AW7wK/qBQpkW9kz+WXEMq+NAkztCLXkIu+iyg1wjg+N8AR94qsrRVJpwJVE0TMTMiM7E7AXDldhRIwZuTaOtTI201eolRLAWvdrLjQX3xGYnMq8grFqE0q93sDqwVcfGK4Yl9ByPjTxVJuHVbDeNri16KN2gmsG+luJ0yF30cnexaDwihQmvBxybkojdJpCQXfQ5RPhi47pPkZOQSqcGshCUSIQNPCj6MRDujGWmr1dcsj6ngFQMPdFbx3lah2uX/1KqBP7gW6Du+a33oVcadKray7E3iaREVNpEYNFO/nQoDH29zorYacs9bRRp46UUyW1DIeLubIQNP9AziyWqVhowUfDj2ZGXR1+jJbWEKPkqSnUxSMz7H2I2uoFxvHhMJzoH93yY3X6RiqFg/XV0gBhDNsVxR1lVPtYGPt0yOuthZRxp42RulUAlrUBY9QXQjakzM6nrwZODDkb9bIi56vxd440bhwm6r1Tu1WVHwzGaowVfwGErdXKW6oopE6wHR+z0ZCXYqeUWdP9vTLPIDItFrhG7YU2Lg8+Isk6MudpaRBl4mjao5WBnioqckO6JnIGfRNmfsJWAzqStfJhHJyFqhfivwzX/Fe7TVin7sNke4gjdrdBP0a0lqEUr0vK1COUtcJUDd5uhjketKJLviw1Wst0OWeFqiK2LGhIrf/H7qXPRWyxo5JwUfD0YDryYmkoueILoRaSCsrFdPSXbmhFz0gfhfK5XtmpcAX5u4GDryxXrpIQUfwUXP7JFL9IzNalwlsRW8zKBPdufJvCI96U/iieGiB4Dh88RCMylz0VtMsnM3iefm2kJYqSLkoq8BwPSVAIFwjyEpeIJIMQkZeGp0E0ZXXPRubZlTaQCL+ojf1+/WGxCZZtEHDQreJAavZqnnx3DRt9cDm94TIYJkl6XllZgr+GhJdgAw+3rgkPOtHZvxEk8nO3ej2KZiopGLhBT8QZH7oZZChsXgKYueIFKL6qKPRSgGTwY+jK646OU65vL/UKgpeL9HqHggsoK32SNn8Htaw5PsXCVi0mDWG377Z8BfpgC7vtQXb0kmpjH4ltgK3u5I3eqS9jhc9LL6gAy8NdROdmqCHRBf1U4KIQNP9AxCCt5CPIwUvDn2CEbWCtLATzxbbIt66wpe1hGbrcgWDAgDH2klO6OCl2rZTMWvfEa81w1fiH4WycYYg/d7xfeLpeBTiT0OBS+rD1RXMxEZtVeAsckZJdkRRDciFWCsPvSAfnJSDD4ceUFLpBe9NLjzfgKU9AMGTNEUvFtR8BE62dkckb0H3tYIBr4ZKDJcdA+sFQ2p+o6Pf/xWMMbg1S576SKeTnYhF315qkaTWzBFHxs7/1GSHUF0I4koeMqiD6crLnp3MwAmOscdf7f4f0gXvVTwPNg5gS8YiBGDN0myA3SPgSTgB2o2pnZJ57wSETKQyDHEctGnElLwqUM18MZ8jgxR8GTgiZ5BQjF4UvBhdKVVrYxFq53jjAoeECre2wZ8+Buxnwe0LHqT8IDfK55vjMHLz1Op3yZCAMmufVdxFYsKAdlOV44hnQbeFkejG1Lw8RFm4A0K3tZDkuwYYycxxjYyxrYwxu4weXwuY2wFY8zPGDvH8FiAMbZS+1uQ6rESOQyVyXWdUKlaImVyzZ1j0Q5XeKMbQNzf+SWw+AFgzzI9yc4sBq/2oZfkR4jBH5SLy6RSwWueBJ8Whw8p+DTH4K0a+I5GYZiMa9cT5qj9NKLF4HM1yY4xZgfwCICTAUwAcCFjzHiG7QJwBYBnTd6ig3M+VfvL0IXGiawgIQNPLvowYi0X62kFNrwlGqYYcTd1jkWHFHyHvi/g0w2+360n2ZmFB2RCm5UkuwNrhSeg99jI36+ryHHIcWWCgo+nk527Uaj3WI2gCAFTkuyMMXibXX88h130swBs4Zxv45x7ATwP4Az1CZzzHZzz1QCCZm9AEElBuuitxNWlS42UTDixXPRrXwWevwjYs7zzY2blYmYKPuBVDLzHvJPdnuUiph9S8CYxeHdT+GcdWAdUjtR72qcCaeBlHD5k4NOZZOeIo0yukeLv8RDmojdZKjyevJ8UkWoDPxDAbuV+tbbPKvmMsWWMsa8YY2cmdWREz0IqwFh96AFg+FHAdx4AqqandkzZRqz14OWiGxve6vyYqYveTMF7TRS8Q/9sTzPw5InA0n/Ep+APrk1tgh2g5wLIiYecZKQziz6eJDup4AlrREuyA3TDnsMKvqsM5ZzPAHARgD8zxkYan8AYu06bBCyrqanp/hES2UE8s2mHC5h5dfKWEs0VYi0XKxulbHxbbBt2AI27xO1oCj6igfdoSXY23cA37xEGq3mPopCLw9/T5gw38J5WMZZUJtgBuifBa1Tw6Uyy09aDNwubGOlopCY38RAtyQ7oEQp+D4DByv1B2j5LcM73aNttAD4BMM3kOU9wzmdwzmf06UM9lIkIhFz06Ut4yXoiNZuRtNeLbc0G4UZ/8gRgwc1in9tkVTWp4P1GA681vPG7O7vom7TLR+tBRcErLnrGtH70SplczQaxTWWCHaAveqPG4G2O9CZrhpoTWUiMdDeSiz4eYrnobU4AzJrXMEWk2sAvBTCaMTacMZYH4AIAlrLhGWMVjDGXdrs3gCMArEvZSIncJp4kO8KcmAq+Xlcyz5wnViZr1CJ0Zj3ZQwo+VgxeSbJrrhbbtlpzFz3QuR/95vcBMNHkJpXIiYb8bBmWSGfSWsjAW3DTdzSSiz4e1HJadZIpseeJYzyN//+UGnjOuR/ATQDeA7AewIuc87WMsbsZY6cDAGNsJmOsGsC5AP7GGNPqWTAewDLG2CoACwHcyzknA08khrzQWamDJ8yJlUXfXgf0nyRc4e21wri1HtAy4zsixOA7xGIz0sMS8OkGX43BS++BVPBtBwGvZkiNBl5dUc7vAZY9BYw5ESitSvy7W8FlkkWfTvc8oB/vsUrlgkGt0qE85UPKGaSCL6w0N+J2Z1pr4IFuaFXLOX8bwNuGfb9Ubi+FcN0bX/cFgMmpHh/RQ4gni54wJ9Ka7JL2eqBsMDDhDODbV4Hhc4GFvwOa94rHO7notYufpxnILxPraqtZ9X6PkmQnFbw08DW6IXUZDXypvnrd2tfFc2ddl9BXjguzGHw6M+gB6+sHeJoAcFLw8SCNupl7HtC6Nab3ekNZRETPIJ5e9IQ5jIna3qgu+l7AjKuAK/4HlA8R++s2i22nJDstNt3RqBvCgC88Bi+T7KShkpn67iYt5s8AR0H4+7pK9Rj8138DKkcDI+Yn8o3jw6wOPp0Z9ICyxG+MNeGpTW1iMFt0A59mQUEGnugZZEBGa05gcwgX/Y7FwL7V+v5gQIvhKuVCxf3EtlYaeIOxkzXpahOcSHXwoUQlJRu8YYdQzcZqB+mir9sqkv26qyJCumRleMDdlH4XvdX2wtSmNjGYrXOTG4k9L+3XGzLwRM8gnl70RGTk+uJv/hD46G59v1tz8ar1wCX9xbZ2k9hGUvDgwkUPmNTBG7LoAT322bC9c/xdfo6nWc+eHzQrzi/ZBdQlYzMhBi+P+1hJdqTgE4PZIyt4mzPtS06Tv5LoGVAWfXKQCr55b/hvKUvkoil4szI5SZiLXlXwwfAsegDoNVK4/eu3A0UmpbEyi75ui7hf2al9RupQl4zNhBh8yEUfIwZPCj4xTvo9MGSO+WMZ4KInA0/0DKgOPjnYHKKhja9NT3gDRPwdCFczBRXCZR3JRa+qm5CL3qPE4JUyOfX/1m+iMPCeZqDX8M5jdJUIT8CBdWIC0J1GK69EUfDNGaDgLZbJSQVPjW7iY+Y1kR8rG5T2/z8ZeKJnQDH45GB36t3p3E2iS5yrWJTIAUChYiAYEyq+SXu+WZmcxBXFRc/sWs0xA8BFKd6618VzTF302ufsXQFUjkrwiyZIXpFQ7n6PtpRtppTJxUiykwqeXPTJ47S/WOsgmEIoBk/0DEK96EnBdwmbU29eA+glcGYuegAo6affjlQmpz4WVgevtaqVbmY5SasYri8EFM3A127qXvc8oMfgZaKdzC1IF7FKGyUdjWLy6yyI/jzCOg5Xahc3sgAZeKJnQC765GCzAy379PvSTR9y0RsMvIzDy65eKqqCj5VkB+iTs+J+euzdrIOYqporR8f+Tskkr1jE4OVCM+lW8FZd9LJNLS0Vm1OQgSd6BpRklxzsToSVqqkK3ubo7IYvGSC2ZslmqsGXj/uNMfig3hJU9jAo6R+Hge9uF71BwafbwFvtZEdtanMSMvBEz4Bi8MlBGgzpIpcGvqNeJGgZFaB00Zs1fAmLwWuGMCyL3q0n2amfXdxX/KmvU1E/q7sNvKtYGPdMWAseiCPJroHi7zkIGXiiZxCqg6e80i4hjW3ZYJExL1307fWd4+8AUKzVwpsZYlXBOwv1pU2NjW6YVPBOMSlwlerNRaIpeGYzz7JPJXlFmoJvDh9LugiVyRkMvLsJ2PuNcr+RFHwOQgae6BmQgk8O8ncsHQCUDlQUfIN5ww/Z7MbURa8oeGe++N8YY/Bqkp3NKZQ7Y0CRpuCjJdmVD+n+RiN5xUIty9JAszr97iRSJ7uv/w78/VigZb8Ig7TsN5+gEVkNyRmiZ2CjGHxSkL9jyQDRA75JW761vQ7oNaLz82WSXawYvLNA65LnNcTg1Sx6hz6JsBKD7273PKBPODb8T/weZQO7fwwqkTrZtewXk6d1bwADpohV/0Z2Q79+olshBU/0DHqNACafF7nrFGENu2LgS6sMLnqTJilSwZvG4AvCb8v14cNi8AE9LOAqFaEBACjWDHwk17+rDOg7Pr7vlgzkynbVS4GRx3b/5xuJ1Mmuo0Fsv30VWP2i+P3Hfad7x0akHFLwRM/AmQ989+/pHkX2I41tyQChnjvqAV+HvpKckcLeWnZ9rBi85qL3dQA8KPb5PeK2/MxzntKT+6IpeAC48i3RSay7Uccz8pju/3wjkZLsZFnj7q+Ag+uAsSenP1+ASDpk4AmCsI5NicF7NINQu1m41s1iuDYbcNpDQNW0zo8xJlrZBjxCQdqdenKas1AYe2bTVajatGbgDNEmdOgR5uPsPzmx79dV8rTfxOYAhh+VnjGoRCqT62gQPf3rt4rf/JDzun9sRMohA08QhHVUF71L67l+4FuxNVPwADDt4sjv58gXBl4qeLdm4PPLAV+7th68vfPr8gqB7/wpoa+QUqSCHzw7MxRxpCS7jgaxyl5ekWg9nAnhBCLpkIEnCMI6IRd9f72l7Kf3iW0iLnGHC/BAU/B54S1eW7QM/WwqbZQx+ExwzwORXfTtDSJn4szHhIJ3UHVJLpJFZw5BEGnH5gSgLSIT8InbTdXAMXcBIxLIwnbki/dwuISBb6sR+9Ue7rYsygXuMx448jZg+uXpHonAzEUf8AOeJuFx6T8pPeMiugUy8ARBWMfhErXodqf4u+BZ0Uwm0Yx1h0sYecY0BS9d9KqBz6LLlN0BHPerdI9CR1XwB9YBvUfrYRBaGjbnyaKpMUEQaWfOTcDpf9Xvjzula+Vojnx9xS27UzE+5fpzssnAZxryt9v2KfDYHFEWJzPoycDnPGTgo7B0Rz2CwfSu50sQGUX/ScCYE5L3fg6XXg9vzxNJdUC4gjdLsiOswZhw0+/4TNyv36rXwFPnupyHDHwENu5vwXl/+xIX/P0r7KxrS/dwCCI3URV82PrwWeqiz0TU7o1NexQDTwo+1yEDH4Ex/Ypx33cPwfq9zTjpz5/hleXV6R4SQeQeYQpeMUTZmmSXididQMUwoP8hovNgu3TRl6dzVEQ3QGdOBBhjOHfGYLx/21xMGVyGH720Cj9/bQ257AkimRx6OTD7e+K2uhCQunQpKfiuMf9O4Ky/CSPfrCj4SH0LiJyBDHwMBpQV4L9Xz8b35o7As0t24bFPt6Z7SASRO0w4Qxh5IIqCJwPfJWZ/DxhymOhT0LRHS7Jjol8/kdPQmWMBh92GO04eh71Nbvzp/Y2YNqQch4/sne5hEURuEabgKcku6ZRWAb42oGGncM9T6CPnof+wRRhjuPfsyRjRpxjX/HsZXlq2G5yTu54gkoadkuxSSqm2dO2BbynBrodABj4OilwO/Pfq2ThkUBl+/PJq/PZ/68Mer2314MWlZPgJIiFUF31YHTwp+KQgDXztJiqR6yGQgY+T/mX5eOaaw3DZnKF46vPteFnJrv+/9zbiJ6+sxopdDWkcIUFkKaqL3qWsH08GPjmUaQY+6CcF30MgA58AdhvDL0+dgDkjKnHna2vw7Z4mNLR58do3ewAgzOgTBGERaeAd+VqPeg1y0SeH4v5i+V2ADHwPgQx8gjjsNvz1ommoLMrD955ejsc/3QqPP4jpQ8rxv1X74PYF0j1Egsgu5IpmDld40xtKsksOdocw8gCVyPUQyMB3gcpiFx675FDUtHjwt0XbcMSoStx+wli0ePx4b+3+dA+PILILVcHb7PpKaKTgk4d005OC7xGQge8iUwaX4+4zJsLGgGuOGoHDRlRiYHkBXqBkO4KID9XAq1uKwSeP0iqxJQPfIyADnwQumDUE3/zyBMwf2xc2G8OVRwzDF1vr8OTi7ekeGkFkDzKL3mHoTU8GPnmUDhJbyqLvEZCBTxJlBXqJz1VHDMeJE/vh92+vx6JNNQAAzjnWVDfB46fYPEGYYldi8ICi4MlFnzRIwfcoyMCnAJuN4YHzpmJMvxLc9OwKbK9twxOLtuG0vy7GMf/3KTXJIQgzOrnoNUNPSXbJo2Ko2Bb3Se84iG6BDHyKKHI58PfLZsBuY7jwia/wh3c24OixfdC7xIUfv7w6VFJHEISGNPBOYwyeFHzSGHMycOELYmU5IuchA59CBvcqxCMXT0dNqwczh1Xg8UsOxas3HI6ZwyrwqwVrsauuHW+t3ocvt9ale6gEkX4iKXiKwScPuwMYexLAWLpHQnQDNDVOMYeP7I2PbpuH/mX5yHeKC9Wfzp2Kk/6yCPP/9AkCQQ6Xw4bXbzwC4weUxng3gshhQkl2xhg8GXiCSARS8N3AsN5FIeMOAEMqC3H/OVNw3Pi+ePjCaSgrcOLGZ1ag1eMPe91r31Rjxj0fYPpvP8APn/+mu4dNEN2L0bCHFDzpEIJIBDLwaeI7hwzA3y6dgdOmVOGhC6dhR10bbnnuG/gCQQCA1x/E/e9uRGmBE5MGluH1lXuxdm9TmkdNECkkYh08GXiCSAQy8BnAYSMqcfcZk/DxhoP44Qsr4Q8E8cqKauxtcuNXp03EwxdOQ2GeHU8t3hHxPV5Yugt3vLKasvOJ7CVSHTyjyxRBJAJNjTOESw4binavH79/ewO2HGhFi9uHKYPKMHd0bzDGcO6hg/Ds17swe0QvPP/1LvzguDGYN0aUunj9Qdz/3ibUtnowfUgFzps5OM3fhiASgOrgCSKp0NQ4g7hu7kg8dvF0uP0B7G1y45ZjR4Np2a5XHjEc/iDHT15ejRW7GnHP/9YhEBRq/f11+1Hb6kG/Uhd+9/Z61LR40vk1CCIxKIueIJIKGfgM4+TJA/DBrfPw1i1H4tjx/UL7h/Uuwi9PnYC7z5iIP58/FZsPtuLtNfsAAE9/uRODexXgv1fPRoc3gHveWpeu4RNE4lAdPEEkFTpzMpA8hw0Tq8o67b/yiOEAgGCQ45GFW/CXjzaj2e3Dku31uOPkcRjdrwTfmzcCD3+8BZfNGYpDh1K/aSKLkAb+hcVAYEZ4Fv3ChcDSpcBPfpK+8RFElkEKPgux2Rh+cNxobDnYijtf+xZ9S1w491CxiMT180aiX6kLv3lzHYJBSrgjsoj8MsBVBsycBZx3HrBmr9i/6HNxf+bM9I6PILIMlktZ1zNmzODLli1L9zC6Bc45VlU3odhlx8DyQhTk6XHK176pxq0vrMID503B2dMHpXGUBBEnfo9Q8p98Apx1GjDFB6wrAV58CZg/P92jI4iMhDG2nHM+w7ifFHyWwhjD1MHlGNW3JMy4A8AZUwZiYlUpHvpoM/xaXX0uTeSIHMbhEm1U588HLjsHWOQFrr+ejDtBJAAZ+BzEZmO4+ZjR2FHXjv+t3od/fr4dc/7wMTXKIbKHhQuB594C7roLePxv4j5BEHFBSXY5ygkT+mFsvxL8+s21aGz3wcaA6/6zHAtuOgKVxa50D48gIrNwoYi5v/iiUO7z54ffJwjCEqTgcxSbjeHmY0ehsd2HY8b1xUvXz0FNqweXPPk1/vPlDtS1Uq08kaEsXRpuzOfPF/eXLk3vuAgiy6AkuxyGc44l2+sxdXA58p12vPvtPtz7zgbsqGvH4F4FeOuWo1Ca70z3MAmCIIguQEl2PRDGGA4bURlaye6kSQPwyY/n49lrZmNvoxs/e2UNJd8RBEHkKGTgeyCHj+qN208Yi7fW7MOTi7enezgEQRBECqAkux7K9+aOwMrdDbjnrfWw21ioSx5BEASRG5CC76HYbAwPXzgdJ07sh9+8uQ5Pf7kj3UMiCIIgkggZ+B5MnsOGv140HceN74dfLliLBav2pntIBEEQRJIgA9/Dcdpt+OtF0zBzWC/c9sJK/On9jXD7AukeFkEQBNFFyMATyHfa8eTlM3DalCo8/PEWnPKXz7DlYGu6h0UQBEF0ATLwBACgJN+JB8+fiv9ePRtNHT6c/ejn+HjDActldL5AEFf+82t8tP5AikdKEARBWIEMPBHGkaN74/Ubj0C/0nxc9a9lOOORz/H19noAQCDIsWJXg+kytB+uO4CFG2vwt0+3dfeQCYIgCBPIwBOdGNyrEG/efCR+d9Yk1Ld5cck/luC1b6px1b+W4uxHv8CNz67oFKd/ZskuAMDXO+qxu749HcMmCIIgFMjAE6bkO+24ePZQ/O/mIzF+QAlufWEVPt9Si7OnDcS7a/fjor9/hVaPHwCwvbYNi7fU4oKZgwEAb6zck86hEwRBECADT8SgvDAPz1x7GK45cjieu+4wPHD+VDxy0XSsqm7CDf9djg5vAH//bBscNobbjh+DWcN64bVv9oRi9wdb3Ph2j/kytcEgj/gYQRAE0TVSbuAZYycxxjYyxrYwxu4weXwuY2wFY8zPGDvH8NjljLHN2t/lqR4rYU6xy4FfnDoBM4f1AgCcMnkA7j17Mj7bXIupd7+PZ5fswulTq9C3NB9nThuIrTVteGPlXlQ3tOPMv36Osx/9Agea3Z3e98Vlu3Hqw4vJyBMEQaSAlLaqZYzZATwC4HgA1QCWMsYWcM7XKU/bBeAKALcbXtsLwK8AzADAASzXXtuQyjET1jh3xmB0+AJYsr0eZ04diKPH9gEAnD61Ci8s240fvrASZQVOBDlHgHP847NtuPM7E8Le46Xl1QCATzYexKSBZd3+HQiCIHKZVCv4WQC2cM63cc69AJ4HcIb6BM75Ds75agBBw2tPBPAB57xeM+ofADgpxeMl4uCyOcPwyEXTcfyEfnDaxaFU7HLgpe/NwbVHDUe+04b/XDULp0+pwjNLdqGhzRt67fbaNizfKeZqizbXpmX8BEEQuUyqDfxAALuV+9XavqS9ljF2HWNsGWNsWU1NTcIDJZJHnsOGO78zAUt+fhymDanADUePRLs3gKc+11eue21FNWwMOHvaQKzY2RBK2CMIgiCSQ9Yn2XHOn+Ccz+Ccz+jTp0+6h0OYMKZfCU6bUoXHP92K1dWN8AeCePWbPThiVG+cc+gg+IMcS7bVARCJd88s2YlVuxvTO2iCIIgsJ9XLxe4BMFi5P0jbZ/W1Rxte+0lSRkV0O789YyJW7GzAjc+uQK8iF6obOvDzU8bj0GEVyHfaRMLe4HLc9uIqfLqpBn1LXPjgtnkoK3Cme+gEQRBZSaoV/FIAoxljwxljeQAuALDA4mvfA3ACY6yCMVYB4ARtH5GFlBfm4aELp2Ffoxs769rw8IXTcMrkAXA57DhsRCWe+3oXZv3+I3y5tQ43HD0SdW1e/O6tdbHfmCAIgjAlpQqec+5njN0EYZjtAJ7inK9ljN0NYBnnfAFjbCaA1wBUADiNMfYbzvlEznk9Y+y3EJMEALibc16fyvESqeXQoRV48+Yj0bfEhcpiV2j/xbOHotXtx+wRvXDG1IEY068EnAOPf7oVM4f1wrkzBkd5V4IgCMIMZnUxkWxgxowZfNmyZekeBpEE3L4ALn/qayzZXo/zZwzGb8+chDxH1qeMEARBJB3G2HLO+QzjfrpiEhlJvtOOZ66ZjZvmj8ILy3bjF6+vsbyyHUEQBJH6JDuCSBiH3YbbTxwLm43hoY82Y3TfElw7d0S6h0UQBJEVkIEnMp4fHjsaWw+24vfvrMeIPkU4dny/dA+JIAgi4yEXPZHx2GwM/3fuFEyqKsMtz32Dr7bVYdOBlk5L1qaT577ehR8+/026h0EQBBGCDDyRFRTk2fH3y2agON+BC574Cic8uAgn/+Uz7Kxr67Yx/HrBWvzh7fUIBjvnAryyvBqvr9yLpnZft42HIAgiGuSiJ7KG/mX5eO37R+CLrXUIBIO4950NOOvRL3D1kcMxZVA55oyshN3GEnrvn768GnNGVuLMaeadlH2BIJ5dsgveQBCN7T784ezJsGmf5fUHsUZbEe+b3Q04emzfxL4gQRBEEiEFT2QVVeUFOOfQQTh/5hC8+v0jMLC8APe/txGXPLkE333sC6zb2xzxtev2NsPj7+zW313fjheW7cazS3ZFfO2Wg63wBoKYPqQcLyzbjQc/3BR6bMP+Znj8Yq2kb3Y1Jv7lCIIgkggZeCJrGd67CG/efCRW/eoE/N+5U1Dd0I4zHlmMD9cdCHse5xwPfrAJpzz0GY7840L847NtYW72j9aL56+sboTbF8C+pg7c+MyKsNXv1moTh/vOmYJzDh2ERxZuCa2GJ41672IXVuzKzdWM/YEgDjS70z0MgiDigAw8kfWUFThxzqGD8OFt8zBhQCm+/8wKLNxwEIBwn//hnQ34y0eb8Z3JAzCmXzHueWs9XlymL1T40YaDsDHx3NXVTXj+6914a80+vLKiOvSctXubUOC0Y3jvIvzqtAkYUFaAH724Eu1eP77Z1YC+JS4cP6EfVu5uNI3RZzuvrKjGvPsXUo4BQWQRZOCJnKG8MA//vmoWRvYtxpX/WooLnvgSxz/4KZ5YtA0XzR6Chy+chv9ePRuzhvXCve9uQH2bFy1uH77aVodzDxXtcL/eXoe31+wDACxYtTf03mv3NmPcgBLYbQwl+U48cN4U7KhrxwPvb8I3uxsxbUg5pg8pR4vbj601rWn5/qlkR1073L4g1u5rSvdQCIKwCBl4IqcoL8zDi987DD87eRz2NHagMM+Bf181C78/SyTFMcZwz1mT0Or24643vsWCVXvhC3CcPX0gxvYrwUvLq7H5YCtG9y3G6uombKtpRTDIsX5vMyZWlYY+Z/aISlw0ewie+nw7dta1Y9qQCkwfWgEgN+Pwda0eAIia40AQRGZBBp7IOUrynfjevJH47CfH4J0fHIV5Y/qEPT6mXwm+P38U3lq9D3e+9i3KCpw4dGgFZg3vhZ117WAMePD8qWAMeGPlXlQ3dKDF48fEqrKw9/npSeNCi+ZMG1yO4ZVFKCtw4pmvd+HNVXvR5vF323dONXWtIh9h/b6WNI+EIAirUJkc0SO59bjROGlif7y1Zi9G9imGw27DrOG98PRXOzFrWC9MGliGOSMq8do3ezCgLB8AwhQ8IGL/f/zuZDyycCumDC6HzcZw5RHD8ORn23Hzc9+gd7ELN84fCYeNwe0L4tI5Q5HvtKfj63aZWi3hcN0+UvAEkS2QgSd6JIwxTKgqxYQwt3svuBw2fHf6IADA1UcOx/eeXo47Xl0Du41hTL+STu9zzLh+OGac3jr3h8eNwU3zR2HpjgY88MFG/OZNfU37hRsP4h+Xz0BhXvaddtJFv+VgC7z+IK3sRxBZQPZdaQgiRfQtyceSnx+LsgInAODY8f3w+o1H4PaXVqFXUZ5l9e2w2zBnZCVeHDEH6/e1oKLIiS+21OHHL6/CVf9ain9fNQsuR3Yp+bpWL/qVunCg2YMtB1vDJkYEQWQmNA0nCIXywjwwpnfDmzSwDO/+cC7+e/XsuN9LegkGlBXgu4cOwp/Om4KvttXjjleya+nbdq8fHb4AjhwlchnITU8Q2QEZeIKwgC3BFrgqZ00bhB8dPwavfbMH9723sVO9/IFmN3704iqc/JfPUNPiSfhzdta1JXUCIRPsZg6rQL7ThvVpNvAfrT+A2tbEfx+C6CmQgSeIbuSmY0bhgpmD8dgnW3HVv5diT2MHgkGOpxZvx/z/+wRvrtqLrTWt+MHz3yCQQMOc9fuaMe/+T3D/exuTNuY6LcGub6kLY/uVpLVUrq7Vg6v/vQx/+3Rr2sbQVbLJe0NkN2TgCaIbYYzhD2dPxm/PnIQvttThiHs/xpx7P8Ld/1uHWcN74cPb5uGeMyfhi611uPO1NXEr1c+31AIAHv1ka1ijnq4gE+x6FbkwbUgFlu9qSJuClov6yDbB2capD3+GRz/J3skJkV1Qkh1BdDOMMVx62FDMG90H/1uzF0u31+PHJ1bhu9MHgjGGIZWF2LCvBU99vh2vrtiDuWN6Y87I3jh3xiCU5jujvvfX2+sxqKIAA8ry8ZOXV2F8/xKMNsn+jwfpoq8sysMlhw3Fv77Ygae/3Ilbjx/TpfdNhDXVwsB/u6cZbl8gq8oOOedYv68FfbTeCQSRakjBE0SaGFJZiO8fPQr/vHIWzjl0UFhy3y9Pm4CPfjQPF80egi0HW/Hb/63DJf9YgmZ35F7wwSDH0h31mDOiEo9cPB0FTjtuf2kV/IFgl8ZZ2ybUemVxHkb1LcZx4/vi6a92osPbeWW+VLNaU/DeQBBr92ZX29xmtx+BIMfOuvZ0D4XoIZCBJ4gMZWSfYvz69In45Mfz8Y/LZmD9vmZc9uTX+PcXO/D+2v1w+8IN7JaaVjS0+zBreC/0LcnHb8+chFXVTfjVgrV4YekubDmYWI/8ulYvCvPsofr9a48agfo2b9hiPN3FmuomHDGqEkD2uenl6oS7G9q7POnKFLz+IJbuqE/3MIgIkIEniCzguAn98NAF07Bxfwt+tWAtrnt6OWb+7kP85s21oYz7JdvqAACzhwsDeOohVThzahWeWbILP31lDc5+9HPsro9fPda1elBZnBe6P2t4L0wZVIYnF2/v1pXzDra4sb/Zjflj+2JoZWHWGfj6dmHgfQGOfU25sfTuW2v24tzHv0zouCJSDxl4gsgSTp48AKt/fQKW/eI4PHPNbBw7ri/+8+VOzLt/If7vvY34ZGMN+pfmY3CvgtBrHjx/Kj65/Wi8ceMR4By46blv4PXHpx7r2ryoLNLjxowxXDt3BLbXtuHD9QeS9v1i8a3mnj9kUDmmD6nAil2NWZWRLhU8AOyoa0vjSJLHgWYxuaSwQ2ZCBp4gsgin3YbexS4cMao3/nzBNHx42zwcO74f/rpwCz7acBAzh/cKi+UzxjCsdxGmDC7HH885BKt2N+Lyp77Gmuom+ANBSwayrtWL3oqCB4CTJvbHwPIC/P2zbUn/ji1uHx7+aHOnGP/q6iYwJtYEmD60AjUtHlQ3dCT981NFfZiBzw2D2KB5JfY05sb3yTXIwBNEFjO8dxEevnAa/nfzkTh7+kBcNmdoxOeeMnkA7jlzEtbvb8Zpf12MUXe+g7G/eBeX/GMJ/vPljk4xfUldmydMwQOiHe/VRw7H0h0NSXeVP/rJVvzpg014Y+We0D5/IIivt9djZJ9iFLkcOGx4LwDAe2v3J/WzU0lju0iQtNsYdtbmhoJv0r5TNk20ehJk4AkiB5g0sAwPnDcVM4f1ivq8Sw4bikU/mY/fnD4Rtx43BhcfNgT7m9345RtrMe/+hWFGFRClXXWtXvQyKHgAOH/mYPQuzsMtz32D6obEFBznPCyOX9vqwb+/2AFALNULACt2NeCkv3yGL7bW4fgJYmGf0f1KMH1IOf771c5uzQPoCvXtXjjtDCP7FOWegicDn5GQgSeIHkZpvhOXHz4MPzhuNH512kR8eNs8PH/dYRhQVoAfPL8Sjytd4po7/PAHOSqLOhv4IpcD/7pyFprdPlz8jyXYVhN/lv5Nz32Di/7xVchIP7FoG9y+AE6bUoWvttdhZ10bbnpmBTq8ATx+yXT85MSxoddefvgw7Khrx6ebahL4FbqfhjYvygvzMKyyCDtzJAbfSAo+oyEDTxAEDhtRiZeun4PTplTh3nc24JdvfIsObyBUA987QnOWSQPL8J+rZqGx3YeT//IZ/vbpVsslYIs21eCt1fvw1bZ6vLl6L7YcbMF/vtyBM6cNxA+OHQ3OgSv/uRR7m9x44LwpOGnSgLD8gpMnDUCfEhf+pSl+ALjr9W/xmzfXZmTyXX2bF70K8zCsdxF21rdnjechGk0dwsDvaSQDn4lQJzuCIACIBL4/nz8VfUtceHLxdnyysQYubd33SAYeAKYNqcAHt87Fna9/iz+8swFvr9mHP55zCMb1F0vKNrR5UZLvgMOu64lAkOP3b6/H4F4FKHE5cf97G5HnsKHY5cCPTxyLAWUFmDCgFOv2NeOUyf0xe0Rlp8/Nc9hwyeyhePDDTVi8uRbeQABPf7UTADCpqgzfPXRQMn+eLtPQ7kVFkRNDKwvh9Qexv9mNqvKC2C/MYKSLfl9TB3yBIJx20oyZBBl4giBC2G0Md506AceO74s/vrsRpfkOnDltIGYOr4j6ur6l+Xji0kPxv9X78KsFa3HKXz7DqYdUwW5jWLBqL44a3Rt/v2xGyAC8sHQ3NuxvwSMXTUdpgQOXPvk1HDaGZ66ZjQFlwuidN2MQ7n9vI3528viIn3vNUcPx1pq9uPm5FShyOTCiTxF6F7vwyze+xfShFRjeuyh5P04Emt0+2BlDkSv65bS+zYux/UswrFKMaUddW0oN/MEWNzgH+pXmp+wzGtt9KC90orHdh/1NbgzuVZiyz+oK/kAQAc7hcmRPa+NkQNMtgiA6cfjI3njjxiPw9NWzceP8UZYujIwxnDalCh/eNg/Xzh2BD9cfwHtr9+OECf3wycYa/PSV1eCc42CLG/e+sx6zhvfCKZP746jRfXDT/FH403lTwpT6ZXOGYcmdx0U1GkUuBx6/5FD4AxzVDR24+/RJ+PP5U+Gw2/Ddx77AJxsPJuX3iMZ1/1mGcx7/Eh5/9Na9je0+VBTmhSYdG/e3pHRct76wEpc/9XXKwhUd3gA8/iAmVglPTSa76X/6yhpc9uTX6R5Gt0MKniCIpNKrKA8/O3k8bj5mNBiEEX74o8340webUNPiQZ7dBrc/iD+cPTkUU79dSZ6T2GwMxTFUMQCM6FOMf145E+v3t+DI0b0BAK9+/3Dc+MwKXPHPpbhx/kjcetyYsBBBsggGOVbtbkKHL4A/vb8JPz/F3NsQDHI0tHvRqygPA8ryMaJ3ET7ecBBXHjE86WMCRHXCmuomNLv9WF3dhCmDy5P+GY0dwj0/saoMn2+py+hEu601rVi5uxG769sz1suQCkjBEwSREopdjpDb+qZjRuG3Z0zEip0N+GjDQdw8fxRG9ilO2mfNGNYLlx6m9wAY2acYr994BC6YORiPLNyKi/6+JCWZ63saO9DhC6CqLB9//2wbvtxaZ/q8ZrcPQQ5UFOaBMYbjJ/bDl1vrQnXkyeZgiwfNbj8A4OXlqVkzQGbQTxigKfgMNvAyV+Cdb/eleSTdCxl4giBSDmMMl84Zhvdvm4ffnjkJ35s3MuWfme+0497vHoIHz5+CdfuaccKDi/CXDzfjYEvy+sBLN/t950zBgNJ8/PnDTabPk13semnlhidO7A9/kGNhikIImw6IcQ2qKMCCVXsjNjHqCtJo9i11oW+JK+FeCN1Bvbbk8VtrsqcxUjIgA08QRLcxsLwAlx42FHmO7rv0nDVtED68bR6OHtsHD364CXP+8DFuee6bpBj6jZohnTK4DJcfPgxLttdj3d7mTs+TxrC80AkAmDqoHH1LXCnrxLfpgOhJcPsJY9HU4UvJmgHS+1BekIeBFQURY/D7mjpw24sroy51nEq8/iBaPH5UFDqxandjRk9Ekg0ZeIIgcp7+Zfn426Uz8OFt83D1kcPx7tr9OP6BRfjT+xvx1bY6+Exq9wNBjkufXIKL//GVqdEGgM0HWlBVlo+SfCcumDkEBU47/vXF9k7Pq28Txk0qeJuN4Xgt+dDYcz8ePlh3AFf9aymm//YDfL1dX7Z184EW9CrKw2lTqjCoogD//HxH0pPtGjQDX1HkxKCKwogx+PfXHsCrK/bgtRV7TB9PNY3a5Oq8GYMBAO/0IBVPBp4giB7DqL7F+Pkp4/HOD47CIYPK8MjCLbjgia8w63cf4mevrsGBZl3VP/3lDny2uRYrdzXi1Ic/wysmseyNB1oxpn8JAKCs0Imzpg/E6yv3orbVE/Y8uZJcRaHeEfC0KVXo8AVw3dPL0Orxx/1dmjp8+N7Ty7BhXzN8gSCeWKQv/LPpQAtG9y2G3cbwvbkjsHxnA77altx122WSXXlBHiZWlWJXfTvW7+s8EdqwX+xLVS5ALOq0337K4HKM7FOEJdvN8yRyETLwBEH0OEb2KcbTV8/GN788AY9fMh1zx/TBqyuqcfwDn+K/X+3Esh31+L/3N2HumD744o5jMWNoL/zmzbVhbn1/IIitB1sxpl9JaN9VRwxDMMhx1b+W4mCLGy8u241HFm4JrQXfS2n5e9iIStx/ziH4Ymsdzn38y7jL5lbsakCQA/933hRcNmcoPt5wANUN7eCcY/MBfVznzhiM3sUuPLJwS1d+sk40tfuQ57Ah32nDBTMHo8Bpx98XdV5dcN2+FjAGrNnTFDL23UmDkv8waWAZ1u9LbXliJkEGniCIHktZgRMnTRqAv1wwDe/+cC5G9yvBL17/Fuc8/iW8gSB+e8ZElBU68YfvTobbF8Q9/1sfeu3O+nZ4A8EwAz+qbwn+dumh2Li/BXP+8DF+8vJq3P/eRry4bDfyHDYU5oX3Ezh3xmA8dcVMHGx249SHP8NDH202DReYsXxHA+w2hqmDy3HhrCEAgOe+3oX9zW60ePwY009UKeQ77bj2qOFYvKUWX21LnnptaPeiotAJxhjKC/NwwazBWLBqb1gsPhDk2LS/BWdMqYLTzky9IKlGnVxNGFCKPY0dIbd9rkMGniAIAmLp3Ze+Nwf/u/lIPHDeFPz7ylkYqnWdG9mnGDccPRILVu3Fj19ahd317dikKe6xioEHgGPH98PTV8/GseP64olLD8WUweXYVtOGXlqJnJF5Y/rg/Vvn4uRJA/DAB5tw+l8/x4pdsZfgXbazHhOrSlGY58CgikIcM64fnv96NxZuEIvvqBOPiw8biiG9CnH9f5dj84HkKNjGdh/KC3SPxDVHjQAHwlT8zro2dPgCOHxUbxwzri9eXbGn241rvRIemaA15VlnEkrIRcjAEwRBaNhsDJMGluHs6YMwZ2R4//vvzx+Ja44cjjdW7cXc+xfizte/BWMirm9k1vBeeOKyGThhYn/84azJsNsYKkxW5JNUFrvw0IXT8MSlh6K21YOzH/0Clz31NZ5cvB0fbzjQaQEfXyCIlbsbcehQvYXwDUePRJvXj5+/tgZAuIEvdjnw36tnI89uwyVPLsG3e5oS+n1UZJtaycDyApw9bSCeXbILu+tFpvoGbRI0vn8pbpw/Cs1uH25/aXW3LgYkDXx5oRPjtZr9SEmTuQYZeIIgCAu4HHb84tQJWPTj+bjtuDEY3rsIJ0/qj4K86G18J1SV4u4zJoY14onECRP7Y+HtR+OnJ43Dur1N+O3/1uGqfy3D2Y99ERajX7u3GW5fEDOG9grtO3RoBT7+0dE4Y2oVjhnXt9OEYkhlIZ6+ejYYGM5+7Av858sdlsMBZjR2eMMMPAD86ISxsNmAP767AQCwYV8zbAwY3a8Yhwwqx89PGY8P1x8ISwhMNQ1tXpQVOOG029C7WNTs9xQFT61qCYIg4qB/WT5uPnY0bj52tOXXXDw7tnGXFLscuOHokbh+3gg0tPvw2eYa/ObNdTjloc9wxpQqXHPUCCzbITLiZwwLXwSoqrwAf7lgWsT3Htu/BG/dciRufXEVfvnGWjy6cCuOHN0bHd4ATptShZMm9bc8TtlbX6V/WT6umzsSD320GVceUY91+1owok8x8p1iEnTF4cOwdEc97n13A4b1LsKJE61/XqLUtXnDkhsnVJWSgicIgiDSB2MMvYrycMbUgXj/1rm4fM4wvLt2P0556DPc/95GDKooSGiluMpiF/51xUz884qZGN2vGJ9trsGS7fW48dkVWLSpxtJ7cM7R2O5DmUHBA8D35o7AgLJ8XPuf5Vi+sx7j+uuhAsYY/nTuVEwZVI5bnvsmrHY/VchkQMmEAaXYcrA15uJAuQAZeIIgiAynd7ELvzxtAr644xjcfcZETBmkZ84ngs3GMH9cXzx99Wws+flxWHj7PIzuW4zvP7MCzyzZiZW7G3Hfuxtw4zMrTEvbOnwBeAPBsCQ7SZHLgWevPQwFTjsa2n2huLekIM+OJy+fgaryAlz096/w2CdbEQimLiZf3+ZDryJX6P6EqlL4g6KUMNchFz1BEESWUF6Yh8vmDMNlc4Yl9X1L8p146oqZuOypr3Hna98CAGwMKMpz4P11+3HzMaNx4/xRsNtEFYBcaKbCRMEDoiLh5Rvm4IH3N+H0KVWdHq8sduH17x+Bn7+2Bn98dwOe/XonLj1sKM6bMRjlhZGTEROhoc2LyQP1SYZcHGfR5hpMGliW1M/KNMjAEwRBEKgqL8AHt87FloOt+HZvE+aM6I08hw2/XrAWD3ywCYu31OKaI4ejttWL55fuCr0mEgPKCnD/uVMiPl5W6MRfL5qG09YOwFOf78Dv396AP72/CWdMrcJlc4YlxfhyzlHf5g1LOBzeuwhHj+2DP3+wGUeM7J2SpXQzBdad5QqpZsaMGXzZsmXpHgZBEERO8cryatz1xrdo1/rmD6ssxLVzR+CiWUNMa/sTYf2+Zvzny514/Zs96PAFcNiIXvjVaRNRVVaAjzcegI0xDKoowLTBFbDZrH1mq8ePSb96Dz87eVzYCoYNbV6c+vBiAMBr3z8cfRPIZQCADm8A+U5b0n6DRGGMLeecz+i0nww8QRAEEYuDLW7sb3KjNN+Jwb0KQ+76ZNPU4cNLWovfZrcfdsbgVcr5xvUvwU9OGov5Y/vGNKy769tx1H0Lcf85h+BcbbEZyerqRlzwxFeoLM7Df6+eHWpqZJVv9zThwr9/hYtnD8UdJ4+L67XJhgw8QRAEkTU0tnvx2Cdb4Q9ynDalCsUuB1btbsRDH2/Gzrp2HDe+H359+gQMqiiM+B4rdzfizEc+x5OXz8Cx4/uZPn7FP7+GjTF8/+iRuHj20Jh9DQBgV107zn7sC9S2epDvtOHznx6DymJXzNelCjLwBEEQRNbjCwTx1OLtePDDTfD6gzh0aAWOG98Px0/oh+G9i8JU/cINB3Hlv5bi1e8fjulDKkzfb8vBFvzyjbX4Ymsd7DaGqvJ8fGdyFW49fjRcjs7G/sN1B/Cz19bAFwji3rMPwQ3PLMdN80fhRyeMTdl3jgUZeIIgCCJnqG5ox4vLqvHhugOhznRFeXYMKC/A3NF9cNa0gdh0oAU/emkVPrn9aAzrHd0F//X2eizaVIMN+5vx4fqDmFhVit+eOSk0MWjq8OHuN9fhlRXVGNe/BH++YCrG9S/F955ehi+31uHzO45BSb55VUGqIQNPEARB5CR7Gjvw8YaD2F7Thu21rfh8Sx28gSB6F+ehttWL1b8+AaVxGN/31+7HT19ZjYZ2HyZWlaJ/aT7W7GlCXZsXNx49EjcdMxp5DtFGZnV1I8545HNMqirDXy+ahqGVRWho8+L7z6zAwRY3bjpmFE6fMjBlOQsAGXiCIAiihyAT9f62aBsCQY7lvzgu7kz3No8fr6yoxoKVe+H2B1BekIefnjQOkwd1Lt97f+1+3P7SKvgCHPPG9MHGAy3Y09iBob0KsflgK+aN6YO/XXpoqGVvsiEDTxAEQfQo3L4A2jz+bkmAq25ox6OfbMXH6w8iwDkevXg6Dh1SgWeW7MQvF6zFUaP74C/nT426qmCikIEnCIIgiBQjbarqMXhx6W785JXVsNsYZg6rwC9PnRhamz4ZRDLw1MmOIAiCIJKEWSjgvJmDMWlgGd5esw/vr9uPiqLuScYjA08QBEEQKWZCVSkmVJXi9hO7r5yOVpMjCIIgiByEDDxBEARB5CBk4AmCIAgiByEDTxAEQRA5CBl4giAIgshByMATBEEQRA6ScgPPGDuJMbaRMbaFMXaHyeMuxtgL2uNLGGPDtP3DGGMdjLGV2t/jqR4rQRAEQeQKKa2DZ4zZATwC4HgA1QCWMsYWcM7XKU+7GkAD53wUY+wCAH8EcL722FbO+dRUjpEgCIIgcpFUK/hZALZwzrdxzr0AngdwhuE5ZwD4t3b7ZQDHsnhXBSAIgiAIIoxUG/iBAHYr96u1fabP4Zz7ATQBqNQeG84Y+4Yx9ilj7KgUj5UgCIIgcoZMblW7D8AQznkdY+xQAK8zxiZyzpvVJzHGrgNwHQAMGTIkDcMkCIIgiMwj1Qp+D4DByv1B2j7T5zDGHADKANRxzj2c8zoA4JwvB7AVwBjjB3DOn+Ccz+Ccz+jTp08KvgJBEARBZB+pNvBLAYxmjA1njOUBuADAAsNzFgC4XLt9DoCPOeecMdZHS9IDY2wEgNEAtqV4vARBEASRE6TURc859zPGbgLwHgA7gKc452sZY3cDWMY5XwDgSQBPM8a2AKiHmAQAwFwAdzPGfACCAK7nnNencrwEQRAEkSswuTh9LjBjxgy+bNmydA+DIAiCILoNxthyzvkM437qZEcQBEEQOQgZeIIgCILIQcjAEwRBEEQOklMxeMZYDYCdSX7b3gBqk/yeRGfod+4+6LfuHuh37j56+m89lHPeqU48pwx8KmCMLTNLXiCSC/3O3Qf91t0D/c7dB/3W5pCLniAIgiByEDLwBEEQBJGDkIGPzRPpHkAPgX7n7oN+6+6Bfufug35rEygGTxAEQRA5CCl4giAIgshByMBHgDF2EmNsI2NsC2PsjnSPJ9dgjO1gjK1hjK1kjC3T9vVijH3AGNusbSvSPc5shDH2FPv/9u4u1IoqDOP4/0ktoqKiQMQMpc6NUh0lIirCuujDLiyCMqIkgj6wMojQuqmLLiLoA8uCotLIkqC0LsQKkwr6UArTVIIoIcWyLrSksDo9Xcw6ONjebKztmc74/OCwZ9bsM7zzss5+z6yZ2UvaJenLWlvH3KqyqPTzjZJmNBf56NIlzw9K2lH69QZJs2rb7it5/krSpc1EPfpImiRpraQtkjZLml/a06d7SIHvoMxitxi4HJgKXCdparNRtdJFtgdrj7csBNbYHgDWlPU4eEuAyw5o65bby6lmahwAbgGeGaEY22AJ/8wzwOOlXw/aXgVQPj/mANPK7zw9PFtm9PQncI/tqcC5wLySz/TpHlLgOzsH+Nr2N7Z/B5YDsxuO6XAwG1halpcCVzYXyuhl+wOqmRnruuV2NvCSK58AJ0iaMCKBjnJd8tzNbGC57X22vwW+pvqciR5s77T9eVn+BdgKTCR9uqcU+M4mAt/V1reXtugfA+9I+kzSLaVtvO2dZfl7YHwzobVSt9ymr/ffHWVo+IXaZabkuQ8kTQamA5+SPt1TCnw05QLbM6iG0+ZJurC+0dXjHXnE4xBIbg+pZ4DTgEFgJ/Boo9G0iKRjgdeBu23/XN+WPt1ZCnxnO4BJtfVTSlv0ie0d5XUXsIJquPKH4aG08rqruQhbp1tu09f7yPYPtods/wU8x/5h+OT5P5A0jqq4L7P9RmlOn+4hBb6z9cCApCmSjqS6OeathmNqDUnHSDpueBm4BPiSKsdzy9vmAm82E2ErdcvtW8CN5c7jc4E9tWHPOEgHXOu9iqpfQ5XnOZKOkjSF6gawdSMd32gkScDzwFbbj9U2pU/3MLbpAP6PbP8p6Q7gbWAM8ILtzQ2H1SbjgRXV3y1jgVdsr5a0HnhN0s1UswJe02CMo5akV4GZwMmStgMPAA/TObergFlUN339Ctw04gGPUl3yPFPSINVw8TbgVgDbmyW9Bmyhuit8nu2hBsIejc4HbgA2SdpQ2u4nfbqnfJNdREREC2WIPiIiooVS4CMiIlooBT4iIqKFUuAjIiJaKAU+IiKihVLgIwJJQ7UZ0Db0cwZFSZPrM65FxMjIc/ARAfCb7cGmg4iI/skZfER0JWmbpEckbZK0TtLppX2ypPfKpCprJJ1a2sdLWiHpi/JzXtnVGEnPlfm835F0dHn/XWWe742Sljd0mBGtlAIfEQBHHzBEf21t2x7bZwBPAU+UtieBpbbPBJYBi0r7IuB922cBM4Dhb4AcABbbngbsBq4u7QuB6WU/tx2aQ4s4POWb7CICSXttH9uhfRtwse1vyoQf39s+SdJPwATbf5T2nbZPlvQjcIrtfbV9TAbetT1Q1hcA42w/JGk1sBdYCay0vfcQH2rEYSNn8BHRi7ssH4x9teUh9t//cwWwmOpsf72k3BcU0Scp8BHRy7W114/L8kdUsywCXA98WJbXALcDSBoj6fhuO5V0BDDJ9lpgAXA88I9RhIj4d/LfckRAuQZfW19te/hRuRMlbaQ6C7+utN0JvCjpXuBH9s/YNR94tszwNURV7LtN1TkGeLn8EyBgke3dfTqeiMNersFHRFflGvzZtn9qOpaIODgZoo+IiGihnMFHRES0UM7gIyIiWigFPiIiooVS4CMiIlooBT4iIqKFUuAjIiJaKAU+IiKihf4G2mwPchaO8ZgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Learning curve\")\n",
    "plt.plot(results.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(results.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot( np.argmin(results.history[\"val_loss\"]), np.min(results.history[\"val_loss\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"log_loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prediction Steps.. 35\n",
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "path_testing_input = \"/media/beta/navchetan-beps/Lars_BEP/Test sets/Input\"\n",
    "path_testing_output = \"/media/beta/navchetan-beps/Lars_BEP/Test sets/Inner\"\n",
    "path = \"/media/beta/navchetan-beps/Lars_BEP/Test sets\"\n",
    "os.chdir(path)\n",
    "indlist_testing_input = list(pd.read_csv('Testing_indices.csv')['Indices'].values)\n",
    "# indlist_validation_output = list(pd.read_csv('Training_indices.csv')['Indices'].values)\n",
    "csvFiletesting_Input = 'Testing_Input.csv'\n",
    "df_testing_input = pd.read_csv(csvFiletesting_Input)\n",
    "csvFiletesting_Output = 'Test_Output.csv'\n",
    "df_testing_output = pd.read_csv(csvFiletesting_Output)\n",
    "BS = 8\n",
    "\n",
    "N_PRED=len(indlist_testing_input)\n",
    "pred_steps = math.ceil(N_PRED/batch_size)\n",
    "    \n",
    "\n",
    "\n",
    "pred_X_generator = CustomImgGenSC(path_testing_input,path_testing_output,indlist_testing_input\\\n",
    "                                 ,df_testing_input,df_testing_output\\\n",
    "                                 ,H=256,W=256,onlyX=True,shuffle=False,BATCH_SIZE=BS)\n",
    "    \n",
    "# path = \"/data/navchetan/Data/Testing\"\n",
    "# os.chdir(path)\n",
    "\n",
    "# Create and save result matrix batch by batch\n",
    "a = 0\n",
    "print('Running prediction Steps.. ' + str(pred_steps))\n",
    "count = 0\n",
    "for step in range(pred_steps): \n",
    "    pred_Y_predict = model.predict_generator(pred_X_generator,steps=1,use_multiprocessing=False,workers=0)\n",
    "    print(step,end=\" \")\n",
    "    b = np.shape(pred_Y_predict)[0]\n",
    "    short_lst = indlist_testing_input[a:a+b]\n",
    "    a+=b\n",
    "    pos = 0\n",
    "    path = \"/media/beta/navchetan-beps/Lars_BEP/Test_outputs/MiniNet/Results_inner\"\n",
    "    os.chdir(path)\n",
    "    for i in short_lst:\n",
    "        S = pred_Y_predict[pos,:,:,0]\n",
    "        pos+=1\n",
    "        count = count + 1\n",
    "        fsave = 'Segment_Output'+str(count).zfill(5)+'.mat'\n",
    "        sio.savemat(fsave, {'S':S})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Testing Data for computation of Jaccard Index\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "0.8049075835391065\n",
      "0.9915166910310959\n",
      "0.8233714838431223\n",
      "0.9811339042556118\n",
      "0.7142883598856707\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "cols = ['sensitivity','specificity','accuracy','dice_score','Jaccard']\n",
    "df = pd.DataFrame(columns=cols)\n",
    "path_segment =\"/media/beta/navchetan-beps/Lars_BEP/Test_outputs/MiniNet/Results_inner\"\n",
    "path_testing_ground_truth =\"/media/beta/navchetan-beps/Lars_BEP/Test sets/Inner\"\n",
    "N_TESTING_SAMPLES = 213\n",
    "print('Loading Testing Data for computation of Jaccard Index')\n",
    "sensitivity_t = 0\n",
    "specificity_t = 0\n",
    "accuracy_t = 0\n",
    "dice_score_t = 0\n",
    "Jaccard_t = 0\n",
    "for i in range(N_TESTING_SAMPLES):\n",
    "    if(i%10 == 0):\n",
    "        print(i)\n",
    "    os.chdir(path_segment)    \n",
    "    pathr = 'Segment_Output'+str(i+1).zfill(5)+'.mat'\n",
    "    x = sio.loadmat(pathr)\n",
    "    Segment_model = x['S']\n",
    "    Segment_model1 = Segment_model*255;\n",
    "    retval1, Segment_model_threshold = cv2.threshold(Segment_model1,100,255,cv2.THRESH_BINARY)\n",
    "    Segment_model_threshold1 = Segment_model_threshold/255.0\n",
    "    \n",
    "    os.chdir(path_testing_ground_truth)\n",
    "    path_t = 'Segment'+str(i+1).zfill(5)+'.mat'\n",
    "    y = sio.loadmat(path_t)\n",
    "    ground_truth_testing  = y['S']\n",
    "    ground_truth1 = ground_truth_testing*255;\n",
    "    retval2, ground_truth_testing_threshold = cv2.threshold(ground_truth1,0,255,cv2.THRESH_BINARY)\n",
    "    ground_truth_testing_threshold1 = ground_truth_testing_threshold/255.0\n",
    "\n",
    "    mcm = confusion_matrix(np.ndarray.flatten(Segment_model_threshold1),np.ndarray.flatten(ground_truth_testing_threshold1))\n",
    "    tn = mcm[0, 0]\n",
    "    fp = mcm[0, 1]\n",
    "    fn = mcm[1, 0]\n",
    "    tp = mcm[1, 1]\n",
    "    \n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    accuracy = (tp + tn) / (tp + tn + fn + fp)\n",
    "    dice_score = 2*tp / (2*tp + fp + fn)\n",
    "    Jaccard = dice_score / (2-dice_score)\n",
    "    \n",
    "    sensitivity_t = sensitivity_t + sensitivity\n",
    "    specificity_t = specificity_t + specificity\n",
    "    accuracy_t = accuracy_t + accuracy\n",
    "    dice_score_t = dice_score_t + dice_score\n",
    "    Jaccard_t = Jaccard_t + Jaccard\n",
    "    \n",
    "    \n",
    "    df = df.append(pd.Series([sensitivity,specificity,accuracy,dice_score,Jaccard],index=df.columns), ignore_index=True)\n",
    "\n",
    "path = \"/media/beta/navchetan-beps/Lars_BEP/Test_outputs/MiniNet\"\n",
    "os.chdir(path)\n",
    "df.to_csv('Metrics_Mininet_inner.csv',index=False)\n",
    "print(sensitivity_t/N_TESTING_SAMPLES)\n",
    "print(specificity_t/N_TESTING_SAMPLES)\n",
    "print(dice_score_t/N_TESTING_SAMPLES)\n",
    "print(accuracy_t/N_TESTING_SAMPLES)\n",
    "print(Jaccard_t/N_TESTING_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n"
     ]
    }
   ],
   "source": [
    "O1 = np.zeros((N_TESTING_SAMPLES,256,256))\n",
    "O2 = np.zeros((N_TESTING_SAMPLES,256,256))\n",
    "Ground_Truth = np.zeros((N_TESTING_SAMPLES,256,256))\n",
    "Segmentation_from_model = np.zeros((N_TESTING_SAMPLES,256,256))\n",
    "\n",
    "path = \"/media/beta/navchetan-beps/Lars_BEP/Test sets/Input\"\n",
    "os.chdir(path)\n",
    "for i in range(N_TESTING_SAMPLES):\n",
    "    if(i%10 == 0):\n",
    "        print(i)\n",
    "    pathr = 'Image'+str(i+1).zfill(5)+'.mat'\n",
    "    x = sio.loadmat(pathr)\n",
    "    O1[i,:,:] = x['U']/255\n",
    "    O2[i,:,:] = x['U']/255\n",
    "    \n",
    "path = \"/media/beta/navchetan-beps/Lars_BEP/Test sets/Inner\" #change directory\n",
    "os.chdir(path)\n",
    "for i in range(N_TESTING_SAMPLES):\n",
    "    if(i%10 == 0):\n",
    "        print(i)\n",
    "    pathr = 'Segment'+str(i+1).zfill(5)+'.mat'\n",
    "    x = sio.loadmat(pathr)\n",
    "    Ground_Truth[i,:,:] = x['S']\n",
    "    retval2, ground_truth_testing_threshold = cv2.threshold(Ground_Truth[i,:,:],0,255,cv2.THRESH_BINARY)\n",
    "    Ground_Truth[i,:,:] = ground_truth_testing_threshold/255.0\n",
    "    \n",
    "path = \"/media/beta/navchetan-beps/Lars_BEP/Test_outputs/MiniNet/Results_inner\"\n",
    "os.chdir(path)\n",
    "for i in range(N_TESTING_SAMPLES):\n",
    "    if(i%10 == 0):\n",
    "        print(i)\n",
    "    pathr = 'Segment_Output'+str(i+1).zfill(5)+'.mat'\n",
    "    x = sio.loadmat(pathr)\n",
    "    Segmentation_from_model[i,:,:] = x['S']\n",
    "    Segment_model1 = Segmentation_from_model[i,:,:]*255;\n",
    "    retval1, Segment_model_threshold = cv2.threshold(Segment_model1,100,255,cv2.THRESH_BINARY)\n",
    "    Segmentation_from_model[i,:,:] = Segment_model_threshold/255.0\n",
    " \n",
    "def concat_images(imga, imgb):\n",
    "    \"\"\"\n",
    "    Combines two color image ndarrays side-by-side.\n",
    "    \"\"\"\n",
    "    [ha,wa] = np.shape(imga)\n",
    "    [hb,wb] = np.shape(imgb)\n",
    "    max_height = np.max([ha, hb])\n",
    "    total_width = wa+wb\n",
    "    new_img = np.zeros(shape=(max_height, total_width))\n",
    "    new_img[:ha,:wa]=imga\n",
    "    new_img[:hb,wa:wa+wb]=imgb\n",
    "    return new_img\n",
    "\n",
    "\n",
    "h = 256\n",
    "w = 256\n",
    "img_array = []    \n",
    "for k in range(N_TESTING_SAMPLES):\n",
    "    print(k)\n",
    "    for i in range(h):\n",
    "        for j in range (w):\n",
    "            if Ground_Truth[k,i,j]>0:\n",
    "                O1[k,i,j]=Ground_Truth[k,i,j]\n",
    "            if Segmentation_from_model[k,i,j]>0:\n",
    "                O2[k,i,j]=Segmentation_from_model[k,i,j]\n",
    "    W1 = O1[k,:,:]\n",
    "    W2 = O2[k,:,:]\n",
    "    img = concat_images(W1,W2)\n",
    "    img_array.append(img)\n",
    "    \n",
    "    path = \"/media/beta/navchetan-beps/Lars_BEP/Test_outputs/MiniNet/Joint_impap\"\n",
    "    os.chdir(path) \n",
    "    pathr = 'Joint_Image'+str(k+1).zfill(5)+'.png'\n",
    "    cv2.imwrite(pathr, img*255) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
